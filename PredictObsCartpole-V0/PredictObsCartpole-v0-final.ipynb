{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 18:52:33,632] Making new env: PredictObsCartpole-v0\n",
      "[2017-05-27 18:52:34,136] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n",
      "[2017-05-27 18:52:34,137] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 54.0 Training loss: 1.1507 Prediction loss: 0.2472 Explore P: 0.9947 RunMean : 54.0000\n",
      "Episode: 1 Total reward: 29.0 Training loss: 1.1670 Prediction loss: 0.1079 Explore P: 0.9918 RunMean : 41.5000\n",
      "Episode: 2 Total reward: 32.0 Training loss: 1.2318 Prediction loss: 0.1519 Explore P: 0.9887 RunMean : 38.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 18:52:37,075] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3 Total reward: 11.0 Training loss: 1.2236 Prediction loss: 0.0682 Explore P: 0.9876 RunMean : 31.5000\n",
      "Episode: 4 Total reward: 14.0 Training loss: 1.2184 Prediction loss: 0.0380 Explore P: 0.9862 RunMean : 28.0000\n",
      "Episode: 5 Total reward: 60.0 Training loss: 1.3775 Prediction loss: 0.0557 Explore P: 0.9804 RunMean : 33.3333\n",
      "Episode: 6 Total reward: 11.0 Training loss: 1.2719 Prediction loss: 0.0380 Explore P: 0.9793 RunMean : 30.1429\n",
      "Episode: 7 Total reward: 13.0 Training loss: 1.2858 Prediction loss: 0.0159 Explore P: 0.9781 RunMean : 28.0000\n",
      "Episode: 8 Total reward: 31.0 Training loss: 1.3267 Prediction loss: 0.0169 Explore P: 0.9751 RunMean : 28.3333\n",
      "Episode: 9 Total reward: 20.0 Training loss: 1.3901 Prediction loss: 0.0171 Explore P: 0.9731 RunMean : 27.5000\n",
      "Episode: 10 Total reward: 46.0 Training loss: 1.8072 Prediction loss: 0.0127 Explore P: 0.9687 RunMean : 29.1818\n",
      "Episode: 11 Total reward: 9.0 Training loss: 3.4569 Prediction loss: 0.0660 Explore P: 0.9679 RunMean : 27.5000\n",
      "Episode: 12 Total reward: 15.0 Training loss: 2.0297 Prediction loss: 0.0139 Explore P: 0.9664 RunMean : 26.5385\n",
      "Episode: 13 Total reward: 18.0 Training loss: 2.2356 Prediction loss: 0.0127 Explore P: 0.9647 RunMean : 25.9286\n",
      "Episode: 14 Total reward: 14.0 Training loss: 1.9106 Prediction loss: 0.0234 Explore P: 0.9634 RunMean : 25.1333\n",
      "Episode: 15 Total reward: 17.0 Training loss: 2.1474 Prediction loss: 0.0212 Explore P: 0.9618 RunMean : 24.6250\n",
      "Episode: 16 Total reward: 18.0 Training loss: 4.1794 Prediction loss: 0.0200 Explore P: 0.9600 RunMean : 24.2353\n",
      "Episode: 17 Total reward: 67.0 Training loss: 6.3755 Prediction loss: 0.0122 Explore P: 0.9537 RunMean : 26.6111\n",
      "Episode: 18 Total reward: 36.0 Training loss: 5.9908 Prediction loss: 0.0110 Explore P: 0.9503 RunMean : 27.1053\n",
      "Episode: 19 Total reward: 17.0 Training loss: 6.7034 Prediction loss: 0.0107 Explore P: 0.9487 RunMean : 26.6000\n",
      "Episode: 20 Total reward: 13.0 Training loss: 10.2678 Prediction loss: 0.0096 Explore P: 0.9475 RunMean : 25.9524\n",
      "Episode: 21 Total reward: 11.0 Training loss: 3.2625 Prediction loss: 0.0026 Explore P: 0.9465 RunMean : 25.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 18:52:45,665] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 22 Total reward: 50.0 Training loss: 4.4852 Prediction loss: 0.0047 Explore P: 0.9418 RunMean : 26.3478\n",
      "Episode: 23 Total reward: 12.0 Training loss: 4.9922 Prediction loss: 0.0084 Explore P: 0.9407 RunMean : 25.7500\n",
      "Episode: 24 Total reward: 30.0 Training loss: 3.7123 Prediction loss: 0.0037 Explore P: 0.9379 RunMean : 25.9200\n",
      "Episode: 25 Total reward: 16.0 Training loss: 6.4470 Prediction loss: 0.0100 Explore P: 0.9364 RunMean : 25.5385\n",
      "Episode: 26 Total reward: 19.0 Training loss: 4.9838 Prediction loss: 0.0051 Explore P: 0.9346 RunMean : 25.2963\n",
      "Episode: 27 Total reward: 31.0 Training loss: 20.6207 Prediction loss: 0.0143 Explore P: 0.9318 RunMean : 25.5000\n",
      "Episode: 28 Total reward: 26.0 Training loss: 21.5904 Prediction loss: 0.0062 Explore P: 0.9294 RunMean : 25.5172\n",
      "Episode: 29 Total reward: 24.0 Training loss: 12.9820 Prediction loss: 0.0186 Explore P: 0.9272 RunMean : 25.4667\n",
      "Episode: 30 Total reward: 18.0 Training loss: 37.1349 Prediction loss: 0.0175 Explore P: 0.9255 RunMean : 25.2258\n",
      "Episode: 31 Total reward: 35.0 Training loss: 12.5682 Prediction loss: 0.0141 Explore P: 0.9223 RunMean : 25.5312\n",
      "Episode: 32 Total reward: 10.0 Training loss: 12.5010 Prediction loss: 0.0070 Explore P: 0.9214 RunMean : 25.0606\n",
      "Episode: 33 Total reward: 12.0 Training loss: 78.7606 Prediction loss: 0.0240 Explore P: 0.9203 RunMean : 24.6765\n",
      "Episode: 34 Total reward: 9.0 Training loss: 38.9875 Prediction loss: 0.0179 Explore P: 0.9195 RunMean : 24.2286\n",
      "Episode: 35 Total reward: 18.0 Training loss: 49.2288 Prediction loss: 0.0225 Explore P: 0.9179 RunMean : 24.0556\n",
      "Episode: 36 Total reward: 24.0 Training loss: 15.3827 Prediction loss: 0.0079 Explore P: 0.9157 RunMean : 24.0541\n",
      "Episode: 37 Total reward: 24.0 Training loss: 4.3559 Prediction loss: 0.0046 Explore P: 0.9135 RunMean : 24.0526\n",
      "Episode: 38 Total reward: 15.0 Training loss: 37.4586 Prediction loss: 0.0099 Explore P: 0.9122 RunMean : 23.8205\n",
      "Episode: 39 Total reward: 20.0 Training loss: 81.9942 Prediction loss: 0.0198 Explore P: 0.9104 RunMean : 23.7250\n",
      "Episode: 40 Total reward: 10.0 Training loss: 16.7300 Prediction loss: 0.0057 Explore P: 0.9095 RunMean : 23.3902\n",
      "Episode: 41 Total reward: 26.0 Training loss: 5.5853 Prediction loss: 0.0109 Explore P: 0.9071 RunMean : 23.4524\n",
      "Episode: 42 Total reward: 11.0 Training loss: 26.4407 Prediction loss: 0.0077 Explore P: 0.9061 RunMean : 23.1628\n",
      "Episode: 43 Total reward: 38.0 Training loss: 166.5047 Prediction loss: 0.0703 Explore P: 0.9027 RunMean : 23.5000\n",
      "Episode: 44 Total reward: 23.0 Training loss: 70.2120 Prediction loss: 0.0131 Explore P: 0.9007 RunMean : 23.4889\n",
      "Episode: 45 Total reward: 25.0 Training loss: 116.3485 Prediction loss: 0.0269 Explore P: 0.8985 RunMean : 23.5217\n",
      "Episode: 46 Total reward: 19.0 Training loss: 32.1631 Prediction loss: 0.0112 Explore P: 0.8968 RunMean : 23.4255\n",
      "Episode: 47 Total reward: 16.0 Training loss: 66.9401 Prediction loss: 0.0111 Explore P: 0.8954 RunMean : 23.2708\n",
      "Episode: 48 Total reward: 12.0 Training loss: 71.7106 Prediction loss: 0.0141 Explore P: 0.8943 RunMean : 23.0408\n",
      "Episode: 49 Total reward: 13.0 Training loss: 5.1387 Prediction loss: 0.0021 Explore P: 0.8932 RunMean : 22.8400\n",
      "Episode: 50 Total reward: 24.0 Training loss: 60.6835 Prediction loss: 0.0090 Explore P: 0.8910 RunMean : 22.8627\n",
      "Episode: 51 Total reward: 22.0 Training loss: 66.5076 Prediction loss: 0.0114 Explore P: 0.8891 RunMean : 22.8462\n",
      "Episode: 52 Total reward: 14.0 Training loss: 127.4475 Prediction loss: 0.0143 Explore P: 0.8879 RunMean : 22.6792\n",
      "Episode: 53 Total reward: 9.0 Training loss: 3.2168 Prediction loss: 0.0037 Explore P: 0.8871 RunMean : 22.4259\n",
      "Episode: 54 Total reward: 10.0 Training loss: 174.2133 Prediction loss: 0.0267 Explore P: 0.8862 RunMean : 22.2000\n",
      "Episode: 55 Total reward: 42.0 Training loss: 2.7851 Prediction loss: 0.0090 Explore P: 0.8825 RunMean : 22.5536\n",
      "Episode: 56 Total reward: 17.0 Training loss: 104.3330 Prediction loss: 0.0135 Explore P: 0.8811 RunMean : 22.4561\n",
      "Episode: 57 Total reward: 24.0 Training loss: 47.7373 Prediction loss: 0.0053 Explore P: 0.8790 RunMean : 22.4828\n",
      "Episode: 58 Total reward: 22.0 Training loss: 150.6252 Prediction loss: 0.0202 Explore P: 0.8771 RunMean : 22.4746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 18:52:59,818] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 59 Total reward: 15.0 Training loss: 138.2730 Prediction loss: 0.0118 Explore P: 0.8758 RunMean : 22.3500\n",
      "Episode: 60 Total reward: 25.0 Training loss: 27.7780 Prediction loss: 0.0092 Explore P: 0.8736 RunMean : 22.3934\n",
      "Episode: 61 Total reward: 14.0 Training loss: 125.3657 Prediction loss: 0.0106 Explore P: 0.8724 RunMean : 22.2581\n",
      "Episode: 62 Total reward: 12.0 Training loss: 46.8232 Prediction loss: 0.0129 Explore P: 0.8714 RunMean : 22.0952\n",
      "Episode: 63 Total reward: 31.0 Training loss: 65.0295 Prediction loss: 0.0218 Explore P: 0.8687 RunMean : 22.2344\n",
      "Episode: 64 Total reward: 26.0 Training loss: 1.2323 Prediction loss: 0.0024 Explore P: 0.8665 RunMean : 22.2923\n",
      "Episode: 65 Total reward: 13.0 Training loss: 1.0015 Prediction loss: 0.0023 Explore P: 0.8653 RunMean : 22.1515\n",
      "Episode: 66 Total reward: 21.0 Training loss: 35.4423 Prediction loss: 0.0056 Explore P: 0.8636 RunMean : 22.1343\n",
      "Episode: 67 Total reward: 20.0 Training loss: 1.3215 Prediction loss: 0.0051 Explore P: 0.8618 RunMean : 22.1029\n",
      "Episode: 68 Total reward: 12.0 Training loss: 109.4048 Prediction loss: 0.0258 Explore P: 0.8608 RunMean : 21.9565\n",
      "Episode: 69 Total reward: 11.0 Training loss: 1.1362 Prediction loss: 0.0012 Explore P: 0.8599 RunMean : 21.8000\n",
      "Episode: 70 Total reward: 16.0 Training loss: 34.3116 Prediction loss: 0.0091 Explore P: 0.8585 RunMean : 21.7183\n",
      "Episode: 71 Total reward: 13.0 Training loss: 43.1539 Prediction loss: 0.0089 Explore P: 0.8574 RunMean : 21.5972\n",
      "Episode: 72 Total reward: 29.0 Training loss: 38.6236 Prediction loss: 0.0109 Explore P: 0.8550 RunMean : 21.6986\n",
      "Episode: 73 Total reward: 21.0 Training loss: 67.4993 Prediction loss: 0.0118 Explore P: 0.8532 RunMean : 21.6892\n",
      "Episode: 74 Total reward: 15.0 Training loss: 79.2869 Prediction loss: 0.0148 Explore P: 0.8519 RunMean : 21.6000\n",
      "Episode: 75 Total reward: 10.0 Training loss: 16.5463 Prediction loss: 0.0073 Explore P: 0.8511 RunMean : 21.4474\n",
      "Episode: 76 Total reward: 27.0 Training loss: 0.8384 Prediction loss: 0.0046 Explore P: 0.8488 RunMean : 21.5195\n",
      "Episode: 77 Total reward: 13.0 Training loss: 58.8941 Prediction loss: 0.0158 Explore P: 0.8477 RunMean : 21.4103\n",
      "Episode: 78 Total reward: 13.0 Training loss: 53.2238 Prediction loss: 0.0095 Explore P: 0.8466 RunMean : 21.3038\n",
      "Episode: 79 Total reward: 17.0 Training loss: 0.9538 Prediction loss: 0.0011 Explore P: 0.8452 RunMean : 21.2500\n",
      "Episode: 80 Total reward: 20.0 Training loss: 25.1082 Prediction loss: 0.0084 Explore P: 0.8436 RunMean : 21.2346\n",
      "Episode: 81 Total reward: 34.0 Training loss: 1.0029 Prediction loss: 0.0049 Explore P: 0.8407 RunMean : 21.3902\n",
      "Episode: 82 Total reward: 46.0 Training loss: 58.6634 Prediction loss: 0.0419 Explore P: 0.8369 RunMean : 21.6867\n",
      "Episode: 83 Total reward: 11.0 Training loss: 0.7311 Prediction loss: 0.0049 Explore P: 0.8360 RunMean : 21.5595\n",
      "Episode: 84 Total reward: 37.0 Training loss: 19.1242 Prediction loss: 0.0048 Explore P: 0.8330 RunMean : 21.7412\n",
      "Episode: 85 Total reward: 22.0 Training loss: 53.3702 Prediction loss: 0.0444 Explore P: 0.8311 RunMean : 21.7442\n",
      "Episode: 86 Total reward: 17.0 Training loss: 19.4955 Prediction loss: 0.0040 Explore P: 0.8298 RunMean : 21.6897\n",
      "Episode: 87 Total reward: 10.0 Training loss: 0.6636 Prediction loss: 0.0033 Explore P: 0.8289 RunMean : 21.5568\n",
      "Episode: 88 Total reward: 16.0 Training loss: 0.5180 Prediction loss: 0.0019 Explore P: 0.8276 RunMean : 21.4944\n",
      "Episode: 89 Total reward: 12.0 Training loss: 39.1895 Prediction loss: 0.0095 Explore P: 0.8266 RunMean : 21.3889\n",
      "Episode: 90 Total reward: 13.0 Training loss: 29.0283 Prediction loss: 0.0091 Explore P: 0.8256 RunMean : 21.2967\n",
      "Episode: 91 Total reward: 13.0 Training loss: 54.0322 Prediction loss: 0.0149 Explore P: 0.8245 RunMean : 21.2065\n",
      "Episode: 92 Total reward: 21.0 Training loss: 40.0636 Prediction loss: 0.0140 Explore P: 0.8228 RunMean : 21.2043\n",
      "Episode: 93 Total reward: 18.0 Training loss: 0.8695 Prediction loss: 0.0021 Explore P: 0.8214 RunMean : 21.1702\n",
      "Episode: 94 Total reward: 43.0 Training loss: 61.0060 Prediction loss: 0.0173 Explore P: 0.8179 RunMean : 21.4000\n",
      "Episode: 95 Total reward: 20.0 Training loss: 0.7183 Prediction loss: 0.0063 Explore P: 0.8163 RunMean : 21.3854\n",
      "Episode: 96 Total reward: 18.0 Training loss: 0.6716 Prediction loss: 0.0034 Explore P: 0.8148 RunMean : 21.3505\n",
      "Episode: 97 Total reward: 22.0 Training loss: 22.3497 Prediction loss: 0.0100 Explore P: 0.8130 RunMean : 21.3571\n",
      "Episode: 98 Total reward: 14.0 Training loss: 11.8565 Prediction loss: 0.0056 Explore P: 0.8119 RunMean : 21.2828\n",
      "Episode: 99 Total reward: 39.0 Training loss: 0.8232 Prediction loss: 0.0015 Explore P: 0.8088 RunMean : 21.4600\n",
      "Episode: 100 Total reward: 26.0 Training loss: 33.0768 Prediction loss: 0.0133 Explore P: 0.8067 RunMean : 21.1800\n",
      "Episode: 101 Total reward: 22.0 Training loss: 10.4436 Prediction loss: 0.0077 Explore P: 0.8050 RunMean : 21.1100\n",
      "Episode: 102 Total reward: 28.0 Training loss: 41.5017 Prediction loss: 0.0136 Explore P: 0.8027 RunMean : 21.0700\n",
      "Episode: 103 Total reward: 41.0 Training loss: 0.4559 Prediction loss: 0.0032 Explore P: 0.7995 RunMean : 21.3700\n",
      "Episode: 104 Total reward: 38.0 Training loss: 0.6143 Prediction loss: 0.0032 Explore P: 0.7965 RunMean : 21.6100\n",
      "Episode: 105 Total reward: 26.0 Training loss: 51.8442 Prediction loss: 0.0527 Explore P: 0.7945 RunMean : 21.2700\n",
      "Episode: 106 Total reward: 29.0 Training loss: 33.3725 Prediction loss: 0.0242 Explore P: 0.7922 RunMean : 21.4500\n",
      "Episode: 107 Total reward: 23.0 Training loss: 12.9288 Prediction loss: 0.0076 Explore P: 0.7904 RunMean : 21.5500\n",
      "Episode: 108 Total reward: 15.0 Training loss: 0.6716 Prediction loss: 0.0015 Explore P: 0.7892 RunMean : 21.3900\n",
      "Episode: 109 Total reward: 29.0 Training loss: 29.1564 Prediction loss: 0.0161 Explore P: 0.7870 RunMean : 21.4800\n",
      "Episode: 110 Total reward: 18.0 Training loss: 43.9167 Prediction loss: 0.0451 Explore P: 0.7856 RunMean : 21.2000\n",
      "Episode: 111 Total reward: 49.0 Training loss: 30.2941 Prediction loss: 0.0144 Explore P: 0.7818 RunMean : 21.6000\n",
      "Episode: 112 Total reward: 32.0 Training loss: 21.6789 Prediction loss: 0.0091 Explore P: 0.7793 RunMean : 21.7700\n",
      "Episode: 113 Total reward: 95.0 Training loss: 0.8132 Prediction loss: 0.0024 Explore P: 0.7720 RunMean : 22.5400\n",
      "Episode: 114 Total reward: 17.0 Training loss: 17.4216 Prediction loss: 0.0100 Explore P: 0.7708 RunMean : 22.5700\n",
      "Episode: 115 Total reward: 17.0 Training loss: 8.8946 Prediction loss: 0.0053 Explore P: 0.7695 RunMean : 22.5700\n",
      "Episode: 116 Total reward: 16.0 Training loss: 21.1690 Prediction loss: 0.0104 Explore P: 0.7682 RunMean : 22.5500\n",
      "Episode: 117 Total reward: 21.0 Training loss: 21.1595 Prediction loss: 0.0060 Explore P: 0.7667 RunMean : 22.0900\n",
      "Episode: 118 Total reward: 10.0 Training loss: 0.8005 Prediction loss: 0.0052 Explore P: 0.7659 RunMean : 21.8300\n",
      "Episode: 119 Total reward: 28.0 Training loss: 18.7568 Prediction loss: 0.0086 Explore P: 0.7638 RunMean : 21.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 18:53:26,169] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 120 Total reward: 19.0 Training loss: 9.2740 Prediction loss: 0.0043 Explore P: 0.7624 RunMean : 22.0000\n",
      "Episode: 121 Total reward: 18.0 Training loss: 12.6172 Prediction loss: 0.0066 Explore P: 0.7610 RunMean : 22.0700\n",
      "Episode: 122 Total reward: 48.0 Training loss: 10.6805 Prediction loss: 0.0054 Explore P: 0.7574 RunMean : 22.0500\n",
      "Episode: 123 Total reward: 15.0 Training loss: 25.4633 Prediction loss: 0.0345 Explore P: 0.7563 RunMean : 22.0800\n",
      "Episode: 124 Total reward: 11.0 Training loss: 24.9549 Prediction loss: 0.0343 Explore P: 0.7555 RunMean : 21.8900\n",
      "Episode: 125 Total reward: 21.0 Training loss: 0.5414 Prediction loss: 0.0049 Explore P: 0.7539 RunMean : 21.9400\n",
      "Episode: 126 Total reward: 13.0 Training loss: 50.0197 Prediction loss: 0.0182 Explore P: 0.7529 RunMean : 21.8800\n",
      "Episode: 127 Total reward: 39.0 Training loss: 12.3481 Prediction loss: 0.0061 Explore P: 0.7500 RunMean : 21.9600\n",
      "Episode: 128 Total reward: 16.0 Training loss: 32.7006 Prediction loss: 0.0155 Explore P: 0.7489 RunMean : 21.8600\n",
      "Episode: 129 Total reward: 72.0 Training loss: 42.3531 Prediction loss: 0.0158 Explore P: 0.7436 RunMean : 22.3400\n",
      "Episode: 130 Total reward: 25.0 Training loss: 14.1183 Prediction loss: 0.0034 Explore P: 0.7417 RunMean : 22.4100\n",
      "Episode: 131 Total reward: 30.0 Training loss: 22.7503 Prediction loss: 0.0096 Explore P: 0.7395 RunMean : 22.3600\n",
      "Episode: 132 Total reward: 19.0 Training loss: 0.9217 Prediction loss: 0.0011 Explore P: 0.7381 RunMean : 22.4500\n",
      "Episode: 133 Total reward: 31.0 Training loss: 10.4974 Prediction loss: 0.0104 Explore P: 0.7359 RunMean : 22.6400\n",
      "Episode: 134 Total reward: 21.0 Training loss: 8.8753 Prediction loss: 0.0089 Explore P: 0.7344 RunMean : 22.7600\n",
      "Episode: 135 Total reward: 12.0 Training loss: 1.1487 Prediction loss: 0.0038 Explore P: 0.7335 RunMean : 22.7000\n",
      "Episode: 136 Total reward: 13.0 Training loss: 13.3626 Prediction loss: 0.0033 Explore P: 0.7326 RunMean : 22.5900\n",
      "Episode: 137 Total reward: 15.0 Training loss: 0.8582 Prediction loss: 0.0005 Explore P: 0.7315 RunMean : 22.5000\n",
      "Episode: 138 Total reward: 11.0 Training loss: 20.2367 Prediction loss: 0.0079 Explore P: 0.7307 RunMean : 22.4600\n",
      "Episode: 139 Total reward: 17.0 Training loss: 0.7337 Prediction loss: 0.0031 Explore P: 0.7295 RunMean : 22.4300\n",
      "Episode: 140 Total reward: 15.0 Training loss: 8.7775 Prediction loss: 0.0029 Explore P: 0.7284 RunMean : 22.4800\n",
      "Episode: 141 Total reward: 23.0 Training loss: 7.5383 Prediction loss: 0.0079 Explore P: 0.7267 RunMean : 22.4500\n",
      "Episode: 142 Total reward: 21.0 Training loss: 19.5357 Prediction loss: 0.0080 Explore P: 0.7252 RunMean : 22.5500\n",
      "Episode: 143 Total reward: 15.0 Training loss: 31.4617 Prediction loss: 0.0117 Explore P: 0.7242 RunMean : 22.3200\n",
      "Episode: 144 Total reward: 15.0 Training loss: 23.9157 Prediction loss: 0.0098 Explore P: 0.7231 RunMean : 22.2400\n",
      "Episode: 145 Total reward: 19.0 Training loss: 8.8820 Prediction loss: 0.0046 Explore P: 0.7217 RunMean : 22.1800\n",
      "Episode: 146 Total reward: 20.0 Training loss: 0.8765 Prediction loss: 0.0007 Explore P: 0.7203 RunMean : 22.1900\n",
      "Episode: 147 Total reward: 24.0 Training loss: 30.2521 Prediction loss: 0.0142 Explore P: 0.7186 RunMean : 22.2700\n",
      "Episode: 148 Total reward: 14.0 Training loss: 23.6047 Prediction loss: 0.0136 Explore P: 0.7176 RunMean : 22.2900\n",
      "Episode: 149 Total reward: 28.0 Training loss: 0.4719 Prediction loss: 0.0022 Explore P: 0.7156 RunMean : 22.4400\n",
      "Episode: 150 Total reward: 37.0 Training loss: 16.8706 Prediction loss: 0.0093 Explore P: 0.7130 RunMean : 22.5700\n",
      "Episode: 151 Total reward: 21.0 Training loss: 6.7277 Prediction loss: 0.0067 Explore P: 0.7116 RunMean : 22.5600\n",
      "Episode: 152 Total reward: 25.0 Training loss: 26.1229 Prediction loss: 0.0067 Explore P: 0.7098 RunMean : 22.6700\n",
      "Episode: 153 Total reward: 14.0 Training loss: 17.5216 Prediction loss: 0.0074 Explore P: 0.7088 RunMean : 22.7200\n",
      "Episode: 154 Total reward: 62.0 Training loss: 31.9639 Prediction loss: 0.0442 Explore P: 0.7045 RunMean : 23.2400\n",
      "Episode: 155 Total reward: 39.0 Training loss: 11.8191 Prediction loss: 0.0115 Explore P: 0.7018 RunMean : 23.2100\n",
      "Episode: 156 Total reward: 25.0 Training loss: 16.1471 Prediction loss: 0.0074 Explore P: 0.7001 RunMean : 23.2900\n",
      "Episode: 157 Total reward: 40.0 Training loss: 23.3728 Prediction loss: 0.0088 Explore P: 0.6973 RunMean : 23.4500\n",
      "Episode: 158 Total reward: 30.0 Training loss: 1.1507 Prediction loss: 0.0189 Explore P: 0.6953 RunMean : 23.5300\n",
      "Episode: 159 Total reward: 31.0 Training loss: 0.7535 Prediction loss: 0.0019 Explore P: 0.6931 RunMean : 23.6900\n",
      "Episode: 160 Total reward: 35.0 Training loss: 12.0382 Prediction loss: 0.0045 Explore P: 0.6908 RunMean : 23.7900\n",
      "Episode: 161 Total reward: 14.0 Training loss: 19.7464 Prediction loss: 0.0400 Explore P: 0.6898 RunMean : 23.7900\n",
      "Episode: 162 Total reward: 57.0 Training loss: 21.2828 Prediction loss: 0.0057 Explore P: 0.6859 RunMean : 24.2400\n",
      "Episode: 163 Total reward: 60.0 Training loss: 11.2446 Prediction loss: 0.0069 Explore P: 0.6819 RunMean : 24.5300\n",
      "Episode: 164 Total reward: 36.0 Training loss: 0.9593 Prediction loss: 0.0089 Explore P: 0.6795 RunMean : 24.6300\n",
      "Episode: 165 Total reward: 15.0 Training loss: 22.3666 Prediction loss: 0.0083 Explore P: 0.6785 RunMean : 24.6500\n",
      "Episode: 166 Total reward: 20.0 Training loss: 16.8655 Prediction loss: 0.0080 Explore P: 0.6771 RunMean : 24.6400\n",
      "Episode: 167 Total reward: 37.0 Training loss: 13.9877 Prediction loss: 0.0073 Explore P: 0.6747 RunMean : 24.8100\n",
      "Episode: 168 Total reward: 42.0 Training loss: 32.4028 Prediction loss: 0.0171 Explore P: 0.6719 RunMean : 25.1100\n",
      "Episode: 169 Total reward: 36.0 Training loss: 18.2416 Prediction loss: 0.0105 Explore P: 0.6695 RunMean : 25.3600\n",
      "Episode: 170 Total reward: 71.0 Training loss: 16.0054 Prediction loss: 0.0089 Explore P: 0.6648 RunMean : 25.9100\n",
      "Episode: 171 Total reward: 24.0 Training loss: 4.5204 Prediction loss: 0.0050 Explore P: 0.6633 RunMean : 26.0200\n",
      "Episode: 172 Total reward: 50.0 Training loss: 11.6704 Prediction loss: 0.0052 Explore P: 0.6600 RunMean : 26.2300\n",
      "Episode: 173 Total reward: 33.0 Training loss: 8.9731 Prediction loss: 0.0066 Explore P: 0.6579 RunMean : 26.3500\n",
      "Episode: 174 Total reward: 75.0 Training loss: 10.0374 Prediction loss: 0.0054 Explore P: 0.6530 RunMean : 26.9500\n",
      "Episode: 175 Total reward: 44.0 Training loss: 9.3458 Prediction loss: 0.0044 Explore P: 0.6502 RunMean : 27.2900\n",
      "Episode: 176 Total reward: 21.0 Training loss: 0.7287 Prediction loss: 0.0017 Explore P: 0.6489 RunMean : 27.2300\n",
      "Episode: 177 Total reward: 32.0 Training loss: 0.7153 Prediction loss: 0.0020 Explore P: 0.6468 RunMean : 27.4200\n",
      "Episode: 178 Total reward: 61.0 Training loss: 36.6522 Prediction loss: 0.0154 Explore P: 0.6430 RunMean : 27.9000\n",
      "Episode: 179 Total reward: 109.9345688967149 Training loss: 11.0803 Prediction loss: 0.0082 Explore P: 0.6362 RunMean : 28.8293\n",
      "Episode: 180 Total reward: 71.0 Training loss: 27.8174 Prediction loss: 0.0063 Explore P: 0.6317 RunMean : 29.3393\n",
      "Episode: 181 Total reward: 29.0 Training loss: 6.3220 Prediction loss: 0.0039 Explore P: 0.6299 RunMean : 29.2893\n",
      "Episode: 182 Total reward: 143.7582242326177 Training loss: 6.2161 Prediction loss: 0.0059 Explore P: 0.6214 RunMean : 30.2669\n",
      "Episode: 183 Total reward: 20.0 Training loss: 15.9064 Prediction loss: 0.0069 Explore P: 0.6202 RunMean : 30.3569\n",
      "Episode: 184 Total reward: 51.0 Training loss: 4.3384 Prediction loss: 0.0018 Explore P: 0.6170 RunMean : 30.4969\n",
      "Episode: 185 Total reward: 22.0 Training loss: 16.2764 Prediction loss: 0.0063 Explore P: 0.6157 RunMean : 30.4969\n",
      "Episode: 186 Total reward: 16.0 Training loss: 0.8549 Prediction loss: 0.0013 Explore P: 0.6147 RunMean : 30.4869\n",
      "Episode: 187 Total reward: 119.02765628125515 Training loss: 5.5055 Prediction loss: 0.0028 Explore P: 0.6077 RunMean : 31.5772\n",
      "Episode: 188 Total reward: 20.0 Training loss: 2.0619 Prediction loss: 0.0038 Explore P: 0.6065 RunMean : 31.6172\n",
      "Episode: 189 Total reward: 139.44699526391608 Training loss: 6.6247 Prediction loss: 0.0345 Explore P: 0.5985 RunMean : 32.8917\n",
      "Episode: 190 Total reward: 227.23313414664804 Training loss: 6.0184 Prediction loss: 0.0022 Explore P: 0.5869 RunMean : 35.0340\n",
      "Episode: 191 Total reward: 40.0 Training loss: 11.7089 Prediction loss: 0.0066 Explore P: 0.5846 RunMean : 35.3040\n",
      "Episode: 192 Total reward: 18.0 Training loss: 0.6828 Prediction loss: 0.0017 Explore P: 0.5835 RunMean : 35.2740\n",
      "Episode: 193 Total reward: 152.4202144729041 Training loss: 12.1509 Prediction loss: 0.0037 Explore P: 0.5754 RunMean : 36.6182\n",
      "Episode: 194 Total reward: 69.0 Training loss: 18.1164 Prediction loss: 0.0052 Explore P: 0.5716 RunMean : 36.8782\n",
      "Episode: 195 Total reward: 33.0 Training loss: 1.2182 Prediction loss: 0.0002 Explore P: 0.5697 RunMean : 37.0082\n",
      "Episode: 196 Total reward: 42.0 Training loss: 6.7704 Prediction loss: 0.0035 Explore P: 0.5674 RunMean : 37.2482\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from gym import wrappers\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "%matplotlib inline\n",
    "# base code originally from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb\n",
    "\n",
    "# Restricts to running on a single GPU, in this case the second GPU (\"1\")\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0., \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for input states\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "            \n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "            \n",
    "            # placeholder for next state\n",
    "            self.next_state_ = tf.placeholder(tf.float32,\\\n",
    "                                             [None, state_size],\\\n",
    "                                             name='next_state')\n",
    "            \n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "            \n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "            \n",
    "                \n",
    "            # Q Value network :\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=None,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.maximum(alpha*self.fc1,self.fc1)\n",
    "            \n",
    "            self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.maximum(alpha*self.fc2,self.fc2)\n",
    "                \n",
    "            out_layer = self.fc2\n",
    "            \n",
    "\n",
    "            # Predict next state network :\n",
    "            self.pred_input = tf.concat([self.inputs_,one_hot_actions],1)\n",
    "            \n",
    "            self.pred_fc1 = tf.layers.dense(self.pred_input, \\\n",
    "                                           hidden_size,\\\n",
    "                                           activation=None,\\\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.pred_fc1 = tf.maximum(alpha*self.pred_fc1,self.pred_fc1)\n",
    "            \n",
    "            self.pred_fc2 = tf.layers.dense(self.pred_fc1, \\\n",
    "                                           hidden_size,\\\n",
    "                                           activation=None,\\\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.pred_fc2 = tf.maximum(alpha*self.pred_fc2,self.pred_fc2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # Linear output layer for next state prediction\n",
    "            self.prediction_state = tf.layers.dense(self.pred_fc2, state_size, \\\n",
    "                                                    activation=None,\\\n",
    "                                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            # loss and optimizer for Q network\n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "            # loss and optimizer for next state network\n",
    "            self.prediction_loss = tf.reduce_mean(tf.square(self.next_state_ - self.prediction_state))\n",
    "            self.pred_opt = tf.train.AdamOptimizer(learning_rate).minimize(self.prediction_loss)\n",
    "\n",
    "\n",
    "            \n",
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N \n",
    "\n",
    "def normalize_state(x, denormalize=False):\n",
    "    normalizer = [2.,3.,0.3,2.]\n",
    "    if denormalize:\n",
    "        y = x * normalizer\n",
    "    else:\n",
    "        \n",
    "        y = x / normalizer\n",
    "    return y\n",
    "\n",
    "def predict_next_state(mainQN,state,sess):\n",
    "    Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: state})\n",
    "    action = np.argmax(Qs)\n",
    "    pred_state = sess.run(mainQN.prediction_state, \n",
    "                          feed_dict={mainQN.inputs_: state,\n",
    "                                    mainQN.actions_: np.expand_dims(action, axis=0)})\n",
    "\n",
    "    return pred_state\n",
    "\n",
    "def initialize_memory_rand_states(memory_size=1000,pretrain_length=32):\n",
    "    \n",
    "    # Initialize the simulation\n",
    "    # Make a random action\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    ii = 0\n",
    "    while ii < pretrain_length or not done:\n",
    "        \n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        action = action[0]\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "            state = normalize_state(state)\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        \n",
    "        ii = ii + 1\n",
    "            \n",
    "    return memory\n",
    "\n",
    "def train_q_network(mainQN,\\\n",
    "                    memory,\\\n",
    "                    train_episodes=1500,\\\n",
    "                    gamma=0.99,\\\n",
    "                    explore_start=1.0,\\\n",
    "                    explore_stop=0.01,\\\n",
    "                    decay_rate=0.0001,\\\n",
    "                    batch_size=32,\\\n",
    "                    max_steps=500,\\\n",
    "                    verbose=True):\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        steps_list = []\n",
    "        \n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            \n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "\n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    #action = env.action_space.sample()\n",
    "                    action = random.randint(0,1)\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "\n",
    "\n",
    "                #action is first\n",
    "                pred_action = []\n",
    "                pred_action.append(action)\n",
    "                \n",
    "                #predict next state from this action\n",
    "                next_state = state.reshape((1, *state.shape))\n",
    "                for iterator in range(5):\n",
    "                    #predict next action from this state\n",
    "                    next_state = predict_next_state(mainQN,next_state,sess)\n",
    "                    pred_action.append(normalize_state(next_state,denormalize=True))\n",
    "                                                \n",
    "                \n",
    "                                   \n",
    "                                   \n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(pred_action)\n",
    "                next_state = normalize_state(next_state)\n",
    "                \n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    t = t+1\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    steps_list.append(total_reward)\n",
    "                    t = max_steps\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = env.reset()\n",
    "                    state = normalize_state(state)\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "                \n",
    "                pred_loss, _ = sess.run([mainQN.prediction_loss, mainQN.pred_opt],\n",
    "                                       feed_dict={mainQN.inputs_: states,\n",
    "                                                mainQN.actions_: actions,\n",
    "                                                mainQN.next_state_: next_states})\n",
    "            \n",
    "            rewards_list.append((ep, total_reward))   \n",
    "            runningMean = np.mean(steps_list[-100:])\n",
    "            if verbose:\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Prediction loss: {:.4f}'.format(pred_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p),\n",
    "                      'RunMean : {:.4f}'.format(runningMean))\n",
    "               \n",
    "            \n",
    "            \n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver, runningMean\n",
    "\n",
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "def generate_and_train_qnetwork(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=128,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=32,\\\n",
    "                   alpha=0.1,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, \\\n",
    "                      hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "\n",
    "    \n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver = train_q_network(mainQN,\\\n",
    "                                  memory,\\\n",
    "                                  train_episodes = train_episodes, \\\n",
    "                                  gamma=gamma,\\\n",
    "                                  explore_start=explore_start,\\\n",
    "                                  explore_stop=explore_stop,\\\n",
    "                                  decay_rate=decay_rate,\\\n",
    "                                  batch_size=batch_size,\\\n",
    "                                  verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    \n",
    "    return avg_train_rewards, mainQN, saver, len(rewards_list)\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('PredictObsCartpole-v0')\n",
    "\n",
    "# Start monitor\n",
    "env = wrappers.Monitor(env, '/tmp/PredictObsCartpole-experiment-1',force=True)\n",
    "\n",
    "# Train network\n",
    "avg_train_rewards, mainQN, saver, number_of_episodes = generate_and_train_qnetwork(train_episodes=500, verbose=True)\n",
    "print('average test reward = ', avg_train_rewards,'   number of trials = ',number_of_episodes)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-24 19:52:17,301] Finished writing results. You can upload them to the scoreboard via gym.upload('D:\\\\tmp\\\\cartpole-experiment-1')\n",
      "[2017-05-24 19:52:17,303] [CartPole-v1] Uploading 392 episodes of training data\n",
      "[2017-05-24 19:52:18,901] [CartPole-v1] Uploading videos of 8 training episodes (51022 bytes)\n",
      "[2017-05-24 19:52:19,329] [CartPole-v1] Creating evaluation object from /tmp/cartpole-experiment-1 with learning curve and training video\n",
      "[2017-05-24 19:52:19,643] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v1 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_O2E4DuxjS7GAjEYSb0edqQ\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gym.upload('/tmp/PredictObsCartpole-experiment-1', api_key='sk_2nAEHbARwKPuKcao8nWRw')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
