{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "# base code from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-23 18:54:53,019] Making new env: CartPole-v0\n",
      "[2017-05-23 18:54:53,030] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.00996908,  0.18984161, -0.01559309, -0.34500568]), 1.0, False, {})\n",
      "(array([-0.00617224, -0.0050551 , -0.0224932 , -0.05728035]), 1.0, False, {})\n",
      "(array([-0.00627335, -0.19984743, -0.02363881,  0.2282218 ]), 1.0, False, {})\n",
      "(array([-0.01027029, -0.39462373, -0.01907437,  0.51335546]), 1.0, False, {})\n",
      "(array([-0.01816277, -0.1992384 , -0.00880726,  0.21472325]), 1.0, False, {})\n",
      "(array([-0.02214754, -0.39423334, -0.0045128 ,  0.50461501]), 1.0, False, {})\n",
      "(array([-0.0300322 , -0.19904808,  0.0055795 ,  0.21051334]), 1.0, False, {})\n",
      "(array([-0.03401317, -0.39424936,  0.00978977,  0.50495107]), 1.0, False, {})\n",
      "(array([-0.04189815, -0.19926673,  0.01988879,  0.21536929]), 1.0, False, {})\n",
      "(array([-0.04588349, -0.39466729,  0.02419617,  0.51425907]), 1.0, False, {})\n",
      "(array([-0.05377683, -0.5901215 ,  0.03448136,  0.81446755]), 1.0, False, {})\n",
      "(array([-0.06557926, -0.78569825,  0.05077071,  1.11779381]), 1.0, False, {})\n",
      "(array([-0.08129323, -0.98144833,  0.07312658,  1.42596075]), 1.0, False, {})\n",
      "(array([-0.10092219, -1.17739382,  0.1016458 ,  1.74057353]), 1.0, False, {})\n",
      "(array([-0.12447007, -0.98356571,  0.13645727,  1.48116445]), 1.0, False, {})\n",
      "(array([-0.14414138, -1.18006322,  0.16608056,  1.81316486]), 1.0, False, {})\n",
      "(array([-0.16774265, -1.37660019,  0.20234385,  2.1525174 ]), 1.0, False, {})\n",
      "(array([-0.19527465, -1.18396243,  0.2453942 ,  1.92853996]), 1.0, True, {})\n",
      "(array([-0.2189539 , -1.38073284,  0.283965  ,  2.28627718]), 0.0, True, {})\n",
      "(array([-0.24656856, -1.57715501,  0.32969055,  2.65147924]), 0.0, True, {})\n",
      "(array([-0.27811166, -1.77294779,  0.38272013,  3.02453364]), 0.0, True, {})\n",
      "(array([-0.31357061, -1.58141571,  0.4432108 ,  2.86781379]), 0.0, True, {})\n",
      "(array([-0.34519893, -1.77603384,  0.50056708,  3.25761442]), 0.0, True, {})\n",
      "(array([-0.38071961, -1.96897375,  0.56571937,  3.65261415]), 0.0, True, {})\n",
      "(array([-0.42009908, -1.77739223,  0.63877165,  3.56760437]), 0.0, True, {})\n",
      "(array([-0.45564693, -1.96704267,  0.71012374,  3.97127527]), 0.0, True, {})\n",
      "(array([-0.49498778, -1.7749532 ,  0.78954924,  3.94445522]), 0.0, True, {})\n",
      "(array([-0.53048684, -1.95965436,  0.86843835,  4.34829605]), 0.0, True, {})\n",
      "(array([-0.56967993, -2.14007576,  0.95540427,  4.747546  ]), 0.0, True, {})\n",
      "(array([-0.61248144, -1.94335587,  1.05035519,  4.81726757]), 0.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "# Create new cart pole environment\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "for a in range(30):\n",
    "    print(env.step(env.action_space.sample()))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0., \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "            \n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "            \n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "            \n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "            \n",
    "                \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=None,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.maximum(alpha*self.fc1,self.fc1)\n",
    "            \n",
    "            if hidden_layers == 1:\n",
    "                out_layer = self.fc1\n",
    "            else:\n",
    "                \n",
    "                self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.fc2 = tf.maximum(alpha*self.fc2,self.fc2)\n",
    "                \n",
    "                if hidden_layers == 2:\n",
    "                    out_layer = self.fc2\n",
    "                else:\n",
    "                    self.fc3 = tf.layers.dense(self.fc2, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                    self.fc3 = tf.maximum(alpha*self.fc3,self.fc3)\n",
    "                    out_layer = self.fc3\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_memory_rand_states(memory_size=10000,pretrain_length=20):\n",
    "    # Initialize the simulation\n",
    "    state = env.reset()\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    for ii in range(pretrain_length):\n",
    "\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_q_network(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   max_steps=220,\\\n",
    "                   alpha=0.,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    \n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        steps_list = []\n",
    "        \n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            \n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "                # Uncomment this next line to watch the training\n",
    "                # env.render() \n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    t = t+1\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    steps_list.append(t)\n",
    "                    t = max_steps\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = env.reset()\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "            \n",
    "            rewards_list.append((ep, total_reward))    \n",
    "            if verbose:\n",
    "\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p),\n",
    "                      'RunMean : {:.4f}'.format(np.mean(steps_list[-100:])))\n",
    "               \n",
    "            \n",
    "            \n",
    "            \n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_q_network(mainQN, saver, test_episodes=100, test_max_steps=500, render=True):\n",
    "\n",
    "\n",
    "    avg_rewards = 0.\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "   \n",
    "        state = env.reset()\n",
    "        for ep in range(test_episodes):\n",
    "            t = 0\n",
    "            while t < test_max_steps:\n",
    "                if render:\n",
    "                    env.render() \n",
    "\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                avg_rewards = avg_rewards + reward / test_episodes\n",
    "                if done:\n",
    "                    t = test_max_steps\n",
    "                    state = env.reset()\n",
    "                    # Take one random step to get the pole and cart moving\n",
    "                    #state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "              \n",
    "    return avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_and_train_qnetwork(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   test_episodes=10,\\\n",
    "                   render=False,\\\n",
    "                   alpha=0.,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver = train_q_network(train_episodes = train_episodes, \\\n",
    "                                                  gamma=gamma,\\\n",
    "                                                  explore_start=explore_start,\\\n",
    "                                                  explore_stop=explore_stop,\\\n",
    "                                                  decay_rate=decay_rate,\\\n",
    "                                                  hidden_size=hidden_size,\\\n",
    "                                                  hidden_layers=hidden_layers,\\\n",
    "                                                  learning_rate=learning_rate,\\\n",
    "                                                  memory_size=memory_size,\\\n",
    "                                                  batch_size=batch_size,\\\n",
    "                                                  alpha=alpha,\\\n",
    "                                                  verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    # test q-network\n",
    "    avg_test_rewards = test_q_network(mainQN, saver, test_episodes=test_episodes, render=verbose)\n",
    "    if verbose:\n",
    "        print('average test reward = ', avg_test_rewards)\n",
    "    \n",
    "    return avg_test_rewards, avg_train_rewards, mainQN, saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 15:54:17,537] Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test reward =  9.249999999999996\n"
     ]
    }
   ],
   "source": [
    "# test implementation\n",
    "average_rewards = test_and_train_qnetwork(train_episodes=100, verbose=False)\n",
    "print('average test reward = ', average_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr=0.0001_ga=0.9 test avg=126.99999999999493  train avg=125.816']\n",
      "['dr=1e-05_ga=0.9 test avg=200.00000000001123  train avg=25.782']\n"
     ]
    }
   ],
   "source": [
    "train_eps = 500\n",
    "verb = False\n",
    "gamma = [0.99]\n",
    "decay_rate = [0.0001,0.00001]\n",
    "exp_start=1.0\n",
    "exp_stop=0.1\n",
    "hidden_size=64\n",
    "hidden_layers=1\n",
    "learning_rate=0.001\n",
    "batch_size=32\n",
    "num_averages = 1\n",
    "results = []\n",
    "alpha_relu = 0.05\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "for gaIndex in range(len(gamma)):\n",
    "    for drIndex in range(len(decay_rate)):\n",
    "        ga = gamma[gaIndex]\n",
    "        dr = decay_rate[drIndex]\n",
    "        train_params_name = 'dr='+str(dr)+'_ga='+str(ga)\n",
    "        average_test_rewards = 0.\n",
    "        average_train_rewards = 0.\n",
    "        for i in range(num_averages):\n",
    "            test,train = test_and_train_qnetwork(train_episodes=train_eps,\\\n",
    "                                   gamma=ga,\\\n",
    "                                   explore_start=exp_start,\\\n",
    "                                   explore_stop=exp_stop,\\\n",
    "                                   decay_rate=dr,\\\n",
    "                                   hidden_layers=hidden_layers,\\\n",
    "                                   hidden_size=hidden_size,\\\n",
    "                                   learning_rate=learning_rate,\\\n",
    "                                   batch_size=batch_size,\\\n",
    "                                   alpha = alpha_relu,\\\n",
    "                                   verbose=verb)\n",
    "            average_test_rewards += test\n",
    "            average_train_rewards += train\n",
    "\n",
    "        average_test_rewards = average_test_rewards / num_averages\n",
    "        average_train_rewards = average_train_rewards / num_averages\n",
    "        results.append([train_params_name+' test avg='+str(average_test_rewards)+'  train avg='+str(average_train_rewards)])\n",
    "        clear_output()\n",
    "        for each in results:\n",
    "            print(each)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 23:46:11,267] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 18.0 Training loss: 1.4925 Explore P: 0.9982 RunMean : 18.0000\n",
      "Episode: 1 Total reward: 11.0 Training loss: 1.4914 Explore P: 0.9971 RunMean : 14.5000\n",
      "Episode: 2 Total reward: 17.0 Training loss: 1.5187 Explore P: 0.9954 RunMean : 15.3333\n",
      "Episode: 3 Total reward: 13.0 Training loss: 2.2786 Explore P: 0.9941 RunMean : 14.7500\n",
      "Episode: 4 Total reward: 14.0 Training loss: 2.8833 Explore P: 0.9927 RunMean : 14.6000\n",
      "Episode: 5 Total reward: 17.0 Training loss: 5.9031 Explore P: 0.9910 RunMean : 15.0000\n",
      "Episode: 6 Total reward: 28.0 Training loss: 13.3053 Explore P: 0.9883 RunMean : 16.8571\n",
      "Episode: 7 Total reward: 17.0 Training loss: 17.7876 Explore P: 0.9866 RunMean : 16.8750\n",
      "Episode: 8 Total reward: 20.0 Training loss: 53.3974 Explore P: 0.9846 RunMean : 17.2222\n",
      "Episode: 9 Total reward: 25.0 Training loss: 101.9795 Explore P: 0.9822 RunMean : 18.0000\n",
      "Episode: 10 Total reward: 15.0 Training loss: 93.9879 Explore P: 0.9807 RunMean : 17.7273\n",
      "Episode: 11 Total reward: 37.0 Training loss: 107.5987 Explore P: 0.9771 RunMean : 19.3333\n",
      "Episode: 12 Total reward: 10.0 Training loss: 66.0609 Explore P: 0.9761 RunMean : 18.6154\n",
      "Episode: 13 Total reward: 16.0 Training loss: 114.6798 Explore P: 0.9745 RunMean : 18.4286\n",
      "Episode: 14 Total reward: 17.0 Training loss: 118.6565 Explore P: 0.9729 RunMean : 18.3333\n",
      "Episode: 15 Total reward: 32.0 Training loss: 146.4624 Explore P: 0.9698 RunMean : 19.1875\n",
      "Episode: 16 Total reward: 11.0 Training loss: 364.5240 Explore P: 0.9687 RunMean : 18.7059\n",
      "Episode: 17 Total reward: 16.0 Training loss: 668.2683 Explore P: 0.9672 RunMean : 18.5556\n",
      "Episode: 18 Total reward: 21.0 Training loss: 160.7688 Explore P: 0.9651 RunMean : 18.6842\n",
      "Episode: 19 Total reward: 23.0 Training loss: 168.0499 Explore P: 0.9629 RunMean : 18.9000\n",
      "Episode: 20 Total reward: 33.0 Training loss: 193.0030 Explore P: 0.9597 RunMean : 19.5714\n",
      "Episode: 21 Total reward: 21.0 Training loss: 85.8659 Explore P: 0.9577 RunMean : 19.6364\n",
      "Episode: 22 Total reward: 28.0 Training loss: 86.0145 Explore P: 0.9550 RunMean : 20.0000\n",
      "Episode: 23 Total reward: 35.0 Training loss: 101.1878 Explore P: 0.9517 RunMean : 20.6250\n",
      "Episode: 24 Total reward: 15.0 Training loss: 51.1328 Explore P: 0.9503 RunMean : 20.4000\n",
      "Episode: 25 Total reward: 21.0 Training loss: 90.0564 Explore P: 0.9483 RunMean : 20.4231\n",
      "Episode: 26 Total reward: 14.0 Training loss: 53.9158 Explore P: 0.9470 RunMean : 20.1852\n",
      "Episode: 27 Total reward: 20.0 Training loss: 58.7501 Explore P: 0.9451 RunMean : 20.1786\n",
      "Episode: 28 Total reward: 24.0 Training loss: 70.6530 Explore P: 0.9428 RunMean : 20.3103\n",
      "Episode: 29 Total reward: 11.0 Training loss: 87.0365 Explore P: 0.9418 RunMean : 20.0000\n",
      "Episode: 30 Total reward: 15.0 Training loss: 70.5862 Explore P: 0.9404 RunMean : 19.8387\n",
      "Episode: 31 Total reward: 12.0 Training loss: 72.5077 Explore P: 0.9392 RunMean : 19.5938\n",
      "Episode: 32 Total reward: 21.0 Training loss: 48.6132 Explore P: 0.9373 RunMean : 19.6364\n",
      "Episode: 33 Total reward: 20.0 Training loss: 43.9540 Explore P: 0.9354 RunMean : 19.6471\n",
      "Episode: 34 Total reward: 44.0 Training loss: 37.6750 Explore P: 0.9313 RunMean : 20.3429\n",
      "Episode: 35 Total reward: 15.0 Training loss: 69.5133 Explore P: 0.9299 RunMean : 20.1944\n",
      "Episode: 36 Total reward: 17.0 Training loss: 73.7781 Explore P: 0.9283 RunMean : 20.1081\n",
      "Episode: 37 Total reward: 13.0 Training loss: 40.8958 Explore P: 0.9271 RunMean : 19.9211\n",
      "Episode: 38 Total reward: 22.0 Training loss: 49.3690 Explore P: 0.9251 RunMean : 19.9744\n",
      "Episode: 39 Total reward: 32.0 Training loss: 80.6522 Explore P: 0.9221 RunMean : 20.2750\n",
      "Episode: 40 Total reward: 21.0 Training loss: 100.5955 Explore P: 0.9202 RunMean : 20.2927\n",
      "Episode: 41 Total reward: 21.0 Training loss: 27.2419 Explore P: 0.9182 RunMean : 20.3095\n",
      "Episode: 42 Total reward: 68.0 Training loss: 59.4555 Explore P: 0.9120 RunMean : 21.4186\n",
      "Episode: 43 Total reward: 19.0 Training loss: 20.1409 Explore P: 0.9103 RunMean : 21.3636\n",
      "Episode: 44 Total reward: 17.0 Training loss: 28.0777 Explore P: 0.9087 RunMean : 21.2667\n",
      "Episode: 45 Total reward: 24.0 Training loss: 42.0503 Explore P: 0.9066 RunMean : 21.3261\n",
      "Episode: 46 Total reward: 23.0 Training loss: 29.1642 Explore P: 0.9045 RunMean : 21.3617\n",
      "Episode: 47 Total reward: 28.0 Training loss: 68.8543 Explore P: 0.9019 RunMean : 21.5000\n",
      "Episode: 48 Total reward: 28.0 Training loss: 19.6849 Explore P: 0.8994 RunMean : 21.6327\n",
      "Episode: 49 Total reward: 16.0 Training loss: 41.3142 Explore P: 0.8980 RunMean : 21.5200\n",
      "Episode: 50 Total reward: 45.0 Training loss: 10.1650 Explore P: 0.8940 RunMean : 21.9804\n",
      "Episode: 51 Total reward: 51.0 Training loss: 21.6722 Explore P: 0.8894 RunMean : 22.5385\n",
      "Episode: 52 Total reward: 47.0 Training loss: 54.0062 Explore P: 0.8852 RunMean : 23.0000\n",
      "Episode: 53 Total reward: 91.0 Training loss: 8.0995 Explore P: 0.8772 RunMean : 24.2593\n",
      "Episode: 54 Total reward: 15.0 Training loss: 88.1167 Explore P: 0.8759 RunMean : 24.0909\n",
      "Episode: 55 Total reward: 25.0 Training loss: 64.8756 Explore P: 0.8737 RunMean : 24.1071\n",
      "Episode: 56 Total reward: 20.0 Training loss: 103.2697 Explore P: 0.8720 RunMean : 24.0351\n",
      "Episode: 57 Total reward: 35.0 Training loss: 88.0152 Explore P: 0.8689 RunMean : 24.2241\n",
      "Episode: 58 Total reward: 12.0 Training loss: 7.1961 Explore P: 0.8679 RunMean : 24.0169\n",
      "Episode: 59 Total reward: 48.0 Training loss: 91.7080 Explore P: 0.8637 RunMean : 24.4167\n",
      "Episode: 60 Total reward: 20.0 Training loss: 23.3160 Explore P: 0.8620 RunMean : 24.3443\n",
      "Episode: 61 Total reward: 18.0 Training loss: 40.0029 Explore P: 0.8604 RunMean : 24.2419\n",
      "Episode: 62 Total reward: 16.0 Training loss: 58.5423 Explore P: 0.8591 RunMean : 24.1111\n",
      "Episode: 63 Total reward: 11.0 Training loss: 5.4821 Explore P: 0.8581 RunMean : 23.9062\n",
      "Episode: 64 Total reward: 35.0 Training loss: 25.6995 Explore P: 0.8551 RunMean : 24.0769\n",
      "Episode: 65 Total reward: 13.0 Training loss: 17.7888 Explore P: 0.8540 RunMean : 23.9091\n",
      "Episode: 66 Total reward: 30.0 Training loss: 132.4348 Explore P: 0.8515 RunMean : 24.0000\n",
      "Episode: 67 Total reward: 32.0 Training loss: 26.3768 Explore P: 0.8487 RunMean : 24.1176\n",
      "Episode: 68 Total reward: 30.0 Training loss: 74.1135 Explore P: 0.8462 RunMean : 24.2029\n",
      "Episode: 69 Total reward: 32.0 Training loss: 39.6947 Explore P: 0.8435 RunMean : 24.3143\n",
      "Episode: 70 Total reward: 62.0 Training loss: 37.5338 Explore P: 0.8383 RunMean : 24.8451\n",
      "Episode: 71 Total reward: 18.0 Training loss: 67.6552 Explore P: 0.8368 RunMean : 24.7500\n",
      "Episode: 72 Total reward: 23.0 Training loss: 22.7235 Explore P: 0.8349 RunMean : 24.7260\n",
      "Episode: 73 Total reward: 49.0 Training loss: 13.3450 Explore P: 0.8308 RunMean : 25.0541\n",
      "Episode: 74 Total reward: 12.0 Training loss: 19.5980 Explore P: 0.8298 RunMean : 24.8800\n",
      "Episode: 75 Total reward: 25.0 Training loss: 6.6093 Explore P: 0.8277 RunMean : 24.8816\n",
      "Episode: 76 Total reward: 52.0 Training loss: 170.5976 Explore P: 0.8234 RunMean : 25.2338\n",
      "Episode: 77 Total reward: 13.0 Training loss: 26.0171 Explore P: 0.8223 RunMean : 25.0769\n",
      "Episode: 78 Total reward: 47.0 Training loss: 84.6598 Explore P: 0.8185 RunMean : 25.3544\n",
      "Episode: 79 Total reward: 12.0 Training loss: 44.2734 Explore P: 0.8175 RunMean : 25.1875\n",
      "Episode: 80 Total reward: 78.0 Training loss: 39.8725 Explore P: 0.8112 RunMean : 25.8395\n",
      "Episode: 81 Total reward: 36.0 Training loss: 31.3318 Explore P: 0.8082 RunMean : 25.9634\n",
      "Episode: 82 Total reward: 47.0 Training loss: 131.2648 Explore P: 0.8044 RunMean : 26.2169\n",
      "Episode: 83 Total reward: 29.0 Training loss: 11.7156 Explore P: 0.8021 RunMean : 26.2500\n",
      "Episode: 84 Total reward: 45.0 Training loss: 94.6602 Explore P: 0.7985 RunMean : 26.4706\n",
      "Episode: 85 Total reward: 70.0 Training loss: 7.7881 Explore P: 0.7929 RunMean : 26.9767\n",
      "Episode: 86 Total reward: 19.0 Training loss: 37.2626 Explore P: 0.7914 RunMean : 26.8851\n",
      "Episode: 87 Total reward: 47.0 Training loss: 51.8599 Explore P: 0.7877 RunMean : 27.1136\n",
      "Episode: 88 Total reward: 59.0 Training loss: 36.3032 Explore P: 0.7831 RunMean : 27.4719\n",
      "Episode: 89 Total reward: 15.0 Training loss: 80.2092 Explore P: 0.7819 RunMean : 27.3333\n",
      "Episode: 90 Total reward: 66.0 Training loss: 89.5635 Explore P: 0.7768 RunMean : 27.7582\n",
      "Episode: 91 Total reward: 18.0 Training loss: 23.9295 Explore P: 0.7754 RunMean : 27.6522\n",
      "Episode: 92 Total reward: 12.0 Training loss: 130.4614 Explore P: 0.7745 RunMean : 27.4839\n",
      "Episode: 93 Total reward: 31.0 Training loss: 114.4566 Explore P: 0.7721 RunMean : 27.5213\n",
      "Episode: 94 Total reward: 20.0 Training loss: 9.7768 Explore P: 0.7705 RunMean : 27.4421\n",
      "Episode: 95 Total reward: 56.0 Training loss: 86.0167 Explore P: 0.7662 RunMean : 27.7396\n",
      "Episode: 96 Total reward: 38.0 Training loss: 38.1432 Explore P: 0.7633 RunMean : 27.8454\n",
      "Episode: 97 Total reward: 92.0 Training loss: 11.7421 Explore P: 0.7563 RunMean : 28.5000\n",
      "Episode: 98 Total reward: 12.0 Training loss: 115.4842 Explore P: 0.7554 RunMean : 28.3333\n",
      "Episode: 99 Total reward: 53.0 Training loss: 138.6193 Explore P: 0.7514 RunMean : 28.5800\n",
      "Episode: 100 Total reward: 66.0 Training loss: 95.0067 Explore P: 0.7465 RunMean : 29.0600\n",
      "Episode: 101 Total reward: 56.0 Training loss: 22.0751 Explore P: 0.7423 RunMean : 29.5100\n",
      "Episode: 102 Total reward: 18.0 Training loss: 131.6800 Explore P: 0.7410 RunMean : 29.5200\n",
      "Episode: 103 Total reward: 30.0 Training loss: 135.0603 Explore P: 0.7387 RunMean : 29.6900\n",
      "Episode: 104 Total reward: 85.0 Training loss: 136.4755 Explore P: 0.7325 RunMean : 30.4000\n",
      "Episode: 105 Total reward: 120.0 Training loss: 13.5266 Explore P: 0.7238 RunMean : 31.4300\n",
      "Episode: 106 Total reward: 41.0 Training loss: 358.0882 Explore P: 0.7208 RunMean : 31.5600\n",
      "Episode: 107 Total reward: 38.0 Training loss: 476.2263 Explore P: 0.7181 RunMean : 31.7700\n",
      "Episode: 108 Total reward: 50.0 Training loss: 271.0281 Explore P: 0.7145 RunMean : 32.0700\n",
      "Episode: 109 Total reward: 23.0 Training loss: 769.7764 Explore P: 0.7128 RunMean : 32.0500\n",
      "Episode: 110 Total reward: 38.0 Training loss: 107.6582 Explore P: 0.7101 RunMean : 32.2800\n",
      "Episode: 111 Total reward: 42.0 Training loss: 166.6652 Explore P: 0.7072 RunMean : 32.3300\n",
      "Episode: 112 Total reward: 125.0 Training loss: 24.3151 Explore P: 0.6984 RunMean : 33.4800\n",
      "Episode: 113 Total reward: 56.0 Training loss: 66.0118 Explore P: 0.6945 RunMean : 33.8800\n",
      "Episode: 114 Total reward: 74.0 Training loss: 206.4217 Explore P: 0.6894 RunMean : 34.4500\n",
      "Episode: 115 Total reward: 107.0 Training loss: 189.5283 Explore P: 0.6820 RunMean : 35.2000\n",
      "Episode: 116 Total reward: 108.0 Training loss: 296.8164 Explore P: 0.6747 RunMean : 36.1700\n",
      "Episode: 117 Total reward: 37.0 Training loss: 27.5295 Explore P: 0.6722 RunMean : 36.3800\n",
      "Episode: 118 Total reward: 74.0 Training loss: 87.0045 Explore P: 0.6672 RunMean : 36.9100\n",
      "Episode: 119 Total reward: 32.0 Training loss: 21.8751 Explore P: 0.6651 RunMean : 37.0000\n",
      "Episode: 120 Total reward: 39.0 Training loss: 432.9986 Explore P: 0.6625 RunMean : 37.0600\n",
      "Episode: 121 Total reward: 70.0 Training loss: 59.5378 Explore P: 0.6579 RunMean : 37.5500\n",
      "Episode: 122 Total reward: 137.0 Training loss: 137.3834 Explore P: 0.6489 RunMean : 38.6400\n",
      "Episode: 123 Total reward: 37.0 Training loss: 940.7988 Explore P: 0.6466 RunMean : 38.6600\n",
      "Episode: 124 Total reward: 38.0 Training loss: 235.6598 Explore P: 0.6441 RunMean : 38.8900\n",
      "Episode: 125 Total reward: 13.0 Training loss: 261.2519 Explore P: 0.6433 RunMean : 38.8100\n",
      "Episode: 126 Total reward: 13.0 Training loss: 144.5898 Explore P: 0.6424 RunMean : 38.8000\n",
      "Episode: 127 Total reward: 94.0 Training loss: 141.9230 Explore P: 0.6364 RunMean : 39.5400\n",
      "Episode: 128 Total reward: 15.0 Training loss: 49.1761 Explore P: 0.6355 RunMean : 39.4500\n",
      "Episode: 129 Total reward: 106.0 Training loss: 496.4590 Explore P: 0.6288 RunMean : 40.4000\n",
      "Episode: 130 Total reward: 15.0 Training loss: 124.6378 Explore P: 0.6278 RunMean : 40.4000\n",
      "Episode: 131 Total reward: 70.0 Training loss: 386.2652 Explore P: 0.6234 RunMean : 40.9800\n",
      "Episode: 132 Total reward: 103.0 Training loss: 991.8735 Explore P: 0.6171 RunMean : 41.8000\n",
      "Episode: 133 Total reward: 178.0 Training loss: 605.7697 Explore P: 0.6062 RunMean : 43.3800\n",
      "Episode: 134 Total reward: 51.0 Training loss: 65.9641 Explore P: 0.6031 RunMean : 43.4500\n",
      "Episode: 135 Total reward: 120.0 Training loss: 358.5672 Explore P: 0.5959 RunMean : 44.5000\n",
      "Episode: 136 Total reward: 65.0 Training loss: 1678.8857 Explore P: 0.5920 RunMean : 44.9800\n",
      "Episode: 137 Total reward: 192.0 Training loss: 711.4715 Explore P: 0.5808 RunMean : 46.7700\n",
      "Episode: 138 Total reward: 98.0 Training loss: 1094.0254 Explore P: 0.5751 RunMean : 47.5300\n",
      "Episode: 139 Total reward: 21.0 Training loss: 670.1309 Explore P: 0.5739 RunMean : 47.4200\n",
      "Episode: 140 Total reward: 23.0 Training loss: 400.3139 Explore P: 0.5726 RunMean : 47.4400\n",
      "Episode: 141 Total reward: 58.0 Training loss: 1615.9486 Explore P: 0.5693 RunMean : 47.8100\n",
      "Episode: 142 Total reward: 35.0 Training loss: 70.0867 Explore P: 0.5673 RunMean : 47.4800\n",
      "Episode: 143 Total reward: 67.0 Training loss: 743.1528 Explore P: 0.5635 RunMean : 47.9600\n",
      "Episode: 144 Total reward: 94.0 Training loss: 191.6973 Explore P: 0.5582 RunMean : 48.7300\n",
      "Episode: 145 Total reward: 125.0 Training loss: 1545.0769 Explore P: 0.5513 RunMean : 49.7400\n",
      "Episode: 146 Total reward: 150.0 Training loss: 697.5911 Explore P: 0.5431 RunMean : 51.0100\n",
      "Episode: 147 Total reward: 178.0 Training loss: 826.5486 Explore P: 0.5335 RunMean : 52.5100\n",
      "Episode: 148 Total reward: 91.0 Training loss: 909.0312 Explore P: 0.5287 RunMean : 53.1400\n",
      "Episode: 149 Total reward: 36.0 Training loss: 915.9777 Explore P: 0.5268 RunMean : 53.3400\n",
      "Episode: 150 Total reward: 200.0 Training loss: 1536.8677 Explore P: 0.5163 RunMean : 54.8900\n",
      "Episode: 151 Total reward: 165.0 Training loss: 243.5783 Explore P: 0.5079 RunMean : 56.0300\n",
      "Episode: 152 Total reward: 29.0 Training loss: 622.4373 Explore P: 0.5064 RunMean : 55.8500\n",
      "Episode: 153 Total reward: 148.0 Training loss: 157.0652 Explore P: 0.4990 RunMean : 56.4200\n",
      "Episode: 154 Total reward: 200.0 Training loss: 102.3581 Explore P: 0.4891 RunMean : 58.2700\n",
      "Episode: 155 Total reward: 51.0 Training loss: 1681.4811 Explore P: 0.4866 RunMean : 58.5300\n",
      "Episode: 156 Total reward: 152.0 Training loss: 1235.0541 Explore P: 0.4793 RunMean : 59.8500\n",
      "Episode: 157 Total reward: 149.0 Training loss: 524.1336 Explore P: 0.4722 RunMean : 60.9900\n",
      "Episode: 158 Total reward: 53.0 Training loss: 355.6386 Explore P: 0.4697 RunMean : 61.4000\n",
      "Episode: 159 Total reward: 33.0 Training loss: 770.6692 Explore P: 0.4681 RunMean : 61.2500\n",
      "Episode: 160 Total reward: 56.0 Training loss: 405.2015 Explore P: 0.4655 RunMean : 61.6100\n",
      "Episode: 161 Total reward: 200.0 Training loss: 1171.4464 Explore P: 0.4563 RunMean : 63.4300\n",
      "Episode: 162 Total reward: 200.0 Training loss: 234.5322 Explore P: 0.4473 RunMean : 65.2700\n",
      "Episode: 163 Total reward: 119.0 Training loss: 104.7572 Explore P: 0.4420 RunMean : 66.3500\n",
      "Episode: 164 Total reward: 200.0 Training loss: 4778.7744 Explore P: 0.4332 RunMean : 68.0000\n",
      "Episode: 165 Total reward: 147.0 Training loss: 701.5013 Explore P: 0.4269 RunMean : 69.3400\n",
      "Episode: 166 Total reward: 200.0 Training loss: 150.4518 Explore P: 0.4184 RunMean : 71.0400\n",
      "Episode: 167 Total reward: 168.0 Training loss: 112.4500 Explore P: 0.4115 RunMean : 72.4000\n",
      "Episode: 168 Total reward: 200.0 Training loss: 976.6042 Explore P: 0.4033 RunMean : 74.1000\n",
      "Episode: 169 Total reward: 93.0 Training loss: 1488.0935 Explore P: 0.3996 RunMean : 74.7100\n",
      "Episode: 170 Total reward: 178.0 Training loss: 860.5004 Explore P: 0.3925 RunMean : 75.8700\n",
      "Episode: 171 Total reward: 200.0 Training loss: 105.5258 Explore P: 0.3848 RunMean : 77.6900\n",
      "Episode: 172 Total reward: 200.0 Training loss: 461.4458 Explore P: 0.3772 RunMean : 79.4600\n",
      "Episode: 173 Total reward: 200.0 Training loss: 1210.4758 Explore P: 0.3697 RunMean : 80.9700\n",
      "Episode: 174 Total reward: 200.0 Training loss: 2149.4014 Explore P: 0.3624 RunMean : 82.8500\n",
      "Episode: 175 Total reward: 55.0 Training loss: 153.8754 Explore P: 0.3604 RunMean : 83.1500\n",
      "Episode: 176 Total reward: 200.0 Training loss: 1194.2722 Explore P: 0.3532 RunMean : 84.6300\n",
      "Episode: 177 Total reward: 200.0 Training loss: 528.7440 Explore P: 0.3462 RunMean : 86.5000\n",
      "Episode: 178 Total reward: 200.0 Training loss: 565.4997 Explore P: 0.3394 RunMean : 88.0300\n",
      "Episode: 179 Total reward: 200.0 Training loss: 70.1372 Explore P: 0.3327 RunMean : 89.9100\n",
      "Episode: 180 Total reward: 200.0 Training loss: 142.8103 Explore P: 0.3261 RunMean : 91.1300\n",
      "Episode: 181 Total reward: 200.0 Training loss: 62.9650 Explore P: 0.3196 RunMean : 92.7700\n",
      "Episode: 182 Total reward: 200.0 Training loss: 117.7940 Explore P: 0.3133 RunMean : 94.3000\n",
      "Episode: 183 Total reward: 200.0 Training loss: 714.1995 Explore P: 0.3071 RunMean : 96.0100\n",
      "Episode: 184 Total reward: 200.0 Training loss: 103.8494 Explore P: 0.3010 RunMean : 97.5600\n",
      "Episode: 185 Total reward: 200.0 Training loss: 1085.1606 Explore P: 0.2951 RunMean : 98.8600\n",
      "Episode: 186 Total reward: 200.0 Training loss: 527.6559 Explore P: 0.2892 RunMean : 100.6700\n",
      "Episode: 187 Total reward: 200.0 Training loss: 81.5769 Explore P: 0.2835 RunMean : 102.2000\n",
      "Episode: 188 Total reward: 200.0 Training loss: 2842.2351 Explore P: 0.2779 RunMean : 103.6100\n",
      "Episode: 189 Total reward: 200.0 Training loss: 338.4325 Explore P: 0.2724 RunMean : 105.4600\n",
      "Episode: 190 Total reward: 200.0 Training loss: 37.7304 Explore P: 0.2670 RunMean : 106.8000\n",
      "Episode: 191 Total reward: 200.0 Training loss: 48.8869 Explore P: 0.2617 RunMean : 108.6200\n",
      "Episode: 192 Total reward: 200.0 Training loss: 248.2348 Explore P: 0.2565 RunMean : 110.5000\n",
      "Episode: 193 Total reward: 200.0 Training loss: 67.0535 Explore P: 0.2514 RunMean : 112.1900\n",
      "Episode: 194 Total reward: 200.0 Training loss: 18.5374 Explore P: 0.2464 RunMean : 113.9900\n",
      "Episode: 195 Total reward: 200.0 Training loss: 39.4984 Explore P: 0.2416 RunMean : 115.4300\n",
      "Episode: 196 Total reward: 200.0 Training loss: 21.4407 Explore P: 0.2368 RunMean : 117.0500\n",
      "Episode: 197 Total reward: 200.0 Training loss: 19.9867 Explore P: 0.2321 RunMean : 118.1300\n",
      "Episode: 198 Total reward: 200.0 Training loss: 36.5899 Explore P: 0.2275 RunMean : 120.0100\n",
      "Episode: 199 Total reward: 200.0 Training loss: 12.0324 Explore P: 0.2230 RunMean : 121.4800\n",
      "Episode: 200 Total reward: 200.0 Training loss: 38.3548 Explore P: 0.2186 RunMean : 122.8200\n",
      "Episode: 201 Total reward: 200.0 Training loss: 17.6837 Explore P: 0.2143 RunMean : 124.2600\n",
      "Episode: 202 Total reward: 200.0 Training loss: 278.1195 Explore P: 0.2100 RunMean : 126.0800\n",
      "Episode: 203 Total reward: 200.0 Training loss: 9.1213 Explore P: 0.2059 RunMean : 127.7800\n",
      "Episode: 204 Total reward: 200.0 Training loss: 12.5584 Explore P: 0.2018 RunMean : 128.9300\n",
      "Episode: 205 Total reward: 200.0 Training loss: 237.2832 Explore P: 0.1978 RunMean : 129.7300\n",
      "Episode: 206 Total reward: 200.0 Training loss: 6.8299 Explore P: 0.1939 RunMean : 131.3200\n",
      "Episode: 207 Total reward: 200.0 Training loss: 4.1983 Explore P: 0.1900 RunMean : 132.9400\n",
      "Episode: 208 Total reward: 200.0 Training loss: 4.7108 Explore P: 0.1863 RunMean : 134.4400\n",
      "Episode: 209 Total reward: 200.0 Training loss: 9.4831 Explore P: 0.1826 RunMean : 136.2100\n",
      "Episode: 210 Total reward: 200.0 Training loss: 4.4108 Explore P: 0.1790 RunMean : 137.8300\n",
      "Episode: 211 Total reward: 200.0 Training loss: 3.4665 Explore P: 0.1754 RunMean : 139.4100\n",
      "Episode: 212 Total reward: 200.0 Training loss: 3.1973 Explore P: 0.1719 RunMean : 140.1600\n",
      "Episode: 213 Total reward: 200.0 Training loss: 97.4718 Explore P: 0.1685 RunMean : 141.6000\n",
      "Episode: 214 Total reward: 193.0 Training loss: 142.2660 Explore P: 0.1653 RunMean : 142.7900\n",
      "Episode: 215 Total reward: 182.0 Training loss: 68.7329 Explore P: 0.1623 RunMean : 143.5400\n",
      "Episode: 216 Total reward: 191.0 Training loss: 2.5449 Explore P: 0.1593 RunMean : 144.3700\n",
      "Episode: 217 Total reward: 200.0 Training loss: 2.5622 Explore P: 0.1561 RunMean : 146.0000\n",
      "Episode: 218 Total reward: 20.0 Training loss: 3.0472 Explore P: 0.1558 RunMean : 145.4600\n",
      "Episode: 219 Total reward: 200.0 Training loss: 4.5295 Explore P: 0.1527 RunMean : 147.1400\n",
      "Episode: 220 Total reward: 193.0 Training loss: 19.7587 Explore P: 0.1498 RunMean : 148.6800\n",
      "Episode: 221 Total reward: 200.0 Training loss: 9.0175 Explore P: 0.1468 RunMean : 149.9800\n",
      "Episode: 222 Total reward: 200.0 Training loss: 16.5015 Explore P: 0.1439 RunMean : 150.6100\n",
      "Episode: 223 Total reward: 200.0 Training loss: 2.0601 Explore P: 0.1411 RunMean : 152.2400\n",
      "Episode: 224 Total reward: 200.0 Training loss: 2.8424 Explore P: 0.1383 RunMean : 153.8600\n",
      "Episode: 225 Total reward: 200.0 Training loss: 1.4066 Explore P: 0.1355 RunMean : 155.7300\n",
      "Episode: 226 Total reward: 170.0 Training loss: 61.4541 Explore P: 0.1333 RunMean : 157.3000\n",
      "Episode: 227 Total reward: 200.0 Training loss: 6.8292 Explore P: 0.1306 RunMean : 158.3600\n",
      "Episode: 228 Total reward: 187.0 Training loss: 1.7582 Explore P: 0.1282 RunMean : 160.0800\n",
      "Episode: 229 Total reward: 41.0 Training loss: 6.8496 Explore P: 0.1277 RunMean : 159.4300\n",
      "Episode: 230 Total reward: 48.0 Training loss: 142.2004 Explore P: 0.1271 RunMean : 159.7600\n",
      "Episode: 231 Total reward: 41.0 Training loss: 5.6812 Explore P: 0.1265 RunMean : 159.4700\n",
      "Episode: 232 Total reward: 134.0 Training loss: 1201.8491 Explore P: 0.1249 RunMean : 159.7800\n",
      "Episode: 233 Total reward: 163.0 Training loss: 2.6297 Explore P: 0.1228 RunMean : 159.6300\n",
      "Episode: 234 Total reward: 165.0 Training loss: 3.2982 Explore P: 0.1208 RunMean : 160.7700\n",
      "Episode: 235 Total reward: 200.0 Training loss: 1307.7147 Explore P: 0.1184 RunMean : 161.5700\n",
      "Episode: 236 Total reward: 164.0 Training loss: 4645.0820 Explore P: 0.1165 RunMean : 162.5600\n",
      "Episode: 237 Total reward: 178.0 Training loss: 22.3652 Explore P: 0.1145 RunMean : 162.4200\n",
      "Episode: 238 Total reward: 200.0 Training loss: 13.5505 Explore P: 0.1122 RunMean : 163.4400\n",
      "Episode: 239 Total reward: 143.0 Training loss: 9.6434 Explore P: 0.1106 RunMean : 164.6600\n",
      "Episode: 240 Total reward: 197.0 Training loss: 18.0842 Explore P: 0.1084 RunMean : 166.4000\n",
      "Episode: 241 Total reward: 200.0 Training loss: 599.3937 Explore P: 0.1063 RunMean : 167.8200\n",
      "Episode: 242 Total reward: 200.0 Training loss: 7.2636 Explore P: 0.1042 RunMean : 169.4700\n",
      "Episode: 243 Total reward: 200.0 Training loss: 7.0390 Explore P: 0.1021 RunMean : 170.8000\n",
      "Episode: 244 Total reward: 200.0 Training loss: 4.9084 Explore P: 0.1001 RunMean : 171.8600\n",
      "Episode: 245 Total reward: 175.0 Training loss: 190.3254 Explore P: 0.0984 RunMean : 172.3600\n",
      "Episode: 246 Total reward: 200.0 Training loss: 5.3877 Explore P: 0.0964 RunMean : 172.8600\n",
      "Episode: 247 Total reward: 200.0 Training loss: 14.5348 Explore P: 0.0945 RunMean : 173.0800\n",
      "Episode: 248 Total reward: 200.0 Training loss: 4.3234 Explore P: 0.0926 RunMean : 174.1700\n",
      "Episode: 249 Total reward: 200.0 Training loss: 9.5000 Explore P: 0.0908 RunMean : 175.8100\n",
      "Episode: 250 Total reward: 200.0 Training loss: 7.6192 Explore P: 0.0890 RunMean : 175.8100\n",
      "Episode: 251 Total reward: 200.0 Training loss: 11.1985 Explore P: 0.0872 RunMean : 176.1600\n",
      "Episode: 252 Total reward: 168.0 Training loss: 9.9137 Explore P: 0.0858 RunMean : 177.5500\n",
      "Episode: 253 Total reward: 200.0 Training loss: 6.8532 Explore P: 0.0841 RunMean : 178.0700\n",
      "Episode: 254 Total reward: 200.0 Training loss: 14.2498 Explore P: 0.0824 RunMean : 178.0700\n",
      "Episode: 255 Total reward: 200.0 Training loss: 24.6079 Explore P: 0.0808 RunMean : 179.5600\n",
      "Episode: 256 Total reward: 200.0 Training loss: 5.6411 Explore P: 0.0792 RunMean : 180.0400\n",
      "Episode: 257 Total reward: 187.0 Training loss: 3.0529 Explore P: 0.0777 RunMean : 180.4200\n",
      "Episode: 258 Total reward: 182.0 Training loss: 2.5676 Explore P: 0.0763 RunMean : 181.7100\n",
      "Episode: 259 Total reward: 171.0 Training loss: 20.5576 Explore P: 0.0750 RunMean : 183.0900\n",
      "Episode: 260 Total reward: 200.0 Training loss: 3.6166 Explore P: 0.0735 RunMean : 184.5300\n",
      "Episode: 261 Total reward: 200.0 Training loss: 3.0754 Explore P: 0.0721 RunMean : 184.5300\n",
      "Episode: 262 Total reward: 178.0 Training loss: 3.5629 Explore P: 0.0708 RunMean : 184.3100\n",
      "Episode: 263 Total reward: 200.0 Training loss: 73.9953 Explore P: 0.0694 RunMean : 185.1200\n",
      "Episode: 264 Total reward: 12.0 Training loss: 194.7737 Explore P: 0.0693 RunMean : 183.2400\n",
      "Episode: 265 Total reward: 12.0 Training loss: 148.5598 Explore P: 0.0692 RunMean : 181.8900\n",
      "Episode: 266 Total reward: 142.0 Training loss: 12619.7617 Explore P: 0.0683 RunMean : 181.3100\n",
      "Episode: 267 Total reward: 11.0 Training loss: 158.4167 Explore P: 0.0682 RunMean : 179.7400\n",
      "Episode: 268 Total reward: 12.0 Training loss: 198.8409 Explore P: 0.0681 RunMean : 177.8600\n",
      "Episode: 269 Total reward: 13.0 Training loss: 2154.2820 Explore P: 0.0680 RunMean : 177.0600\n",
      "Episode: 270 Total reward: 162.0 Training loss: 2297.7341 Explore P: 0.0669 RunMean : 176.9000\n",
      "Episode: 271 Total reward: 32.0 Training loss: 15494.2715 Explore P: 0.0667 RunMean : 175.2200\n",
      "Episode: 272 Total reward: 14.0 Training loss: 7779.0181 Explore P: 0.0666 RunMean : 173.3600\n",
      "Episode: 273 Total reward: 15.0 Training loss: 41.5481 Explore P: 0.0665 RunMean : 171.5100\n",
      "Episode: 274 Total reward: 17.0 Training loss: 16126.3184 Explore P: 0.0664 RunMean : 169.6800\n",
      "Episode: 275 Total reward: 19.0 Training loss: 72.5798 Explore P: 0.0663 RunMean : 169.3200\n",
      "Episode: 276 Total reward: 19.0 Training loss: 6354.0742 Explore P: 0.0662 RunMean : 167.5100\n",
      "Episode: 277 Total reward: 15.0 Training loss: 6047.6772 Explore P: 0.0661 RunMean : 165.6600\n",
      "Episode: 278 Total reward: 12.0 Training loss: 38.0057 Explore P: 0.0660 RunMean : 163.7800\n",
      "Episode: 279 Total reward: 12.0 Training loss: 52.9385 Explore P: 0.0659 RunMean : 161.9000\n",
      "Episode: 280 Total reward: 14.0 Training loss: 14642.2285 Explore P: 0.0658 RunMean : 160.0400\n",
      "Episode: 281 Total reward: 14.0 Training loss: 5947.8594 Explore P: 0.0657 RunMean : 158.1800\n",
      "Episode: 282 Total reward: 12.0 Training loss: 5352.7173 Explore P: 0.0656 RunMean : 156.3000\n",
      "Episode: 283 Total reward: 11.0 Training loss: 67.9899 Explore P: 0.0656 RunMean : 154.4100\n",
      "Episode: 284 Total reward: 13.0 Training loss: 7169.1704 Explore P: 0.0655 RunMean : 152.5400\n",
      "Episode: 285 Total reward: 14.0 Training loss: 3574.6565 Explore P: 0.0654 RunMean : 150.6800\n",
      "Episode: 286 Total reward: 19.0 Training loss: 1909.0175 Explore P: 0.0653 RunMean : 148.8700\n",
      "Episode: 287 Total reward: 14.0 Training loss: 37.1936 Explore P: 0.0652 RunMean : 147.0100\n",
      "Episode: 288 Total reward: 15.0 Training loss: 7030.0815 Explore P: 0.0651 RunMean : 145.1600\n",
      "Episode: 289 Total reward: 16.0 Training loss: 79.0676 Explore P: 0.0650 RunMean : 143.3200\n",
      "Episode: 290 Total reward: 18.0 Training loss: 5171.7896 Explore P: 0.0649 RunMean : 141.5000\n",
      "Episode: 291 Total reward: 26.0 Training loss: 2832.7222 Explore P: 0.0647 RunMean : 139.7600\n",
      "Episode: 292 Total reward: 30.0 Training loss: 18.9717 Explore P: 0.0645 RunMean : 138.0600\n",
      "Episode: 293 Total reward: 30.0 Training loss: 93.8849 Explore P: 0.0643 RunMean : 136.3600\n",
      "Episode: 294 Total reward: 33.0 Training loss: 6245.1035 Explore P: 0.0641 RunMean : 134.6900\n",
      "Episode: 295 Total reward: 63.0 Training loss: 1889.2583 Explore P: 0.0637 RunMean : 133.3200\n",
      "Episode: 296 Total reward: 167.0 Training loss: 63.5300 Explore P: 0.0626 RunMean : 132.9900\n",
      "Episode: 297 Total reward: 200.0 Training loss: 31.2393 Explore P: 0.0614 RunMean : 132.9900\n",
      "Episode: 298 Total reward: 200.0 Training loss: 334.5709 Explore P: 0.0602 RunMean : 132.9900\n",
      "Episode: 299 Total reward: 200.0 Training loss: 73.8430 Explore P: 0.0590 RunMean : 132.9900\n",
      "Episode: 300 Total reward: 200.0 Training loss: 122.3489 Explore P: 0.0578 RunMean : 132.9900\n",
      "Episode: 301 Total reward: 200.0 Training loss: 26.6937 Explore P: 0.0567 RunMean : 132.9900\n",
      "Episode: 302 Total reward: 200.0 Training loss: 31.5777 Explore P: 0.0555 RunMean : 132.9900\n",
      "Episode: 303 Total reward: 200.0 Training loss: 42.7104 Explore P: 0.0544 RunMean : 132.9900\n",
      "Episode: 304 Total reward: 200.0 Training loss: 66.0251 Explore P: 0.0534 RunMean : 132.9900\n",
      "Episode: 305 Total reward: 200.0 Training loss: 43.4003 Explore P: 0.0523 RunMean : 132.9900\n",
      "Episode: 306 Total reward: 200.0 Training loss: 34.3689 Explore P: 0.0513 RunMean : 132.9900\n",
      "Episode: 307 Total reward: 189.0 Training loss: 15.2998 Explore P: 0.0503 RunMean : 132.8800\n",
      "Episode: 308 Total reward: 200.0 Training loss: 37.9041 Explore P: 0.0493 RunMean : 132.8800\n",
      "Episode: 309 Total reward: 176.0 Training loss: 32.3561 Explore P: 0.0485 RunMean : 132.6400\n",
      "Episode: 310 Total reward: 200.0 Training loss: 12.8085 Explore P: 0.0475 RunMean : 132.6400\n",
      "Episode: 311 Total reward: 200.0 Training loss: 23.5822 Explore P: 0.0466 RunMean : 132.6400\n",
      "Episode: 312 Total reward: 200.0 Training loss: 26.2657 Explore P: 0.0456 RunMean : 132.6400\n",
      "Episode: 313 Total reward: 200.0 Training loss: 56.5404 Explore P: 0.0447 RunMean : 132.6400\n",
      "Episode: 314 Total reward: 200.0 Training loss: 16.1546 Explore P: 0.0438 RunMean : 132.7100\n",
      "Episode: 315 Total reward: 200.0 Training loss: 13.1475 Explore P: 0.0430 RunMean : 132.8900\n",
      "Episode: 316 Total reward: 200.0 Training loss: 23.3289 Explore P: 0.0421 RunMean : 132.9800\n",
      "Episode: 317 Total reward: 200.0 Training loss: 444.7524 Explore P: 0.0413 RunMean : 132.9800\n",
      "Episode: 318 Total reward: 164.0 Training loss: 1046.7406 Explore P: 0.0406 RunMean : 134.4200\n",
      "Episode: 319 Total reward: 159.0 Training loss: 2842.7202 Explore P: 0.0400 RunMean : 134.0100\n",
      "Episode: 320 Total reward: 148.0 Training loss: 167.7127 Explore P: 0.0394 RunMean : 133.5600\n",
      "Episode: 321 Total reward: 140.0 Training loss: 9215.4844 Explore P: 0.0388 RunMean : 132.9600\n",
      "Episode: 322 Total reward: 139.0 Training loss: 6629.9756 Explore P: 0.0383 RunMean : 132.3500\n",
      "Episode: 323 Total reward: 123.0 Training loss: 26677.3555 Explore P: 0.0378 RunMean : 131.5800\n",
      "Episode: 324 Total reward: 144.0 Training loss: 291.6861 Explore P: 0.0373 RunMean : 131.0200\n",
      "Episode: 325 Total reward: 149.0 Training loss: 317.0569 Explore P: 0.0368 RunMean : 130.5100\n",
      "Episode: 326 Total reward: 177.0 Training loss: 10868.0225 Explore P: 0.0361 RunMean : 130.5800\n",
      "Episode: 327 Total reward: 179.0 Training loss: 6310.1523 Explore P: 0.0355 RunMean : 130.3700\n",
      "Episode: 328 Total reward: 200.0 Training loss: 6981.5894 Explore P: 0.0348 RunMean : 130.5000\n",
      "Episode: 329 Total reward: 200.0 Training loss: 2279.0249 Explore P: 0.0341 RunMean : 132.0900\n",
      "Episode: 330 Total reward: 200.0 Training loss: 305.7375 Explore P: 0.0334 RunMean : 133.6100\n",
      "Episode: 331 Total reward: 200.0 Training loss: 133.4977 Explore P: 0.0327 RunMean : 135.2000\n",
      "Episode: 332 Total reward: 200.0 Training loss: 149.8156 Explore P: 0.0321 RunMean : 135.8600\n",
      "Episode: 333 Total reward: 200.0 Training loss: 1016.7480 Explore P: 0.0315 RunMean : 136.2300\n",
      "Episode: 334 Total reward: 200.0 Training loss: 113.1747 Explore P: 0.0308 RunMean : 136.5800\n",
      "Episode: 335 Total reward: 200.0 Training loss: 114.0760 Explore P: 0.0302 RunMean : 136.5800\n",
      "Episode: 336 Total reward: 200.0 Training loss: 108.4523 Explore P: 0.0296 RunMean : 136.9400\n",
      "Episode: 337 Total reward: 200.0 Training loss: 53.7008 Explore P: 0.0290 RunMean : 137.1600\n",
      "Episode: 338 Total reward: 200.0 Training loss: 60.7448 Explore P: 0.0285 RunMean : 137.1600\n",
      "Episode: 339 Total reward: 200.0 Training loss: 1035.1061 Explore P: 0.0279 RunMean : 137.7300\n",
      "Episode: 340 Total reward: 200.0 Training loss: 53.2565 Explore P: 0.0273 RunMean : 137.7600\n",
      "Episode: 341 Total reward: 200.0 Training loss: 89.4060 Explore P: 0.0268 RunMean : 137.7600\n",
      "Episode: 342 Total reward: 200.0 Training loss: 93.7685 Explore P: 0.0263 RunMean : 137.7600\n",
      "Episode: 343 Total reward: 200.0 Training loss: 2488.3972 Explore P: 0.0258 RunMean : 137.7600\n",
      "Episode: 344 Total reward: 200.0 Training loss: 1993.5586 Explore P: 0.0252 RunMean : 137.7600\n",
      "Episode: 345 Total reward: 200.0 Training loss: 1957.6909 Explore P: 0.0247 RunMean : 138.0100\n",
      "Episode: 346 Total reward: 200.0 Training loss: 79.1643 Explore P: 0.0243 RunMean : 138.0100\n",
      "Episode: 347 Total reward: 200.0 Training loss: 61.9647 Explore P: 0.0238 RunMean : 138.0100\n",
      "Episode: 348 Total reward: 199.0 Training loss: 52.1062 Explore P: 0.0233 RunMean : 138.0000\n",
      "Episode: 349 Total reward: 200.0 Training loss: 30.7050 Explore P: 0.0228 RunMean : 138.0000\n",
      "Episode: 350 Total reward: 200.0 Training loss: 34.7363 Explore P: 0.0224 RunMean : 138.0000\n",
      "Episode: 351 Total reward: 200.0 Training loss: 40.0020 Explore P: 0.0219 RunMean : 138.0000\n",
      "Episode: 352 Total reward: 200.0 Training loss: 29.7809 Explore P: 0.0215 RunMean : 138.3200\n",
      "Episode: 353 Total reward: 200.0 Training loss: 1510.6571 Explore P: 0.0211 RunMean : 138.3200\n",
      "Episode: 354 Total reward: 200.0 Training loss: 35.0586 Explore P: 0.0207 RunMean : 138.3200\n",
      "Episode: 355 Total reward: 200.0 Training loss: 69.9832 Explore P: 0.0203 RunMean : 138.3200\n",
      "Episode: 356 Total reward: 187.0 Training loss: 27.5668 Explore P: 0.0199 RunMean : 138.1900\n",
      "Episode: 357 Total reward: 200.0 Training loss: 34.7690 Explore P: 0.0195 RunMean : 138.3200\n",
      "Episode: 358 Total reward: 200.0 Training loss: 992.3911 Explore P: 0.0191 RunMean : 138.5000\n",
      "Episode: 359 Total reward: 200.0 Training loss: 18.5413 Explore P: 0.0187 RunMean : 138.7900\n",
      "Episode: 360 Total reward: 200.0 Training loss: 149.1168 Explore P: 0.0184 RunMean : 138.7900\n",
      "Episode: 361 Total reward: 200.0 Training loss: 24.5088 Explore P: 0.0180 RunMean : 138.7900\n",
      "Episode: 362 Total reward: 200.0 Training loss: 12.8543 Explore P: 0.0176 RunMean : 139.0100\n",
      "Episode: 363 Total reward: 167.0 Training loss: 22.8448 Explore P: 0.0173 RunMean : 138.6800\n",
      "Episode: 364 Total reward: 161.0 Training loss: 1673.8771 Explore P: 0.0171 RunMean : 140.1700\n",
      "Episode: 365 Total reward: 200.0 Training loss: 15.1566 Explore P: 0.0167 RunMean : 142.0500\n",
      "Episode: 366 Total reward: 199.0 Training loss: 19.0580 Explore P: 0.0164 RunMean : 142.6200\n",
      "Episode: 367 Total reward: 200.0 Training loss: 19.7018 Explore P: 0.0161 RunMean : 144.5100\n",
      "Episode: 368 Total reward: 200.0 Training loss: 9.8563 Explore P: 0.0158 RunMean : 146.3900\n",
      "Episode: 369 Total reward: 200.0 Training loss: 9.8830 Explore P: 0.0154 RunMean : 148.2600\n",
      "Episode: 370 Total reward: 200.0 Training loss: 896.9227 Explore P: 0.0151 RunMean : 148.6400\n",
      "Episode: 371 Total reward: 143.0 Training loss: 19.5967 Explore P: 0.0149 RunMean : 149.7500\n",
      "Episode: 372 Total reward: 128.0 Training loss: 471.5793 Explore P: 0.0147 RunMean : 150.8900\n",
      "Episode: 373 Total reward: 79.0 Training loss: 38.9000 Explore P: 0.0146 RunMean : 151.5300\n",
      "Episode: 374 Total reward: 92.0 Training loss: 23.8098 Explore P: 0.0145 RunMean : 152.2800\n",
      "Episode: 375 Total reward: 14.0 Training loss: 7249.4185 Explore P: 0.0145 RunMean : 152.2300\n",
      "Episode: 376 Total reward: 117.0 Training loss: 3006.0278 Explore P: 0.0143 RunMean : 153.2100\n",
      "Episode: 377 Total reward: 130.0 Training loss: 30.1495 Explore P: 0.0141 RunMean : 154.3600\n",
      "Episode: 378 Total reward: 200.0 Training loss: 21.7581 Explore P: 0.0138 RunMean : 156.2400\n",
      "Episode: 379 Total reward: 182.0 Training loss: 15.0294 Explore P: 0.0136 RunMean : 157.9400\n",
      "Episode: 380 Total reward: 200.0 Training loss: 23.6191 Explore P: 0.0133 RunMean : 159.8000\n",
      "Episode: 381 Total reward: 188.0 Training loss: 17.5403 Explore P: 0.0131 RunMean : 161.5400\n",
      "Episode: 382 Total reward: 200.0 Training loss: 5.2505 Explore P: 0.0128 RunMean : 163.4200\n",
      "Episode: 383 Total reward: 200.0 Training loss: 15.5094 Explore P: 0.0126 RunMean : 165.3100\n",
      "Episode: 384 Total reward: 200.0 Training loss: 11.2154 Explore P: 0.0123 RunMean : 167.1800\n",
      "Episode: 385 Total reward: 187.0 Training loss: 360.4696 Explore P: 0.0121 RunMean : 168.9100\n",
      "Episode: 386 Total reward: 160.0 Training loss: 35.3017 Explore P: 0.0119 RunMean : 170.3200\n",
      "Episode: 387 Total reward: 146.0 Training loss: 7.0839 Explore P: 0.0117 RunMean : 171.6400\n",
      "Episode: 388 Total reward: 157.0 Training loss: 7.3157 Explore P: 0.0115 RunMean : 173.0600\n",
      "Episode: 389 Total reward: 200.0 Training loss: 8.5077 Explore P: 0.0113 RunMean : 174.9000\n",
      "Episode: 390 Total reward: 194.0 Training loss: 36.7746 Explore P: 0.0111 RunMean : 176.6600\n",
      "Episode: 391 Total reward: 182.0 Training loss: 5.8650 Explore P: 0.0109 RunMean : 178.2200\n",
      "Episode: 392 Total reward: 200.0 Training loss: 7.9656 Explore P: 0.0107 RunMean : 179.9200\n",
      "Episode: 393 Total reward: 200.0 Training loss: 5.7996 Explore P: 0.0105 RunMean : 181.6200\n",
      "Episode: 394 Total reward: 200.0 Training loss: 180.9993 Explore P: 0.0103 RunMean : 183.2900\n",
      "Episode: 395 Total reward: 200.0 Training loss: 16.6530 Explore P: 0.0100 RunMean : 184.6600\n",
      "Episode: 396 Total reward: 200.0 Training loss: 7.6939 Explore P: 0.0098 RunMean : 184.9900\n",
      "Episode: 397 Total reward: 200.0 Training loss: 9.1720 Explore P: 0.0097 RunMean : 184.9900\n",
      "Episode: 398 Total reward: 200.0 Training loss: 9.4565 Explore P: 0.0095 RunMean : 184.9900\n",
      "Episode: 399 Total reward: 178.0 Training loss: 7.4579 Explore P: 0.0093 RunMean : 184.7700\n",
      "Episode: 400 Total reward: 200.0 Training loss: 11.4264 Explore P: 0.0091 RunMean : 184.7700\n",
      "Episode: 401 Total reward: 200.0 Training loss: 72.6458 Explore P: 0.0089 RunMean : 184.7700\n",
      "Episode: 402 Total reward: 192.0 Training loss: 8.0871 Explore P: 0.0088 RunMean : 184.6900\n",
      "Episode: 403 Total reward: 200.0 Training loss: 7.1818 Explore P: 0.0086 RunMean : 184.6900\n",
      "Episode: 404 Total reward: 167.0 Training loss: 4.7975 Explore P: 0.0084 RunMean : 184.3600\n",
      "Episode: 405 Total reward: 200.0 Training loss: 12.3458 Explore P: 0.0083 RunMean : 184.3600\n",
      "Episode: 406 Total reward: 200.0 Training loss: 6.8152 Explore P: 0.0081 RunMean : 184.3600\n",
      "Episode: 407 Total reward: 200.0 Training loss: 126.1375 Explore P: 0.0080 RunMean : 184.4700\n",
      "Episode: 408 Total reward: 200.0 Training loss: 147.6619 Explore P: 0.0078 RunMean : 184.4700\n",
      "Episode: 409 Total reward: 200.0 Training loss: 432.3317 Explore P: 0.0076 RunMean : 184.7100\n",
      "Episode: 410 Total reward: 200.0 Training loss: 6.2884 Explore P: 0.0075 RunMean : 184.7100\n",
      "Episode: 411 Total reward: 200.0 Training loss: 8.0208 Explore P: 0.0073 RunMean : 184.7100\n",
      "Episode: 412 Total reward: 200.0 Training loss: 13.7745 Explore P: 0.0072 RunMean : 184.7100\n",
      "Episode: 413 Total reward: 200.0 Training loss: 11.4377 Explore P: 0.0071 RunMean : 184.7100\n",
      "Episode: 414 Total reward: 200.0 Training loss: 10.4220 Explore P: 0.0069 RunMean : 184.7100\n",
      "Episode: 415 Total reward: 200.0 Training loss: 33.9699 Explore P: 0.0068 RunMean : 184.7100\n",
      "Episode: 416 Total reward: 133.0 Training loss: 9.6245 Explore P: 0.0067 RunMean : 184.0400\n",
      "Episode: 417 Total reward: 200.0 Training loss: 209.9394 Explore P: 0.0066 RunMean : 184.0400\n",
      "Episode: 418 Total reward: 200.0 Training loss: 7.9487 Explore P: 0.0064 RunMean : 184.4000\n",
      "Episode: 419 Total reward: 161.0 Training loss: 106.1868 Explore P: 0.0063 RunMean : 184.4200\n",
      "Episode: 420 Total reward: 155.0 Training loss: 4.3940 Explore P: 0.0062 RunMean : 184.4900\n",
      "Episode: 421 Total reward: 200.0 Training loss: 66.4740 Explore P: 0.0061 RunMean : 185.0900\n",
      "Episode: 422 Total reward: 200.0 Training loss: 5.1507 Explore P: 0.0060 RunMean : 185.7000\n",
      "Episode: 423 Total reward: 200.0 Training loss: 11.2423 Explore P: 0.0059 RunMean : 186.4700\n",
      "Episode: 424 Total reward: 140.0 Training loss: 6.1062 Explore P: 0.0058 RunMean : 186.4300\n",
      "Episode: 425 Total reward: 173.0 Training loss: 6.7225 Explore P: 0.0057 RunMean : 186.6700\n",
      "Episode: 426 Total reward: 186.0 Training loss: 5.8279 Explore P: 0.0056 RunMean : 186.7600\n",
      "Episode: 427 Total reward: 158.0 Training loss: 6.3759 Explore P: 0.0055 RunMean : 186.5500\n",
      "Episode: 428 Total reward: 200.0 Training loss: 4.7404 Explore P: 0.0054 RunMean : 186.5500\n",
      "Episode: 429 Total reward: 200.0 Training loss: 3.9333 Explore P: 0.0053 RunMean : 186.5500\n",
      "Episode: 430 Total reward: 200.0 Training loss: 7.2178 Explore P: 0.0052 RunMean : 186.5500\n",
      "Episode: 431 Total reward: 200.0 Training loss: 3.7438 Explore P: 0.0051 RunMean : 186.5500\n",
      "Episode: 432 Total reward: 124.0 Training loss: 5.6157 Explore P: 0.0050 RunMean : 185.7900\n",
      "Episode: 433 Total reward: 10.0 Training loss: 4.1691 Explore P: 0.0050 RunMean : 183.8900\n",
      "Episode: 434 Total reward: 179.0 Training loss: 87.1141 Explore P: 0.0049 RunMean : 183.6800\n",
      "Episode: 435 Total reward: 11.0 Training loss: 5080.6807 Explore P: 0.0049 RunMean : 181.7900\n",
      "Episode: 436 Total reward: 10.0 Training loss: 101.7375 Explore P: 0.0049 RunMean : 179.8900\n",
      "Episode: 437 Total reward: 9.0 Training loss: 120.6229 Explore P: 0.0049 RunMean : 177.9800\n",
      "Episode: 438 Total reward: 11.0 Training loss: 174.4408 Explore P: 0.0049 RunMean : 176.0900\n",
      "Episode: 439 Total reward: 9.0 Training loss: 2045.2438 Explore P: 0.0049 RunMean : 174.1800\n",
      "Episode: 440 Total reward: 10.0 Training loss: 2689.3394 Explore P: 0.0049 RunMean : 172.2800\n",
      "Episode: 441 Total reward: 9.0 Training loss: 278.1476 Explore P: 0.0049 RunMean : 170.3700\n",
      "Episode: 442 Total reward: 8.0 Training loss: 5063.4316 Explore P: 0.0049 RunMean : 168.4500\n",
      "Episode: 443 Total reward: 10.0 Training loss: 9607.4492 Explore P: 0.0049 RunMean : 166.5500\n",
      "Episode: 444 Total reward: 10.0 Training loss: 308.1340 Explore P: 0.0049 RunMean : 164.6500\n",
      "Episode: 445 Total reward: 9.0 Training loss: 240.4724 Explore P: 0.0049 RunMean : 162.7400\n",
      "Episode: 446 Total reward: 9.0 Training loss: 17745.5605 Explore P: 0.0049 RunMean : 160.8300\n",
      "Episode: 447 Total reward: 10.0 Training loss: 15134.9336 Explore P: 0.0049 RunMean : 158.9300\n",
      "Episode: 448 Total reward: 12.0 Training loss: 1298.5889 Explore P: 0.0048 RunMean : 157.0600\n",
      "Episode: 449 Total reward: 10.0 Training loss: 10530.5312 Explore P: 0.0048 RunMean : 155.1600\n",
      "Episode: 450 Total reward: 12.0 Training loss: 193.3132 Explore P: 0.0048 RunMean : 153.2800\n",
      "Episode: 451 Total reward: 12.0 Training loss: 136.8082 Explore P: 0.0048 RunMean : 151.4000\n",
      "Episode: 452 Total reward: 14.0 Training loss: 127.1069 Explore P: 0.0048 RunMean : 149.5400\n",
      "Episode: 453 Total reward: 12.0 Training loss: 126.7400 Explore P: 0.0048 RunMean : 147.6600\n",
      "Episode: 454 Total reward: 13.0 Training loss: 193.8479 Explore P: 0.0048 RunMean : 145.7900\n",
      "Episode: 455 Total reward: 35.0 Training loss: 165.9700 Explore P: 0.0048 RunMean : 144.1400\n",
      "Episode: 456 Total reward: 43.0 Training loss: 94.3203 Explore P: 0.0048 RunMean : 142.7000\n",
      "Episode: 457 Total reward: 74.0 Training loss: 4640.3921 Explore P: 0.0047 RunMean : 141.4400\n",
      "Episode: 458 Total reward: 81.0 Training loss: 135.8872 Explore P: 0.0047 RunMean : 140.2500\n",
      "Episode: 459 Total reward: 200.0 Training loss: 574.1745 Explore P: 0.0046 RunMean : 140.2500\n",
      "Episode: 460 Total reward: 133.0 Training loss: 161.2666 Explore P: 0.0045 RunMean : 139.5800\n",
      "Episode: 461 Total reward: 167.0 Training loss: 151.2941 Explore P: 0.0045 RunMean : 139.2500\n",
      "Episode: 462 Total reward: 157.0 Training loss: 1818.1439 Explore P: 0.0044 RunMean : 138.8200\n",
      "Episode: 463 Total reward: 163.0 Training loss: 205.2445 Explore P: 0.0043 RunMean : 138.7800\n",
      "Episode: 464 Total reward: 171.0 Training loss: 146.8141 Explore P: 0.0043 RunMean : 138.8800\n",
      "Episode: 465 Total reward: 200.0 Training loss: 209.8804 Explore P: 0.0042 RunMean : 138.8800\n",
      "Episode: 466 Total reward: 200.0 Training loss: 406.7407 Explore P: 0.0041 RunMean : 138.8900\n",
      "Episode: 467 Total reward: 200.0 Training loss: 136.0037 Explore P: 0.0040 RunMean : 138.8900\n",
      "Episode: 468 Total reward: 200.0 Training loss: 2177.4678 Explore P: 0.0039 RunMean : 138.8900\n",
      "Episode: 469 Total reward: 197.0 Training loss: 188.0074 Explore P: 0.0039 RunMean : 138.8600\n",
      "Episode: 470 Total reward: 200.0 Training loss: 88.0035 Explore P: 0.0038 RunMean : 138.8600\n",
      "Episode: 471 Total reward: 162.0 Training loss: 112.2626 Explore P: 0.0037 RunMean : 139.0500\n",
      "Episode: 472 Total reward: 200.0 Training loss: 140.4216 Explore P: 0.0036 RunMean : 139.7700\n",
      "Episode: 473 Total reward: 200.0 Training loss: 3880.3611 Explore P: 0.0036 RunMean : 140.9800\n",
      "Episode: 474 Total reward: 200.0 Training loss: 280.1893 Explore P: 0.0035 RunMean : 142.0600\n",
      "Episode: 475 Total reward: 200.0 Training loss: 226.9929 Explore P: 0.0034 RunMean : 143.9200\n",
      "Episode: 476 Total reward: 200.0 Training loss: 228.2379 Explore P: 0.0034 RunMean : 144.7500\n",
      "Episode: 477 Total reward: 155.0 Training loss: 84.8101 Explore P: 0.0033 RunMean : 145.0000\n",
      "Episode: 478 Total reward: 200.0 Training loss: 66.2454 Explore P: 0.0032 RunMean : 145.0000\n",
      "Episode: 479 Total reward: 192.0 Training loss: 63.4538 Explore P: 0.0032 RunMean : 145.1000\n",
      "Episode: 480 Total reward: 200.0 Training loss: 131.8575 Explore P: 0.0031 RunMean : 145.1000\n",
      "Episode: 481 Total reward: 156.0 Training loss: 123.2280 Explore P: 0.0031 RunMean : 144.7800\n",
      "Episode: 482 Total reward: 200.0 Training loss: 357.9794 Explore P: 0.0030 RunMean : 144.7800\n",
      "Episode: 483 Total reward: 200.0 Training loss: 594.7640 Explore P: 0.0030 RunMean : 144.7800\n",
      "Episode: 484 Total reward: 191.0 Training loss: 111.0892 Explore P: 0.0029 RunMean : 144.6900\n",
      "Episode: 485 Total reward: 200.0 Training loss: 477.6303 Explore P: 0.0028 RunMean : 144.8200\n",
      "Episode: 486 Total reward: 200.0 Training loss: 100.2061 Explore P: 0.0028 RunMean : 145.2200\n",
      "Episode: 487 Total reward: 200.0 Training loss: 1569.0776 Explore P: 0.0027 RunMean : 145.7600\n",
      "Episode: 488 Total reward: 178.0 Training loss: 126.0265 Explore P: 0.0027 RunMean : 145.9700\n",
      "Episode: 489 Total reward: 171.0 Training loss: 881.2190 Explore P: 0.0026 RunMean : 145.6800\n",
      "Episode: 490 Total reward: 200.0 Training loss: 2577.4866 Explore P: 0.0026 RunMean : 145.7400\n",
      "Episode: 491 Total reward: 179.0 Training loss: 1072.6664 Explore P: 0.0025 RunMean : 145.7100\n",
      "Episode: 492 Total reward: 200.0 Training loss: 61.1987 Explore P: 0.0025 RunMean : 145.7100\n",
      "Episode: 493 Total reward: 200.0 Training loss: 52.6404 Explore P: 0.0024 RunMean : 145.7100\n",
      "Episode: 494 Total reward: 200.0 Training loss: 1647.8966 Explore P: 0.0024 RunMean : 145.7100\n",
      "Episode: 495 Total reward: 200.0 Training loss: 49.7070 Explore P: 0.0023 RunMean : 145.7100\n",
      "Episode: 496 Total reward: 200.0 Training loss: 3024.8425 Explore P: 0.0023 RunMean : 145.7100\n",
      "Episode: 497 Total reward: 200.0 Training loss: 56.2087 Explore P: 0.0022 RunMean : 145.7100\n",
      "Episode: 498 Total reward: 200.0 Training loss: 42.0507 Explore P: 0.0022 RunMean : 145.7100\n",
      "Episode: 499 Total reward: 200.0 Training loss: 76.3396 Explore P: 0.0022 RunMean : 145.9300\n",
      "average training reward =  122.75\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 23:49:10,457] Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test reward =  199.99999999999292\n",
      "test= 199.99999999999292\n",
      "122.75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2837c036908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gym import wrappers\n",
    "    \n",
    "env = gym.make('CartPole-v0')\n",
    "#env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1',force=True)\n",
    "\n",
    "test,train, mainQN, saver = test_and_train_qnetwork(memory_size=10000,\\\n",
    "                                     train_episodes=500,\\\n",
    "                                           gamma=0.99,\\\n",
    "                                           explore_start=1.,\\\n",
    "                                           explore_stop=0.0,\\\n",
    "                                           decay_rate=0.0001,\\\n",
    "                                           hidden_layers=1,\\\n",
    "                                           hidden_size=32,\\\n",
    "                                           learning_rate=0.004,\\\n",
    "                                           batch_size=64,\\\n",
    "                                           alpha=0.1,\\\n",
    "                                           verbose=True)\n",
    "print('test=',str(test))\n",
    "print(train)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 23:50:18,379] Making new env: CartPole-v0\n",
      "[2017-05-22 23:50:18,382] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 23:50:18,392] Restoring parameters from checkpoints\\cartpole.ckpt\n",
      "[2017-05-22 23:50:18,484] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000000.mp4\n",
      "[2017-05-22 23:50:21,927] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000001.mp4\n",
      "[2017-05-22 23:50:25,823] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000008.mp4\n",
      "[2017-05-22 23:50:30,765] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000027.mp4\n",
      "[2017-05-22 23:50:37,157] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000064.mp4\n",
      "[2017-05-22 23:50:45,686] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.0.2564.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.9999999998906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Monitor.close of <Monitor<TimeLimit<CartPoleEnv<CartPole-v0>>>>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1',force=True)\n",
    "avg_test_rewards = test_q_network(mainQN, saver, test_episodes=200, render=False)\n",
    "print(avg_test_rewards)\n",
    "env.close\n",
    "#     if verbose:\n",
    "#         print('average test reward = ', avg_test_rewards)\n",
    "    \n",
    "#     return avg_test_rewards, avg_train_rewards, mainQN, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 23:52:13,462] Finished writing results. You can upload them to the scoreboard via gym.upload('D:\\\\tmp\\\\cartpole-experiment-1')\n",
      "[2017-05-22 23:52:13,463] [CartPole-v0] Uploading 200 episodes of training data\n",
      "[2017-05-22 23:52:14,715] [CartPole-v0] Uploading videos of 6 training episodes (79922 bytes)\n",
      "[2017-05-22 23:52:15,216] [CartPole-v0] Creating evaluation object from /tmp/cartpole-experiment-1 with learning curve and training video\n",
      "[2017-05-22 23:52:15,550] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_XidnJOdDQlK8HQV5xk1QRA\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "gym.upload('/tmp/cartpole-experiment-1', api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 21:43:41,579] Making new env: CartPole-v0\n",
      "[2017-05-22 21:43:41,580] Clearing 3 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-22 21:43:41,585] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-2\\openaigym.video.0.2340.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03158918  0.01501189 -0.04439814  0.02392597]\n",
      "[-0.03128894 -0.17944616 -0.04391962  0.30227684]\n",
      "[-0.03487786  0.01627333 -0.03787408 -0.00392752]\n",
      "[-0.0345524   0.21191741 -0.03795263 -0.3083155 ]\n",
      "[-0.03031405  0.0173562  -0.04411894 -0.02783925]\n",
      "[-0.02996692  0.21308217 -0.04467573 -0.33410928]\n",
      "[-0.02570528  0.40881056 -0.05135791 -0.64053921]\n",
      "[-0.01752907  0.60460946 -0.0641687  -0.9489429 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 21:43:43,488] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-2\\openaigym.video.0.2340.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00543688  0.80053387 -0.08314756 -1.26107733]\n",
      "[ 0.0105738   0.99661498 -0.1083691  -1.57860008]\n",
      "[ 0.0305061   1.19284804 -0.1399411  -1.90302116]\n",
      "[ 0.05436306  1.38917853 -0.17800153 -2.2356465 ]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.01721082 -0.01583853 -0.02459659  0.00499927]\n",
      "[ 0.01689405 -0.21059925 -0.02449661  0.28982131]\n",
      "[ 0.01268206 -0.01513671 -0.01870018 -0.01048581]\n",
      "[ 0.01237933 -0.20998555 -0.0189099   0.27623882]\n",
      "[ 0.00817962 -0.40483268 -0.01338512  0.56289808]\n",
      "[  8.29628614e-05  -5.99764277e-01  -2.12716057e-03   8.51334169e-01]\n",
      "[-0.01191232 -0.79485716  0.01489952  1.14334745]\n",
      "[-0.02780947 -0.99017059  0.03776647  1.44066537]\n",
      "[-0.04761288 -0.7955336   0.06657978  1.16001877]\n",
      "[-0.06352355 -0.99145678  0.08978015  1.47281241]\n",
      "[-0.08335269 -0.79753982  0.1192364   1.2094684 ]\n",
      "[-0.09930348 -0.60414221  0.14342577  0.95640423]\n",
      "[-0.11138633 -0.80087136  0.16255386  1.29049072]\n",
      "[-0.12740375 -0.99764366  0.18836367  1.6293388 ]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.00567666 -0.00429267 -0.02422598 -0.0220614 ]\n",
      "[ 0.0055908   0.19116817 -0.02466721 -0.32228838]\n",
      "[ 0.00941417  0.38663254 -0.03111298 -0.62264717]\n",
      "[ 0.01714682  0.5821748  -0.04356592 -0.92496434]\n",
      "[ 0.02879031  0.3876675  -0.06206521 -0.64628455]\n",
      "[ 0.03654366  0.58359685 -0.0749909  -0.95784816]\n",
      "[ 0.0482156   0.38955898 -0.09414786 -0.68963604]\n",
      "[ 0.05600678  0.58585263 -0.10794058 -1.01041114]\n",
      "[ 0.06772383  0.39232375 -0.12814881 -0.75348026]\n",
      "[ 0.07557031  0.58895772 -0.14321841 -1.08358537]\n",
      "[ 0.08734946  0.78564897 -0.16489012 -1.41756401]\n",
      "[ 0.10306244  0.59290733 -0.1932414  -1.18063128]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.046459   -0.00798891 -0.0432039  -0.03734576]\n",
      "[ 0.04629922  0.1877251  -0.04395081 -0.34334084]\n",
      "[ 0.05005372 -0.00674493 -0.05081763 -0.06483487]\n",
      "[ 0.04991882 -0.20110285 -0.05211433  0.2113917 ]\n",
      "[ 0.04589677 -0.005276   -0.04788649 -0.09726445]\n",
      "[ 0.04579125 -0.19968009 -0.04983178  0.17993415]\n",
      "[ 0.04179764 -0.00388178 -0.0462331  -0.12804321]\n",
      "[ 0.04172001  0.19187094 -0.04879396 -0.43494623]\n",
      "[ 0.04555743  0.38764848 -0.05749289 -0.74260275]\n",
      "[ 0.0533104   0.58351491 -0.07234494 -1.05281056]\n",
      "[ 0.06498069  0.77951769 -0.09340115 -1.36729654]\n",
      "[ 0.08057105  0.58568073 -0.12074709 -1.10522846]\n",
      "[ 0.09228466  0.78216544 -0.14285165 -1.43322304]\n",
      "[ 0.10792797  0.5890655  -0.17151612 -1.18837919]\n",
      "[ 0.11970928  0.78594449 -0.1952837  -1.52954336]\n",
      "Episode finished after 15 timesteps\n",
      "[ 0.01739391 -0.02319332 -0.02043269 -0.02869286]\n",
      "[ 0.01693004  0.1722156  -0.02100654 -0.32775184]\n",
      "[ 0.02037435  0.36763022 -0.02756158 -0.62698457]\n",
      "[ 0.02772696  0.17290359 -0.04010127 -0.34310764]\n",
      "[ 0.03118503  0.36857242 -0.04696343 -0.64816154]\n",
      "[ 0.03855648  0.17413512 -0.05992666 -0.37062933]\n",
      "[ 0.04203918 -0.02008651 -0.06733924 -0.09742775]\n",
      "[ 0.04163745  0.17593271 -0.0692878  -0.41057295]\n",
      "[ 0.0451561   0.37196503 -0.07749926 -0.72427021]\n",
      "[ 0.05259541  0.17799554 -0.09198466 -0.45695127]\n",
      "[ 0.05615532  0.37428936 -0.10112369 -0.77715259]\n",
      "[ 0.0636411   0.18069269 -0.11666674 -0.51792082]\n",
      "[ 0.06725496  0.37724734 -0.12702516 -0.84497083]\n",
      "[ 0.0747999   0.18406619 -0.14392457 -0.59477881]\n",
      "[ 0.07848123 -0.00877928 -0.15582015 -0.35066932]\n",
      "[ 0.07830564 -0.20138175 -0.16283353 -0.11089178]\n",
      "[ 0.07427801 -0.39384133 -0.16505137  0.126317  ]\n",
      "[ 0.06640118 -0.58626143 -0.16252503  0.36271956]\n",
      "[ 0.05467595 -0.38924767 -0.15527064  0.02352106]\n",
      "[ 0.046891   -0.1922793  -0.15480022 -0.31384349]\n",
      "[ 0.04304541 -0.38489626 -0.16107709 -0.07370263]\n",
      "[ 0.03534749 -0.57738644 -0.16255114  0.16414285]\n",
      "[ 0.02379976 -0.76985343 -0.15926828  0.40145772]\n",
      "[ 0.00840269 -0.57287323 -0.15123913  0.06309986]\n",
      "[-0.00305478 -0.37594289 -0.14997713 -0.27321875]\n",
      "[-0.01057363 -0.56864216 -0.15544151 -0.03134273]\n",
      "[-0.02194648 -0.37167227 -0.15606836 -0.36875134]\n",
      "[-0.02937992 -0.56427228 -0.16344339 -0.12906066]\n",
      "[-0.04066537 -0.75672154 -0.1660246   0.10792733]\n",
      "[-0.0557998  -0.55965776 -0.16386605 -0.23219106]\n",
      "[-0.06699295 -0.75210532 -0.16850988  0.00465192]\n",
      "[-0.08203506 -0.55501781 -0.16841684 -0.33609973]\n",
      "[-0.09313542 -0.35794955 -0.17513883 -0.67680055]\n",
      "[-0.10029441 -0.16088257 -0.18867484 -1.01910703]\n",
      "[-0.10351206  0.03618421 -0.20905698 -1.36460325]\n",
      "Episode finished after 35 timesteps\n",
      "[ 0.02298201 -0.00600255 -0.0044421  -0.04751785]\n",
      "[ 0.02286196  0.18918281 -0.00539245 -0.34159898]\n",
      "[ 0.02664562 -0.005862   -0.01222443 -0.05062139]\n",
      "[ 0.02652838  0.18943308 -0.01323686 -0.34713602]\n",
      "[ 0.03031704  0.38474078 -0.02017958 -0.64396343]\n",
      "[ 0.03801186  0.1899058  -0.03305885 -0.35770285]\n",
      "[ 0.04180997 -0.00473095 -0.04021291 -0.07562478]\n",
      "[ 0.04171535  0.19094373 -0.0417254  -0.38071892]\n",
      "[ 0.04553423 -0.00356165 -0.04933978 -0.10147851]\n",
      "[ 0.04546299 -0.19794304 -0.05136935  0.17523874]\n",
      "[ 0.04150413 -0.00212499 -0.04786458 -0.13319683]\n",
      "[ 0.04146163  0.19364874 -0.05052851 -0.44058791]\n",
      "[ 0.04533461 -0.00072308 -0.05934027 -0.16425136]\n",
      "[ 0.04532015  0.19519592 -0.0626253  -0.4750484 ]\n",
      "[ 0.04922407  0.00101164 -0.07212627 -0.20274278]\n",
      "[ 0.0492443  -0.19300867 -0.07618112  0.06634427]\n",
      "[ 0.04538413  0.00311816 -0.07485424 -0.2493683 ]\n",
      "[ 0.04544649 -0.19085943 -0.0798416   0.01879671]\n",
      "[ 0.0416293  -0.38475098 -0.07946567  0.28525902]\n",
      "[ 0.03393428 -0.57865496 -0.07376049  0.55185881]\n",
      "[ 0.02236118 -0.38257882 -0.06272331  0.23687839]\n",
      "[ 0.0147096  -0.18661945 -0.05798574 -0.07491121]\n",
      "[ 0.01097722 -0.38086425 -0.05948397  0.19892802]\n",
      "[ 0.00335993 -0.18494418 -0.05550541 -0.11191028]\n",
      "[ -3.38953218e-04  -3.79228664e-01  -5.77436144e-02   1.62757421e-01]\n",
      "[-0.00792353 -0.57347848 -0.05448847  0.43667932]\n",
      "[-0.0193931  -0.76778849 -0.04575488  0.71170008]\n",
      "[-0.03474887 -0.962248   -0.03152088  0.98963683]\n",
      "[-0.05399383 -1.15693416 -0.01172814  1.27225541]\n",
      "[-0.07713251 -0.96166451  0.01371697  0.97592308]\n",
      "[-0.0963658  -1.15696773  0.03323543  1.27288301]\n",
      "[-0.11950515 -0.96228527  0.05869309  0.99079001]\n",
      "[-0.13875086 -1.15814155  0.07850889  1.3013144 ]\n",
      "[-0.16191369 -0.96409867  0.10453518  1.03420454]\n",
      "[-0.18119566 -0.77051035  0.12521927  0.77608461]\n",
      "[-0.19660587 -0.57731262  0.14074096  0.52527537]\n",
      "[-0.20815212 -0.38442252  0.15124647  0.28004245]\n",
      "[-0.21584057 -0.19174526  0.15684732  0.03862308]\n",
      "[-0.21967548 -0.38872801  0.15761978  0.37639442]\n",
      "[-0.22745004 -0.19615479  0.16514767  0.13726396]\n",
      "[-0.23137313 -0.00373597  0.16789295 -0.09910422]\n",
      "[-0.23144785  0.1886317   0.16591086 -0.33446945]\n",
      "[-0.22767522  0.38105168  0.15922147 -0.57058173]\n",
      "[-0.22005419  0.18409729  0.14780984 -0.23227348]\n",
      "[-0.21637224  0.37683205  0.14316437 -0.47492522]\n",
      "[-0.2088356   0.56967292  0.13366586 -0.71928053]\n",
      "[-0.19744214  0.37297963  0.11928025 -0.38769151]\n",
      "[-0.18998255  0.17638435  0.11152642 -0.05990865]\n",
      "[-0.18645486  0.36974522  0.11032825 -0.31542719]\n",
      "[-0.17905996  0.17323881  0.10401971  0.00991103]\n",
      "[-0.17559518 -0.02320922  0.10421793  0.33351699]\n",
      "[-0.17605937 -0.21964817  0.11088827  0.6571613 ]\n",
      "[-0.18045233 -0.41612474  0.12403149  0.98260046]\n",
      "[-0.18877482 -0.22286329  0.1436835   0.73130709]\n",
      "[-0.19323209 -0.02998838  0.15830964  0.48707374]\n",
      "[-0.19383186 -0.22694831  0.16805112  0.82516809]\n",
      "[-0.19837082 -0.42392091  0.18455448  1.16563956]\n",
      "[-0.20684924 -0.23161665  0.20786727  0.93603322]\n",
      "Episode finished after 58 timesteps\n",
      "[-0.03783027 -0.01103463  0.01544723 -0.04519796]\n",
      "[-0.03805097  0.18386244  0.01454327 -0.3329674 ]\n",
      "[-0.03437372  0.37877441  0.00788392 -0.62102886]\n",
      "[-0.02679823  0.57378538 -0.00453665 -0.91121837]\n",
      "[-0.01532252  0.76896842 -0.02276102 -1.2053237 ]\n",
      "[  5.68478771e-05   5.74147910e-01  -4.68674957e-02  -9.19859785e-01]\n",
      "[ 0.01153981  0.37968968 -0.06526469 -0.64226673]\n",
      "[ 0.0191336   0.18553524 -0.07811003 -0.37082931]\n",
      "[ 0.0228443   0.381675   -0.08552661 -0.68708289]\n",
      "[ 0.0304778   0.18783781 -0.09926827 -0.42250404]\n",
      "[ 0.03423456  0.38421569 -0.10771835 -0.74475766]\n",
      "[ 0.04191887  0.58064625 -0.1226135  -1.06930371]\n",
      "[ 0.0535318   0.77715756 -0.14399958 -1.39781579]\n",
      "[ 0.06907495  0.5840897  -0.17195589 -1.15340112]\n",
      "[ 0.08075674  0.78098553 -0.19502392 -1.49469538]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.04240573 -0.01728493  0.0278611  -0.00057409]\n",
      "[-0.04275143 -0.21279514  0.02784961  0.30076751]\n",
      "[-0.04700734 -0.01808098  0.03386497  0.01699625]\n",
      "[-0.04736895 -0.21367181  0.03420489  0.32016869]\n",
      "[-0.05164239 -0.01905326  0.04060826  0.0384659 ]\n",
      "[-0.05202346  0.17546355  0.04137758 -0.24113323]\n",
      "[-0.04851419 -0.02022429  0.03655492  0.06430883]\n",
      "[-0.04891867 -0.21585076  0.03784109  0.36829725]\n",
      "[-0.05323569 -0.02128636  0.04520704  0.0877822 ]\n",
      "[-0.05366141 -0.21702617  0.04696268  0.39437829]\n",
      "[-0.05800194 -0.02260098  0.05485025  0.11686399]\n",
      "[-0.05845396 -0.21846418  0.05718753  0.42633485]\n",
      "[-0.06282324 -0.02419689  0.06571423  0.15221427]\n",
      "[-0.06330718 -0.22019525  0.06875851  0.46488331]\n",
      "[-0.06771108 -0.02610884  0.07805618  0.19464069]\n",
      "[-0.06823326 -0.22225554  0.08194899  0.51089011]\n",
      "[-0.07267837 -0.02837781  0.09216679  0.24511553]\n",
      "[-0.07324593 -0.22468703  0.0970691   0.56538824]\n",
      "[-0.07773967 -0.4210272   0.10837687  0.8870056 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 21:43:46,601] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-2\\openaigym.video.0.2340.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08616021 -0.61744012  0.12611698  1.21169691]\n",
      "[-0.09850901 -0.42415134  0.15035092  0.96104664]\n",
      "[-0.10699204 -0.62093918  0.16957185  1.29693513]\n",
      "[-0.11941082 -0.42832723  0.19551055  1.06177668]\n",
      "Episode finished after 23 timesteps\n",
      "[ 0.01929627 -0.04759821  0.01128067 -0.03405203]\n",
      "[ 0.0183443   0.14736017  0.01059963 -0.32315455]\n",
      "[ 0.02129151 -0.0479111   0.00413654 -0.02714787]\n",
      "[ 0.02033328 -0.24309212  0.00359359  0.26683731]\n",
      "[ 0.01547144 -0.43826518  0.00893033  0.56065151]\n",
      "[ 0.00670614 -0.63351132  0.02014336  0.85613453]\n",
      "[-0.00596409 -0.82890187  0.03726605  1.15508265]\n",
      "[-0.02254213 -0.63428515  0.06036771  0.87431393]\n",
      "[-0.03522783 -0.83017359  0.07785398  1.18534868]\n",
      "[-0.0518313  -1.0262142   0.10156096  1.50138481]\n",
      "[-0.07235558 -0.83246154  0.13158865  1.24206102]\n",
      "[-0.08900482 -1.02900389  0.15642987  1.57290131]\n",
      "[-0.10958489 -1.22560767  0.1878879   1.91000915]\n",
      "Episode finished after 13 timesteps\n",
      "[-0.02936307  0.04969574  0.03889237  0.02069656]\n",
      "[-0.02836916  0.24423898  0.0393063  -0.25946615]\n",
      "[-0.02348438  0.04857858  0.03411698  0.04535083]\n",
      "[-0.02251281 -0.14701555  0.03502399  0.34859975]\n",
      "[-0.02545312  0.0475912   0.04199599  0.06716358]\n",
      "[-0.02450129 -0.1481069   0.04333926  0.37279511]\n",
      "[-0.02746343  0.04637344  0.05079516  0.09408627]\n",
      "[-0.02653596 -0.14943836  0.05267689  0.40235249]\n",
      "[-0.02952473 -0.34526634  0.06072394  0.71116685]\n",
      "[-0.03643006 -0.15103551  0.07494727  0.43819947]\n",
      "[-0.03945077  0.04295005  0.08371126  0.17005185]\n",
      "[-0.03859177 -0.15326411  0.0871123   0.48792483]\n",
      "[-0.04165705 -0.34950017  0.0968708   0.8067414 ]\n",
      "[-0.04864705 -0.54580693  0.11300562  1.12825652]\n",
      "[-0.05956319 -0.35233186  0.13557076  0.87304797]\n",
      "[-0.06660983 -0.15928798  0.15303171  0.62587492]\n",
      "[-0.06979559 -0.35617751  0.16554921  0.96257371]\n",
      "[-0.07691914 -0.55309028  0.18480069  1.30235404]\n",
      "Episode finished after 18 timesteps\n",
      "[ 0.0288528  -0.01424239  0.03316189 -0.01845229]\n",
      "[ 0.02856796 -0.20982384  0.03279284  0.28450639]\n",
      "[ 0.02437148 -0.40539778  0.03848297  0.58734895]\n",
      "[ 0.01626352 -0.21083531  0.05022995  0.30703252]\n",
      "[ 0.01204682 -0.40663569  0.0563706   0.61512405]\n",
      "[ 0.0039141  -0.21234479  0.06867308  0.3407148 ]\n",
      "[ -3.32792617e-04  -4.08373210e-01   7.54873785e-02   6.54238373e-01]\n",
      "[-0.00850026 -0.60446056  0.08857215  0.96970398]\n",
      "[-0.02058947 -0.80065265  0.10796623  1.28884471]\n",
      "[-0.03660252 -0.60705694  0.13374312  1.03182245]\n",
      "[-0.04874366 -0.41394302  0.15437957  0.78394177]\n",
      "[-0.05702252 -0.22124128  0.1700584   0.54353435]\n",
      "[-0.06144735 -0.41829361  0.18092909  0.88460562]\n",
      "[-0.06981322 -0.61534984  0.1986212   1.22826854]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.02402884  0.03851564 -0.01227161 -0.01467831]\n",
      "[ 0.02479915  0.23381141 -0.01256518 -0.31120768]\n",
      "[ 0.02947538  0.4291101  -0.01878933 -0.60782666]\n",
      "[ 0.03805758  0.62448964 -0.03094587 -0.90636797]\n",
      "[ 0.05054737  0.42980005 -0.04907323 -0.62357004]\n",
      "[ 0.05914337  0.23539637 -0.06154463 -0.34673731]\n",
      "[ 0.0638513   0.04120137 -0.06847937 -0.07407901]\n",
      "[ 0.06467533  0.23723483 -0.06996095 -0.3875572 ]\n",
      "[ 0.06942002  0.43327651 -0.0777121  -0.70145211]\n",
      "[ 0.07808555  0.23931285 -0.09174114 -0.43420909]\n",
      "[ 0.08287181  0.43560574 -0.10042532 -0.7543443 ]\n",
      "[ 0.09158393  0.2420011  -0.11551221 -0.49487597]\n",
      "[ 0.09642395  0.04868144 -0.12540973 -0.24071404]\n",
      "[ 0.09739758  0.24535093 -0.13022401 -0.57017536]\n",
      "[ 0.10230459  0.05227259 -0.14162751 -0.32118782]\n",
      "[ 0.10335005  0.24909747 -0.14805127 -0.65496854]\n",
      "[ 0.108332    0.44593667 -0.16115064 -0.99036556]\n",
      "[ 0.11725073  0.64280555 -0.18095795 -1.32901621]\n",
      "[ 0.13010684  0.83968967 -0.20753828 -1.67243199]\n",
      "Episode finished after 19 timesteps\n",
      "[  4.65557646e-02  -8.98761061e-05   4.96372319e-02   4.58966789e-02]\n",
      "[ 0.04655397 -0.19588716  0.05055517  0.35381822]\n",
      "[ 0.04263622 -0.39169015  0.05763153  0.66200435]\n",
      "[ 0.03480242 -0.19741543  0.07087162  0.38801036]\n",
      "[ 0.03085411 -0.00336727  0.07863182  0.11848764]\n",
      "[ 0.03078677  0.19054515  0.08100158 -0.1483883 ]\n",
      "[ 0.03459767 -0.00563769  0.07803381  0.16870951]\n",
      "[ 0.03448492  0.18828562  0.081408   -0.09837159]\n",
      "[ 0.03825063  0.38215218  0.07944057 -0.36430083]\n",
      "[ 0.04589367  0.57606054  0.07215455 -0.63091511]\n",
      "[ 0.05741488  0.77010549  0.05953625 -0.90003013]\n",
      "[ 0.07281699  0.57422948  0.04153565 -0.58924336]\n",
      "[ 0.08430158  0.37855128  0.02975078 -0.28377126]\n",
      "[ 0.09187261  0.57323655  0.02407536 -0.56692449]\n",
      "[ 0.10333734  0.76801265  0.01273687 -0.85192649]\n",
      "[ 0.11869759  0.57271939 -0.00430166 -0.55526583]\n",
      "[ 0.13015198  0.3776581  -0.01540698 -0.26394128]\n",
      "[ 0.13770514  0.18275941 -0.02068581  0.02384258]\n",
      "[ 0.14136033  0.37817181 -0.02020895 -0.27529451]\n",
      "[ 0.14892377  0.57357618 -0.02571484 -0.57428223]\n",
      "[ 0.16039529  0.378824   -0.03720049 -0.28980988]\n",
      "[ 0.16797177  0.57445612 -0.04299669 -0.59398946]\n",
      "[ 0.17946089  0.77015269 -0.05487648 -0.89990015]\n",
      "[ 0.19486395  0.96597364 -0.07287448 -1.20931499]\n",
      "[ 0.21418342  1.16195711 -0.09706078 -1.52391608]\n",
      "[ 0.23742256  1.35810798 -0.1275391  -1.84524863]\n",
      "[ 0.26458472  1.1646019  -0.16444407 -1.59474194]\n",
      "[ 0.28787676  0.97176798 -0.19633891 -1.35752216]\n",
      "Episode finished after 28 timesteps\n",
      "[ 0.04812963  0.03402094 -0.04908267  0.04699764]\n",
      "[ 0.04881005  0.22981108 -0.04814272 -0.26075841]\n",
      "[ 0.05340627  0.03540825 -0.05335789  0.01635948]\n",
      "[ 0.05411444  0.23125321 -0.0530307  -0.29266964]\n",
      "[ 0.0587395   0.03692589 -0.05888409 -0.01717216]\n",
      "[ 0.05947802  0.23284071 -0.05922753 -0.32783698]\n",
      "[ 0.06413483  0.42875365 -0.06578427 -0.63859382]\n",
      "[ 0.07270991  0.23460766 -0.07855615 -0.36733137]\n",
      "[ 0.07740206  0.04068474 -0.08590278 -0.10041583]\n",
      "[ 0.07821575 -0.15310775 -0.08791109  0.16397667]\n",
      "[ 0.0751536   0.04315549 -0.08463156 -0.15509391]\n",
      "[ 0.07601671 -0.15065916 -0.08773344  0.10973555]\n",
      "[ 0.07300353 -0.34442152 -0.08553873  0.37350068]\n",
      "[ 0.0661151  -0.14819522 -0.07806871  0.05511967]\n",
      "[ 0.06315119  0.04795428 -0.07696632 -0.26113732]\n",
      "[ 0.06411028  0.24408573 -0.08218907 -0.5770693 ]\n",
      "[ 0.06899199  0.44025767 -0.09373045 -0.89447031]\n",
      "[ 0.07779715  0.24652322 -0.11161986 -0.63266063]\n",
      "[ 0.08272761  0.44301082 -0.12427307 -0.95830605]\n",
      "[ 0.09158783  0.24975903 -0.14343919 -0.70710621]\n",
      "[ 0.09658301  0.05688476 -0.15758132 -0.46279262]\n",
      "[ 0.0977207  -0.13570023 -0.16683717 -0.22363181]\n",
      "[ 0.0950067  -0.32809353 -0.1713098   0.01212814]\n",
      "[ 0.08844483 -0.13098169 -0.17106724 -0.32933081]\n",
      "[ 0.08582519  0.06611009 -0.17765386 -0.67070209]\n",
      "[ 0.08714739  0.26319865 -0.1910679  -1.0136379 ]\n",
      "Episode finished after 26 timesteps\n",
      "[ 0.03257094 -0.04544678 -0.03526248  0.02116087]\n",
      "[ 0.031662    0.15016266 -0.03483926 -0.28243591]\n",
      "[ 0.03466526 -0.04444548 -0.04048798 -0.00094151]\n",
      "[ 0.03377635 -0.23896409 -0.04050681  0.27869707]\n",
      "[ 0.02899707 -0.0432884  -0.03493287 -0.02648145]\n",
      "[ 0.0281313   0.15231665 -0.0354625  -0.3299782 ]\n",
      "[ 0.03117763  0.347925   -0.04206206 -0.63363004]\n",
      "[ 0.03813613  0.54360769 -0.05473467 -0.93925705]\n",
      "[ 0.04900829  0.73942308 -0.07351981 -1.24862422]\n",
      "[ 0.06379675  0.93540652 -0.09849229 -1.56340061]\n",
      "[ 0.08250488  0.74159062 -0.1297603  -1.30299567]\n",
      "[ 0.09733669  0.54833124 -0.15582022 -1.05358628]\n",
      "[ 0.10830331  0.74513699 -0.17689194 -1.39084432]\n",
      "[ 0.12320605  0.94196517 -0.20470883 -1.73321487]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.02952904 -0.04796749 -0.03110508 -0.0454169 ]\n",
      "[ 0.02856969  0.14758636 -0.03201342 -0.34774921]\n",
      "[ 0.03152141  0.34314868 -0.0389684  -0.65035272]\n",
      "[ 0.03838439  0.53879113 -0.05197546 -0.95504741]\n",
      "[ 0.04916021  0.73457225 -0.07107641 -1.26359642]\n",
      "[ 0.06385165  0.93052717 -0.09634834 -1.57766554]\n",
      "[ 0.0824622   1.12665595 -0.12790165 -1.89877687]\n",
      "[ 0.10499532  1.32290997 -0.16587718 -2.22825396]\n",
      "Episode finished after 8 timesteps\n",
      "[ 0.04305961  0.0322213   0.03516371 -0.04491327]\n",
      "[ 0.04370403 -0.16338678  0.03426544  0.25865346]\n",
      "[ 0.0404363  -0.35898073  0.03943851  0.56194423]\n",
      "[ 0.03325668 -0.5546333   0.0506774   0.86678679]\n",
      "[ 0.02216402 -0.36023626  0.06801313  0.59045837]\n",
      "[ 0.01495929 -0.16612919  0.0798223   0.31995138]\n",
      "[ 0.01163671  0.0277706   0.08622133  0.05347063]\n",
      "[ 0.01219212 -0.16847505  0.08729074  0.37206326]\n",
      "[ 0.00882262  0.02530539  0.094732    0.10813022]\n",
      "[ 0.00932873  0.21895116  0.09689461 -0.15322649]\n",
      "[ 0.01370775  0.41256181  0.09383008 -0.41383777]\n",
      "[ 0.02195899  0.60623724  0.08555332 -0.67552742]\n",
      "[ 0.03408373  0.80007269  0.07204278 -0.94009518]\n",
      "[ 0.05008519  0.9941535   0.05324087 -1.20929898]\n",
      "[ 0.06996826  1.18854898  0.02905489 -1.48483361]\n",
      "[  9.37392350e-02   9.93085136e-01  -6.41779686e-04  -1.18322064e+00]\n",
      "[ 0.11360094  0.79797152 -0.02430619 -0.89073896]\n",
      "[ 0.12956037  0.60318762 -0.04212097 -0.60579474]\n",
      "[ 0.14162412  0.40867919 -0.05423687 -0.32667078]\n",
      "[ 0.1497977   0.6045297  -0.06077028 -0.63595237]\n",
      "[ 0.1618883   0.8004442  -0.07348933 -0.94713712]\n",
      "[ 0.17789718  0.60638469 -0.09243207 -0.67841997]\n",
      "[ 0.19002488  0.80266092 -0.10600047 -0.99871386]\n",
      "[ 0.20607809  0.60910339 -0.12597475 -0.74111298]\n",
      "[ 0.21826016  0.41592486 -0.14079701 -0.49058008]\n",
      "[ 0.22657866  0.61272293 -0.15060861 -0.82411376]\n",
      "[ 0.23883312  0.41994654 -0.16709088 -0.58233426]\n",
      "[ 0.24723205  0.61696669 -0.17873757 -0.92264501]\n",
      "[ 0.25957138  0.42465098 -0.19719047 -0.69103664]\n",
      "Episode finished after 29 timesteps\n",
      "[ 0.00869642 -0.0317159   0.03414911 -0.00617802]\n",
      "[ 0.0080621  -0.22731054  0.03402555  0.29708076]\n",
      "[ 0.00351589 -0.4229006   0.03996717  0.60029762]\n",
      "[-0.00494212 -0.61855822  0.05197312  0.9052969 ]\n",
      "[-0.01731328 -0.814344    0.07007906  1.21385224]\n",
      "[-0.03360016 -0.6201928   0.0943561   0.94392664]\n",
      "[-0.04600402 -0.42645997  0.11323464  0.6823196 ]\n",
      "[-0.05453322 -0.23307745  0.12688103  0.42732341]\n",
      "[-0.05919477 -0.42974663  0.1354275   0.75715876]\n",
      "[-0.0677897  -0.23672518  0.15057067  0.50997173]\n",
      "[-0.0725242  -0.43361184  0.16077011  0.84606093]\n",
      "[-0.08119644 -0.24100538  0.17769133  0.60794001]\n",
      "[-0.08601655 -0.43810806  0.18985013  0.95090553]\n",
      "[-0.09477871 -0.63520771  0.20886824  1.29672419]\n",
      "Episode finished after 14 timesteps\n",
      "[-0.00869149  0.01991773  0.00031862 -0.0106579 ]\n",
      "[ -8.29313096e-03  -1.75208792e-01   1.05461338e-04   2.82125536e-01]\n",
      "[-0.01179731  0.01991165  0.00574797 -0.01052413]\n",
      "[-0.01139907  0.2149507   0.00553749 -0.30138797]\n",
      "[ -7.10005966e-03   4.09993293e-01  -4.90269881e-04  -5.92319357e-01]\n",
      "[ 0.00109981  0.21487821 -0.01233666 -0.2997909 ]\n",
      "[ 0.00539737  0.41017381 -0.01833248 -0.5963389 ]\n",
      "[ 0.01360085  0.21531315 -0.03025925 -0.30948647]\n",
      "[ 0.01790711  0.0206351  -0.03644898 -0.02649794]\n",
      "[ 0.01831981  0.21626029 -0.03697894 -0.33045445]\n",
      "[ 0.02264502  0.41188859 -0.04358803 -0.63456562]\n",
      "[ 0.03088279  0.60759056 -0.05627934 -0.94065058]\n",
      "[ 0.0430346   0.41327046 -0.07509235 -0.66616932]\n",
      "[ 0.05130001  0.60935207 -0.08841574 -0.98151927]\n",
      "[ 0.06348705  0.80554051 -0.10804613 -1.30061281]\n",
      "[ 0.07959786  0.61194283 -0.13405838 -1.04361347]\n",
      "[ 0.09183672  0.80856539 -0.15493065 -1.37519627]\n",
      "[ 0.10800803  0.61568135 -0.18243458 -1.13470329]\n",
      "[ 0.12032165  0.81265971 -0.20512864 -1.47860626]\n",
      "Episode finished after 19 timesteps\n",
      "[-0.0225129   0.04749441 -0.0175406   0.033015  ]\n",
      "[-0.02156301 -0.14737166 -0.0168803   0.32011248]\n",
      "[-0.02451044 -0.3422492  -0.01047805  0.60742456]\n",
      "[-0.03135542 -0.14698233  0.00167044  0.31145985]\n",
      "[-0.03429507  0.04811579  0.00789964  0.01930419]\n",
      "[-0.03333276 -0.14711856  0.00828572  0.31446904]\n",
      "[-0.03627513  0.04788439  0.0145751   0.02441064]\n",
      "[-0.03531744  0.24279432  0.01506332 -0.26363828]\n",
      "[-0.03046155  0.43769806  0.00979055 -0.55153227]\n",
      "[-0.02170759  0.63268115 -0.0012401  -0.84111451]\n",
      "[-0.00905397  0.82782001 -0.01806239 -1.13418716]\n",
      "[ 0.00750243  1.02317361 -0.04074613 -1.43247982]\n",
      "[ 0.0279659   0.82857745 -0.06939573 -1.1528039 ]\n",
      "[ 0.04453745  0.63442595 -0.0924518  -0.88266358]\n",
      "[ 0.05722597  0.83067369 -0.11010508 -1.20292017]\n",
      "[ 0.07383945  0.63713394 -0.13416348 -0.94667402]\n",
      "[ 0.08658213  0.83378258 -0.15309696 -1.27832208]\n",
      "[ 0.10325778  0.64090721 -0.1786634  -1.03722783]\n",
      "[ 0.11607592  0.44855136 -0.19940796 -0.80553495]\n",
      "Episode finished after 19 timesteps\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-2',force=True)\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
