{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "# base code from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 21:05:06,414] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01959151 -0.01626964  0.04402413  0.0121828 ]\n"
     ]
    }
   ],
   "source": [
    "# Create new cart pole environment\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0., \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "            \n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "            \n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "            \n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "            \n",
    "                \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=None,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.maximum(alpha*self.fc1,self.fc1)\n",
    "            \n",
    "            if hidden_layers == 1:\n",
    "                out_layer = self.fc1\n",
    "            else:\n",
    "                \n",
    "                self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.fc2 = tf.maximum(alpha*self.fc2,self.fc2)\n",
    "                \n",
    "                if hidden_layers == 2:\n",
    "                    out_layer = self.fc2\n",
    "                else:\n",
    "                    self.fc3 = tf.layers.dense(self.fc2, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                    self.fc3 = tf.maximum(alpha*self.fc3,self.fc3)\n",
    "                    out_layer = self.fc3\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_memory_rand_states(memory_size=10000,pretrain_length=20):\n",
    "    # Initialize the simulation\n",
    "    state = env.reset()\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    for ii in range(pretrain_length):\n",
    "\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_q_network(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   max_steps=195,\\\n",
    "                   alpha=0.,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    \n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        state = env.reset()\n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            \n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "                # Uncomment this next line to watch the training\n",
    "                # env.render() \n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    t = max_steps\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "                \n",
    "            if verbose:\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p))\n",
    "            rewards_list.append((ep, total_reward))\n",
    "            \n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "            \n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_q_network(mainQN, saver, test_episodes=20, test_max_steps=500, render=True):\n",
    "\n",
    "    env.reset()\n",
    "    avg_rewards = 0.\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "   \n",
    "        state = env.reset()\n",
    "        for ep in range(test_episodes):\n",
    "            t = 0\n",
    "            while t < test_max_steps:\n",
    "                if render:\n",
    "                    env.render() \n",
    "\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                avg_rewards = avg_rewards + reward / test_episodes\n",
    "                if done:\n",
    "                    t = test_max_steps\n",
    "                    state = env.reset()\n",
    "                    # Take one random step to get the pole and cart moving\n",
    "                    #state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "                    \n",
    "    return avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_and_train_qnetwork(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   test_episodes=20,\\\n",
    "                   render=False,\\\n",
    "                   alpha=0.,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver = train_q_network(train_episodes = train_episodes, \\\n",
    "                                                  gamma=gamma,\\\n",
    "                                                  explore_start=explore_start,\\\n",
    "                                                  explore_stop=explore_stop,\\\n",
    "                                                  decay_rate=decay_rate,\\\n",
    "                                                  hidden_size=hidden_size,\\\n",
    "                                                  hidden_layers=hidden_layers,\\\n",
    "                                                  learning_rate=learning_rate,\\\n",
    "                                                  memory_size=memory_size,\\\n",
    "                                                  batch_size=batch_size,\\\n",
    "                                                  alpha=alpha,\\\n",
    "                                                  verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    # test q-network\n",
    "    avg_test_rewards = test_q_network(mainQN, saver, test_episodes=test_episodes, render=verbose)\n",
    "    if verbose:\n",
    "        print('average test reward = ', avg_test_rewards)\n",
    "    \n",
    "    return avg_test_rewards, avg_train_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 15:54:17,537] Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test reward =  9.249999999999996\n"
     ]
    }
   ],
   "source": [
    "# test implementation\n",
    "average_rewards = test_and_train_qnetwork(train_episodes=100, verbose=False)\n",
    "print('average test reward = ', average_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr=0.0001_ga=0.99 test avg=189.52500000000884  train avg=110.806']\n",
      "['dr=0.0002_ga=0.99 test avg=200.00000000001123  train avg=139.79']\n",
      "['dr=0.0004_ga=0.99 test avg=10.075000000000008  train avg=136.847']\n",
      "['dr=0.0001_ga=0.98 test avg=194.92500000001007  train avg=130.697']\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 21:04:17,130] Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-a17cef74137d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0maverage_train_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_averages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_and_train_qnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_eps\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mga\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mexplore_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexp_start\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mexplore_stop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexp_stop\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdr\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mhidden_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m                                   \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0maverage_test_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0maverage_train_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-203-01f4996479c9>\u001b[0m in \u001b[0;36mtest_and_train_qnetwork\u001b[0;34m(train_episodes, gamma, explore_start, explore_stop, decay_rate, hidden_size, hidden_layers, learning_rate, memory_size, batch_size, test_episodes, render, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;31m# train q-network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrewards_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_q_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_episodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[1;33m,\u001b[0m                                                   \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mexplore_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplore_start\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mexplore_stop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplore_stop\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay_rate\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mhidden_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mmemory_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemory_size\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m                                                  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-4b80eb0028b6>\u001b[0m in \u001b[0;36mtrain_q_network\u001b[0;34m(train_episodes, gamma, explore_start, explore_stop, decay_rate, hidden_size, hidden_layers, learning_rate, memory_size, batch_size, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[1;31m# Get action from Q-network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mQs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_eps = 500\n",
    "verb = False\n",
    "gamma = [0.99,0.98,0.96]\n",
    "decay_rate = [0.0001,0.0002,0.0004]\n",
    "exp_start=1.0\n",
    "exp_stop=0.1\n",
    "hidden_size=64\n",
    "hidden_layers=1\n",
    "learning_rate=0.001\n",
    "batch_size=20\n",
    "num_averages = 2\n",
    "results = []\n",
    "alpha_relu = [0., 0.1]\n",
    "\n",
    "\n",
    "for gaIndex in range(len(gamma)):\n",
    "    for drIndex in range(len(decay_rate)):\n",
    "        ga = gamma[gaIndex]\n",
    "        dr = decay_rate[drIndex]\n",
    "        train_params_name = 'dr='+str(dr)+'_ga='+str(ga)\n",
    "        average_test_rewards = 0.\n",
    "        average_train_rewards = 0.\n",
    "        for i in range(num_averages):\n",
    "            test,train = test_and_train_qnetwork(train_episodes=train_eps,\\\n",
    "                                   gamma=ga,\\\n",
    "                                   explore_start=exp_start,\\\n",
    "                                   explore_stop=exp_stop,\\\n",
    "                                   decay_rate=dr,\\\n",
    "                                   hidden_layers=hidden_layers,\\\n",
    "                                   hidden_size=hidden_size,\\\n",
    "                                   learning_rate=learning_rate,\\\n",
    "                                   batch_size=batch_size,\\\n",
    "                                   alpha = alpha_relu,\\\n",
    "                                   verbose=verb)\n",
    "            average_test_rewards += test\n",
    "            average_train_rewards += train\n",
    "\n",
    "        average_test_rewards = average_test_rewards / num_averages\n",
    "        average_train_rewards = average_train_rewards / num_averages\n",
    "        results.append([train_params_name+' test avg='+str(average_test_rewards)+'  train avg='+str(average_train_rewards)])\n",
    "        clear_output()\n",
    "        for each in results:\n",
    "            print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 14.0 Training loss: 1.2950 Explore P: 0.9987\n",
      "Episode: 1 Total reward: 43.0 Training loss: 1.2038 Explore P: 0.9949\n",
      "Episode: 2 Total reward: 23.0 Training loss: 1.3081 Explore P: 0.9928\n",
      "Episode: 3 Total reward: 9.0 Training loss: 1.2267 Explore P: 0.9920\n",
      "Episode: 4 Total reward: 21.0 Training loss: 1.6801 Explore P: 0.9902\n",
      "Episode: 5 Total reward: 41.0 Training loss: 1.7272 Explore P: 0.9865\n",
      "Episode: 6 Total reward: 10.0 Training loss: 1.5540 Explore P: 0.9856\n",
      "Episode: 7 Total reward: 12.0 Training loss: 1.6622 Explore P: 0.9846\n",
      "Episode: 8 Total reward: 23.0 Training loss: 2.3423 Explore P: 0.9825\n",
      "Episode: 9 Total reward: 19.0 Training loss: 2.1084 Explore P: 0.9809\n",
      "Episode: 10 Total reward: 10.0 Training loss: 2.5088 Explore P: 0.9800\n",
      "Episode: 11 Total reward: 15.0 Training loss: 2.9160 Explore P: 0.9787\n",
      "Episode: 12 Total reward: 37.0 Training loss: 2.2928 Explore P: 0.9754\n",
      "Episode: 13 Total reward: 22.0 Training loss: 3.6535 Explore P: 0.9735\n",
      "Episode: 14 Total reward: 10.0 Training loss: 2.0162 Explore P: 0.9726\n",
      "Episode: 15 Total reward: 22.0 Training loss: 3.4907 Explore P: 0.9707\n",
      "Episode: 16 Total reward: 13.0 Training loss: 2.6444 Explore P: 0.9696\n",
      "Episode: 17 Total reward: 38.0 Training loss: 2.2128 Explore P: 0.9663\n",
      "Episode: 18 Total reward: 16.0 Training loss: 17.1694 Explore P: 0.9649\n",
      "Episode: 19 Total reward: 13.0 Training loss: 3.9892 Explore P: 0.9638\n",
      "Episode: 20 Total reward: 19.0 Training loss: 2.8718 Explore P: 0.9621\n",
      "Episode: 21 Total reward: 21.0 Training loss: 2.9944 Explore P: 0.9603\n",
      "Episode: 22 Total reward: 13.0 Training loss: 1.9803 Explore P: 0.9592\n",
      "Episode: 23 Total reward: 17.0 Training loss: 2.1726 Explore P: 0.9577\n",
      "Episode: 24 Total reward: 41.0 Training loss: 2.6086 Explore P: 0.9542\n",
      "Episode: 25 Total reward: 9.0 Training loss: 2.1158 Explore P: 0.9535\n",
      "Episode: 26 Total reward: 12.0 Training loss: 3.1998 Explore P: 0.9524\n",
      "Episode: 27 Total reward: 51.0 Training loss: 30.8354 Explore P: 0.9481\n",
      "Episode: 28 Total reward: 13.0 Training loss: 1.9667 Explore P: 0.9470\n",
      "Episode: 29 Total reward: 18.0 Training loss: 51.2338 Explore P: 0.9455\n",
      "Episode: 30 Total reward: 16.0 Training loss: 31.4922 Explore P: 0.9441\n",
      "Episode: 31 Total reward: 28.0 Training loss: 43.3826 Explore P: 0.9418\n",
      "Episode: 32 Total reward: 11.0 Training loss: 1.3410 Explore P: 0.9408\n",
      "Episode: 33 Total reward: 20.0 Training loss: 35.8847 Explore P: 0.9392\n",
      "Episode: 34 Total reward: 10.0 Training loss: 36.8755 Explore P: 0.9383\n",
      "Episode: 35 Total reward: 10.0 Training loss: 36.8015 Explore P: 0.9375\n",
      "Episode: 36 Total reward: 26.0 Training loss: 1.1731 Explore P: 0.9353\n",
      "Episode: 37 Total reward: 39.0 Training loss: 1.3209 Explore P: 0.9321\n",
      "Episode: 38 Total reward: 11.0 Training loss: 34.6431 Explore P: 0.9311\n",
      "Episode: 39 Total reward: 12.0 Training loss: 1.9911 Explore P: 0.9301\n",
      "Episode: 40 Total reward: 14.0 Training loss: 41.2826 Explore P: 0.9290\n",
      "Episode: 41 Total reward: 30.0 Training loss: 79.0376 Explore P: 0.9265\n",
      "Episode: 42 Total reward: 16.0 Training loss: 1.9748 Explore P: 0.9252\n",
      "Episode: 43 Total reward: 30.0 Training loss: 1.1798 Explore P: 0.9227\n",
      "Episode: 44 Total reward: 57.0 Training loss: 31.6160 Explore P: 0.9180\n",
      "Episode: 45 Total reward: 12.0 Training loss: 0.9803 Explore P: 0.9170\n",
      "Episode: 46 Total reward: 21.0 Training loss: 0.8162 Explore P: 0.9153\n",
      "Episode: 47 Total reward: 24.0 Training loss: 0.8424 Explore P: 0.9134\n",
      "Episode: 48 Total reward: 37.0 Training loss: 74.4270 Explore P: 0.9104\n",
      "Episode: 49 Total reward: 21.0 Training loss: 0.9263 Explore P: 0.9087\n",
      "Episode: 50 Total reward: 17.0 Training loss: 21.5335 Explore P: 0.9073\n",
      "Episode: 51 Total reward: 11.0 Training loss: 0.8422 Explore P: 0.9064\n",
      "Episode: 52 Total reward: 14.0 Training loss: 1.4654 Explore P: 0.9053\n",
      "Episode: 53 Total reward: 19.0 Training loss: 1.8798 Explore P: 0.9038\n",
      "Episode: 54 Total reward: 15.0 Training loss: 2.3297 Explore P: 0.9026\n",
      "Episode: 55 Total reward: 21.0 Training loss: 2.7335 Explore P: 0.9009\n",
      "Episode: 56 Total reward: 18.0 Training loss: 2.8234 Explore P: 0.8994\n",
      "Episode: 57 Total reward: 91.0 Training loss: 0.9581 Explore P: 0.8922\n",
      "Episode: 58 Total reward: 25.0 Training loss: 0.6489 Explore P: 0.8902\n",
      "Episode: 59 Total reward: 16.0 Training loss: 0.8050 Explore P: 0.8889\n",
      "Episode: 60 Total reward: 10.0 Training loss: 1.2561 Explore P: 0.8882\n",
      "Episode: 61 Total reward: 14.0 Training loss: 1.2305 Explore P: 0.8871\n",
      "Episode: 62 Total reward: 38.0 Training loss: 59.6238 Explore P: 0.8841\n",
      "Episode: 63 Total reward: 31.0 Training loss: 2.3410 Explore P: 0.8816\n",
      "Episode: 64 Total reward: 40.0 Training loss: 80.9429 Explore P: 0.8785\n",
      "Episode: 65 Total reward: 23.0 Training loss: 1.5319 Explore P: 0.8767\n",
      "Episode: 66 Total reward: 16.0 Training loss: 0.9120 Explore P: 0.8755\n",
      "Episode: 67 Total reward: 14.0 Training loss: 0.6579 Explore P: 0.8744\n",
      "Episode: 68 Total reward: 28.0 Training loss: 67.1225 Explore P: 0.8722\n",
      "Episode: 69 Total reward: 38.0 Training loss: 0.6937 Explore P: 0.8693\n",
      "Episode: 70 Total reward: 16.0 Training loss: 1.1676 Explore P: 0.8681\n",
      "Episode: 71 Total reward: 18.0 Training loss: 24.5115 Explore P: 0.8667\n",
      "Episode: 72 Total reward: 12.0 Training loss: 1.0572 Explore P: 0.8658\n",
      "Episode: 73 Total reward: 16.0 Training loss: 1.9446 Explore P: 0.8646\n",
      "Episode: 74 Total reward: 22.0 Training loss: 1.5935 Explore P: 0.8629\n",
      "Episode: 75 Total reward: 16.0 Training loss: 1.0019 Explore P: 0.8617\n",
      "Episode: 76 Total reward: 18.0 Training loss: 0.8487 Explore P: 0.8603\n",
      "Episode: 77 Total reward: 17.0 Training loss: 0.9592 Explore P: 0.8590\n",
      "Episode: 78 Total reward: 35.0 Training loss: 40.2732 Explore P: 0.8563\n",
      "Episode: 79 Total reward: 25.0 Training loss: 0.6510 Explore P: 0.8545\n",
      "Episode: 80 Total reward: 23.0 Training loss: 1.8261 Explore P: 0.8527\n",
      "Episode: 81 Total reward: 13.0 Training loss: 1.6329 Explore P: 0.8517\n",
      "Episode: 82 Total reward: 19.0 Training loss: 0.9618 Explore P: 0.8503\n",
      "Episode: 83 Total reward: 20.0 Training loss: 1.6045 Explore P: 0.8488\n",
      "Episode: 84 Total reward: 12.0 Training loss: 26.9265 Explore P: 0.8479\n",
      "Episode: 85 Total reward: 17.0 Training loss: 0.8160 Explore P: 0.8466\n",
      "Episode: 86 Total reward: 9.0 Training loss: 1.5020 Explore P: 0.8460\n",
      "Episode: 87 Total reward: 36.0 Training loss: 1.6046 Explore P: 0.8433\n",
      "Episode: 88 Total reward: 48.0 Training loss: 1.3214 Explore P: 0.8397\n",
      "Episode: 89 Total reward: 26.0 Training loss: 45.0774 Explore P: 0.8378\n",
      "Episode: 90 Total reward: 15.0 Training loss: 0.7351 Explore P: 0.8367\n",
      "Episode: 91 Total reward: 25.0 Training loss: 1.6392 Explore P: 0.8349\n",
      "Episode: 92 Total reward: 43.0 Training loss: 0.8463 Explore P: 0.8317\n",
      "Episode: 93 Total reward: 57.0 Training loss: 1.0914 Explore P: 0.8276\n",
      "Episode: 94 Total reward: 27.0 Training loss: 36.7040 Explore P: 0.8256\n",
      "Episode: 95 Total reward: 15.0 Training loss: 1.9662 Explore P: 0.8245\n",
      "Episode: 96 Total reward: 59.0 Training loss: 1.6551 Explore P: 0.8202\n",
      "Episode: 97 Total reward: 81.0 Training loss: 33.1900 Explore P: 0.8144\n",
      "Episode: 98 Total reward: 36.0 Training loss: 1.5426 Explore P: 0.8119\n",
      "Episode: 99 Total reward: 13.0 Training loss: 65.3253 Explore P: 0.8109\n",
      "Episode: 100 Total reward: 12.0 Training loss: 1.7215 Explore P: 0.8101\n",
      "Episode: 101 Total reward: 10.0 Training loss: 2.4860 Explore P: 0.8094\n",
      "Episode: 102 Total reward: 45.0 Training loss: 42.6265 Explore P: 0.8062\n",
      "Episode: 103 Total reward: 26.0 Training loss: 1.2432 Explore P: 0.8044\n",
      "Episode: 104 Total reward: 14.0 Training loss: 71.0640 Explore P: 0.8034\n",
      "Episode: 105 Total reward: 33.0 Training loss: 30.8217 Explore P: 0.8011\n",
      "Episode: 106 Total reward: 18.0 Training loss: 42.5329 Explore P: 0.7998\n",
      "Episode: 107 Total reward: 19.0 Training loss: 0.7893 Explore P: 0.7985\n",
      "Episode: 108 Total reward: 34.0 Training loss: 2.2972 Explore P: 0.7961\n",
      "Episode: 109 Total reward: 22.0 Training loss: 1.1210 Explore P: 0.7946\n",
      "Episode: 110 Total reward: 49.0 Training loss: 67.2238 Explore P: 0.7912\n",
      "Episode: 111 Total reward: 22.0 Training loss: 28.3577 Explore P: 0.7897\n",
      "Episode: 112 Total reward: 50.0 Training loss: 36.5080 Explore P: 0.7862\n",
      "Episode: 113 Total reward: 11.0 Training loss: 1.8174 Explore P: 0.7855\n",
      "Episode: 114 Total reward: 21.0 Training loss: 3.2358 Explore P: 0.7840\n",
      "Episode: 115 Total reward: 19.0 Training loss: 2.5843 Explore P: 0.7827\n",
      "Episode: 116 Total reward: 11.0 Training loss: 1.4340 Explore P: 0.7820\n",
      "Episode: 117 Total reward: 16.0 Training loss: 42.4578 Explore P: 0.7809\n",
      "Episode: 118 Total reward: 41.0 Training loss: 1.3768 Explore P: 0.7781\n",
      "Episode: 119 Total reward: 16.0 Training loss: 38.5049 Explore P: 0.7770\n",
      "Episode: 120 Total reward: 39.0 Training loss: 91.6516 Explore P: 0.7744\n",
      "Episode: 121 Total reward: 18.0 Training loss: 67.2656 Explore P: 0.7732\n",
      "Episode: 122 Total reward: 11.0 Training loss: 2.1114 Explore P: 0.7724\n",
      "Episode: 123 Total reward: 27.0 Training loss: 73.6840 Explore P: 0.7706\n",
      "Episode: 124 Total reward: 53.0 Training loss: 1.4873 Explore P: 0.7671\n",
      "Episode: 125 Total reward: 56.0 Training loss: 1.6812 Explore P: 0.7633\n",
      "Episode: 126 Total reward: 21.0 Training loss: 1.4400 Explore P: 0.7620\n",
      "Episode: 127 Total reward: 21.0 Training loss: 0.2778 Explore P: 0.7606\n",
      "Episode: 128 Total reward: 42.0 Training loss: 0.7240 Explore P: 0.7578\n",
      "Episode: 129 Total reward: 24.0 Training loss: 1.9122 Explore P: 0.7562\n",
      "Episode: 130 Total reward: 21.0 Training loss: 2.2156 Explore P: 0.7548\n",
      "Episode: 131 Total reward: 15.0 Training loss: 2.1133 Explore P: 0.7539\n",
      "Episode: 132 Total reward: 9.0 Training loss: 1.9295 Explore P: 0.7533\n",
      "Episode: 133 Total reward: 38.0 Training loss: 1.5799 Explore P: 0.7508\n",
      "Episode: 134 Total reward: 14.0 Training loss: 1.9818 Explore P: 0.7499\n",
      "Episode: 135 Total reward: 25.0 Training loss: 20.3045 Explore P: 0.7483\n",
      "Episode: 136 Total reward: 36.0 Training loss: 26.3973 Explore P: 0.7459\n",
      "Episode: 137 Total reward: 20.0 Training loss: 2.2882 Explore P: 0.7446\n",
      "Episode: 138 Total reward: 17.0 Training loss: 76.5406 Explore P: 0.7435\n",
      "Episode: 139 Total reward: 11.0 Training loss: 2.8745 Explore P: 0.7428\n",
      "Episode: 140 Total reward: 28.0 Training loss: 48.9531 Explore P: 0.7410\n",
      "Episode: 141 Total reward: 13.0 Training loss: 2.6625 Explore P: 0.7402\n",
      "Episode: 142 Total reward: 14.0 Training loss: 1.2405 Explore P: 0.7393\n",
      "Episode: 143 Total reward: 12.0 Training loss: 3.0734 Explore P: 0.7385\n",
      "Episode: 144 Total reward: 28.0 Training loss: 2.3873 Explore P: 0.7368\n",
      "Episode: 145 Total reward: 25.0 Training loss: 2.0710 Explore P: 0.7352\n",
      "Episode: 146 Total reward: 16.0 Training loss: 1.6668 Explore P: 0.7342\n",
      "Episode: 147 Total reward: 23.0 Training loss: 1.9704 Explore P: 0.7327\n",
      "Episode: 148 Total reward: 22.0 Training loss: 23.2998 Explore P: 0.7313\n",
      "Episode: 149 Total reward: 22.0 Training loss: 42.1932 Explore P: 0.7299\n",
      "Episode: 150 Total reward: 22.0 Training loss: 2.5107 Explore P: 0.7285\n",
      "Episode: 151 Total reward: 20.0 Training loss: 1.6459 Explore P: 0.7273\n",
      "Episode: 152 Total reward: 17.0 Training loss: 47.6017 Explore P: 0.7262\n",
      "Episode: 153 Total reward: 28.0 Training loss: 0.8518 Explore P: 0.7245\n",
      "Episode: 154 Total reward: 61.0 Training loss: 4.9145 Explore P: 0.7207\n",
      "Episode: 155 Total reward: 40.0 Training loss: 1.9854 Explore P: 0.7182\n",
      "Episode: 156 Total reward: 30.0 Training loss: 1.9381 Explore P: 0.7163\n",
      "Episode: 157 Total reward: 56.0 Training loss: 48.6238 Explore P: 0.7129\n",
      "Episode: 158 Total reward: 45.0 Training loss: 1.5254 Explore P: 0.7101\n",
      "Episode: 159 Total reward: 35.0 Training loss: 63.0335 Explore P: 0.7080\n",
      "Episode: 160 Total reward: 30.0 Training loss: 1.9690 Explore P: 0.7062\n",
      "Episode: 161 Total reward: 41.0 Training loss: 2.8798 Explore P: 0.7037\n",
      "Episode: 162 Total reward: 26.0 Training loss: 47.6816 Explore P: 0.7021\n",
      "Episode: 163 Total reward: 41.0 Training loss: 2.3732 Explore P: 0.6997\n",
      "Episode: 164 Total reward: 46.0 Training loss: 1.4083 Explore P: 0.6969\n",
      "Episode: 165 Total reward: 20.0 Training loss: 2.5146 Explore P: 0.6957\n",
      "Episode: 166 Total reward: 27.0 Training loss: 69.4100 Explore P: 0.6941\n",
      "Episode: 167 Total reward: 49.0 Training loss: 3.3086 Explore P: 0.6912\n",
      "Episode: 168 Total reward: 26.0 Training loss: 74.6898 Explore P: 0.6897\n",
      "Episode: 169 Total reward: 14.0 Training loss: 115.0106 Explore P: 0.6889\n",
      "Episode: 170 Total reward: 14.0 Training loss: 108.6169 Explore P: 0.6880\n",
      "Episode: 171 Total reward: 50.0 Training loss: 1.9625 Explore P: 0.6851\n",
      "Episode: 172 Total reward: 14.0 Training loss: 36.8756 Explore P: 0.6843\n",
      "Episode: 173 Total reward: 41.0 Training loss: 4.0172 Explore P: 0.6819\n",
      "Episode: 174 Total reward: 20.0 Training loss: 1.5213 Explore P: 0.6807\n",
      "Episode: 175 Total reward: 54.0 Training loss: 55.1719 Explore P: 0.6776\n",
      "Episode: 176 Total reward: 59.0 Training loss: 2.4758 Explore P: 0.6742\n",
      "Episode: 177 Total reward: 70.0 Training loss: 66.6378 Explore P: 0.6702\n",
      "Episode: 178 Total reward: 49.0 Training loss: 0.7979 Explore P: 0.6674\n",
      "Episode: 179 Total reward: 73.0 Training loss: 1.7485 Explore P: 0.6633\n",
      "Episode: 180 Total reward: 24.0 Training loss: 129.0555 Explore P: 0.6619\n",
      "Episode: 181 Total reward: 16.0 Training loss: 35.4852 Explore P: 0.6610\n",
      "Episode: 182 Total reward: 41.0 Training loss: 1.5941 Explore P: 0.6587\n",
      "Episode: 183 Total reward: 18.0 Training loss: 58.6734 Explore P: 0.6577\n",
      "Episode: 184 Total reward: 68.0 Training loss: 71.8614 Explore P: 0.6540\n",
      "Episode: 185 Total reward: 33.0 Training loss: 108.2626 Explore P: 0.6521\n",
      "Episode: 186 Total reward: 48.0 Training loss: 4.4029 Explore P: 0.6495\n",
      "Episode: 187 Total reward: 27.0 Training loss: 1.8781 Explore P: 0.6480\n",
      "Episode: 188 Total reward: 23.0 Training loss: 2.9747 Explore P: 0.6468\n",
      "Episode: 189 Total reward: 26.0 Training loss: 6.3835 Explore P: 0.6453\n",
      "Episode: 190 Total reward: 33.0 Training loss: 65.5677 Explore P: 0.6435\n",
      "Episode: 191 Total reward: 23.0 Training loss: 2.2533 Explore P: 0.6423\n",
      "Episode: 192 Total reward: 32.0 Training loss: 50.3050 Explore P: 0.6406\n",
      "Episode: 193 Total reward: 40.0 Training loss: 1.0499 Explore P: 0.6384\n",
      "Episode: 194 Total reward: 100.0 Training loss: 3.8965 Explore P: 0.6330\n",
      "Episode: 195 Total reward: 82.0 Training loss: 103.6125 Explore P: 0.6287\n",
      "Episode: 196 Total reward: 57.0 Training loss: 3.1541 Explore P: 0.6257\n",
      "Episode: 197 Total reward: 34.0 Training loss: 112.2290 Explore P: 0.6239\n",
      "Episode: 198 Total reward: 58.0 Training loss: 2.8151 Explore P: 0.6209\n",
      "Episode: 199 Total reward: 95.0 Training loss: 2.8864 Explore P: 0.6159\n",
      "Episode: 200 Total reward: 71.0 Training loss: 17.2930 Explore P: 0.6123\n",
      "Episode: 201 Total reward: 56.0 Training loss: 1.8980 Explore P: 0.6094\n",
      "Episode: 202 Total reward: 29.0 Training loss: 1.8140 Explore P: 0.6080\n",
      "Episode: 203 Total reward: 28.0 Training loss: 2.0336 Explore P: 0.6065\n",
      "Episode: 204 Total reward: 53.0 Training loss: 43.1645 Explore P: 0.6039\n",
      "Episode: 205 Total reward: 44.0 Training loss: 59.0535 Explore P: 0.6016\n",
      "Episode: 206 Total reward: 57.0 Training loss: 2.9689 Explore P: 0.5988\n",
      "Episode: 207 Total reward: 14.0 Training loss: 4.1544 Explore P: 0.5981\n",
      "Episode: 208 Total reward: 10.0 Training loss: 2.9960 Explore P: 0.5976\n",
      "Episode: 209 Total reward: 17.0 Training loss: 2.9737 Explore P: 0.5968\n",
      "Episode: 210 Total reward: 32.0 Training loss: 1.6920 Explore P: 0.5952\n",
      "Episode: 211 Total reward: 29.0 Training loss: 4.2356 Explore P: 0.5937\n",
      "Episode: 212 Total reward: 69.0 Training loss: 170.3257 Explore P: 0.5903\n",
      "Episode: 213 Total reward: 28.0 Training loss: 62.0985 Explore P: 0.5890\n",
      "Episode: 214 Total reward: 22.0 Training loss: 3.4588 Explore P: 0.5879\n",
      "Episode: 215 Total reward: 25.0 Training loss: 3.4134 Explore P: 0.5867\n",
      "Episode: 216 Total reward: 12.0 Training loss: 3.1059 Explore P: 0.5861\n",
      "Episode: 217 Total reward: 26.0 Training loss: 2.4273 Explore P: 0.5848\n",
      "Episode: 218 Total reward: 11.0 Training loss: 111.1724 Explore P: 0.5843\n",
      "Episode: 219 Total reward: 65.0 Training loss: 7.8368 Explore P: 0.5812\n",
      "Episode: 220 Total reward: 40.0 Training loss: 1.7213 Explore P: 0.5792\n",
      "Episode: 221 Total reward: 55.0 Training loss: 2.6709 Explore P: 0.5766\n",
      "Episode: 222 Total reward: 77.0 Training loss: 4.5109 Explore P: 0.5730\n",
      "Episode: 223 Total reward: 59.0 Training loss: 51.4517 Explore P: 0.5702\n",
      "Episode: 224 Total reward: 73.0 Training loss: 105.4673 Explore P: 0.5668\n",
      "Episode: 225 Total reward: 27.0 Training loss: 101.3617 Explore P: 0.5655\n",
      "Episode: 226 Total reward: 73.0 Training loss: 2.3083 Explore P: 0.5621\n",
      "Episode: 227 Total reward: 47.0 Training loss: 4.2325 Explore P: 0.5599\n",
      "Episode: 228 Total reward: 116.0 Training loss: 2.5855 Explore P: 0.5546\n",
      "Episode: 229 Total reward: 50.0 Training loss: 3.3933 Explore P: 0.5524\n",
      "Episode: 230 Total reward: 46.0 Training loss: 3.7403 Explore P: 0.5503\n",
      "Episode: 231 Total reward: 51.0 Training loss: 2.3712 Explore P: 0.5480\n",
      "Episode: 232 Total reward: 15.0 Training loss: 3.0539 Explore P: 0.5473\n",
      "Episode: 233 Total reward: 44.0 Training loss: 2.9151 Explore P: 0.5454\n",
      "Episode: 234 Total reward: 45.0 Training loss: 97.0554 Explore P: 0.5434\n",
      "Episode: 235 Total reward: 64.0 Training loss: 114.0582 Explore P: 0.5405\n",
      "Episode: 236 Total reward: 83.0 Training loss: 3.2818 Explore P: 0.5369\n",
      "Episode: 237 Total reward: 41.0 Training loss: 127.3661 Explore P: 0.5351\n",
      "Episode: 238 Total reward: 68.0 Training loss: 3.8434 Explore P: 0.5322\n",
      "Episode: 239 Total reward: 29.0 Training loss: 1.2326 Explore P: 0.5309\n",
      "Episode: 240 Total reward: 78.0 Training loss: 4.8712 Explore P: 0.5276\n",
      "Episode: 241 Total reward: 61.0 Training loss: 3.5467 Explore P: 0.5250\n",
      "Episode: 242 Total reward: 58.0 Training loss: 4.6339 Explore P: 0.5225\n",
      "Episode: 243 Total reward: 17.0 Training loss: 6.4599 Explore P: 0.5218\n",
      "Episode: 244 Total reward: 85.0 Training loss: 3.2161 Explore P: 0.5182\n",
      "Episode: 245 Total reward: 104.0 Training loss: 5.3233 Explore P: 0.5139\n",
      "Episode: 246 Total reward: 28.0 Training loss: 4.1344 Explore P: 0.5127\n",
      "Episode: 247 Total reward: 59.0 Training loss: 2.8226 Explore P: 0.5103\n",
      "Episode: 248 Total reward: 88.0 Training loss: 8.3360 Explore P: 0.5067\n",
      "Episode: 249 Total reward: 115.0 Training loss: 1.0907 Explore P: 0.5021\n",
      "Episode: 250 Total reward: 40.0 Training loss: 4.4562 Explore P: 0.5005\n",
      "Episode: 251 Total reward: 101.0 Training loss: 2.6964 Explore P: 0.4964\n",
      "Episode: 252 Total reward: 143.0 Training loss: 4.2368 Explore P: 0.4908\n",
      "Episode: 253 Total reward: 61.0 Training loss: 4.9512 Explore P: 0.4884\n",
      "Episode: 254 Total reward: 148.0 Training loss: 132.6966 Explore P: 0.4827\n",
      "Episode: 255 Total reward: 15.0 Training loss: 4.9502 Explore P: 0.4821\n",
      "Episode: 256 Total reward: 33.0 Training loss: 110.7123 Explore P: 0.4809\n",
      "Episode: 257 Total reward: 82.0 Training loss: 122.0011 Explore P: 0.4778\n",
      "Episode: 258 Total reward: 195.0 Training loss: 6.7378 Explore P: 0.4705\n",
      "Episode: 259 Total reward: 88.0 Training loss: 2.6970 Explore P: 0.4672\n",
      "Episode: 260 Total reward: 112.0 Training loss: 3.1953 Explore P: 0.4631\n",
      "Episode: 261 Total reward: 111.0 Training loss: 5.7868 Explore P: 0.4591\n",
      "Episode: 262 Total reward: 32.0 Training loss: 3.5962 Explore P: 0.4580\n",
      "Episode: 263 Total reward: 187.0 Training loss: 4.2968 Explore P: 0.4514\n",
      "Episode: 264 Total reward: 128.0 Training loss: 1.7742 Explore P: 0.4469\n",
      "Episode: 265 Total reward: 61.0 Training loss: 1.6341 Explore P: 0.4448\n",
      "Episode: 266 Total reward: 59.0 Training loss: 3.7490 Explore P: 0.4427\n",
      "Episode: 267 Total reward: 29.0 Training loss: 2.5088 Explore P: 0.4418\n",
      "Episode: 268 Total reward: 148.0 Training loss: 4.1611 Explore P: 0.4367\n",
      "Episode: 269 Total reward: 57.0 Training loss: 389.7439 Explore P: 0.4348\n",
      "Episode: 270 Total reward: 79.0 Training loss: 5.8451 Explore P: 0.4322\n",
      "Episode: 271 Total reward: 195.0 Training loss: 118.9026 Explore P: 0.4258\n",
      "Episode: 272 Total reward: 195.0 Training loss: 4.9191 Explore P: 0.4195\n",
      "Episode: 273 Total reward: 139.0 Training loss: 2.1984 Explore P: 0.4151\n",
      "Episode: 274 Total reward: 147.0 Training loss: 3.5972 Explore P: 0.4105\n",
      "Episode: 275 Total reward: 150.0 Training loss: 1.7937 Explore P: 0.4058\n",
      "Episode: 276 Total reward: 195.0 Training loss: 4.4248 Explore P: 0.3999\n",
      "Episode: 277 Total reward: 195.0 Training loss: 3.7911 Explore P: 0.3942\n",
      "Episode: 278 Total reward: 68.0 Training loss: 3.9203 Explore P: 0.3922\n",
      "Episode: 279 Total reward: 37.0 Training loss: 142.6249 Explore P: 0.3911\n",
      "Episode: 280 Total reward: 88.0 Training loss: 78.6779 Explore P: 0.3885\n",
      "Episode: 281 Total reward: 34.0 Training loss: 4.8391 Explore P: 0.3875\n",
      "Episode: 282 Total reward: 118.0 Training loss: 4.2272 Explore P: 0.3842\n",
      "Episode: 283 Total reward: 119.0 Training loss: 3.0339 Explore P: 0.3808\n",
      "Episode: 284 Total reward: 140.0 Training loss: 4.6647 Explore P: 0.3769\n",
      "Episode: 285 Total reward: 158.0 Training loss: 4.6681 Explore P: 0.3726\n",
      "Episode: 286 Total reward: 132.0 Training loss: 3.9857 Explore P: 0.3690\n",
      "Episode: 287 Total reward: 28.0 Training loss: 481.5614 Explore P: 0.3682\n",
      "Episode: 288 Total reward: 156.0 Training loss: 2.1259 Explore P: 0.3641\n",
      "Episode: 289 Total reward: 86.0 Training loss: 5.0071 Explore P: 0.3618\n",
      "Episode: 290 Total reward: 195.0 Training loss: 3.9941 Explore P: 0.3568\n",
      "Episode: 291 Total reward: 195.0 Training loss: 2.1988 Explore P: 0.3518\n",
      "Episode: 292 Total reward: 195.0 Training loss: 3.0562 Explore P: 0.3470\n",
      "Episode: 293 Total reward: 195.0 Training loss: 1.8717 Explore P: 0.3422\n",
      "Episode: 294 Total reward: 177.0 Training loss: 1.9589 Explore P: 0.3379\n",
      "Episode: 295 Total reward: 195.0 Training loss: 2.4860 Explore P: 0.3333\n",
      "Episode: 296 Total reward: 195.0 Training loss: 8.0244 Explore P: 0.3288\n",
      "Episode: 297 Total reward: 158.0 Training loss: 76.6308 Explore P: 0.3252\n",
      "Episode: 298 Total reward: 152.0 Training loss: 3.2609 Explore P: 0.3218\n",
      "Episode: 299 Total reward: 195.0 Training loss: 3.8325 Explore P: 0.3176\n",
      "Episode: 300 Total reward: 195.0 Training loss: 406.6972 Explore P: 0.3134\n",
      "Episode: 301 Total reward: 195.0 Training loss: 1.5850 Explore P: 0.3092\n",
      "Episode: 302 Total reward: 175.0 Training loss: 2.1654 Explore P: 0.3056\n",
      "Episode: 303 Total reward: 195.0 Training loss: 420.9322 Explore P: 0.3016\n",
      "Episode: 304 Total reward: 195.0 Training loss: 1.2141 Explore P: 0.2977\n",
      "Episode: 305 Total reward: 195.0 Training loss: 1.8071 Explore P: 0.2939\n",
      "Episode: 306 Total reward: 195.0 Training loss: 4.6880 Explore P: 0.2902\n",
      "Episode: 307 Total reward: 195.0 Training loss: 605.0326 Explore P: 0.2865\n",
      "Episode: 308 Total reward: 195.0 Training loss: 1.6797 Explore P: 0.2829\n",
      "Episode: 309 Total reward: 195.0 Training loss: 3.3276 Explore P: 0.2794\n",
      "Episode: 310 Total reward: 195.0 Training loss: 2.8481 Explore P: 0.2759\n",
      "Episode: 311 Total reward: 195.0 Training loss: 1.9665 Explore P: 0.2725\n",
      "Episode: 312 Total reward: 195.0 Training loss: 1.7915 Explore P: 0.2692\n",
      "Episode: 313 Total reward: 195.0 Training loss: 5.2477 Explore P: 0.2659\n",
      "Episode: 314 Total reward: 195.0 Training loss: 4.0453 Explore P: 0.2627\n",
      "Episode: 315 Total reward: 195.0 Training loss: 1018.0612 Explore P: 0.2596\n",
      "Episode: 316 Total reward: 195.0 Training loss: 0.1921 Explore P: 0.2565\n",
      "Episode: 317 Total reward: 195.0 Training loss: 2.3689 Explore P: 0.2535\n",
      "Episode: 318 Total reward: 195.0 Training loss: 0.1977 Explore P: 0.2505\n",
      "Episode: 319 Total reward: 195.0 Training loss: 1.0942 Explore P: 0.2476\n",
      "Episode: 320 Total reward: 195.0 Training loss: 1.4111 Explore P: 0.2447\n",
      "Episode: 321 Total reward: 195.0 Training loss: 2.9553 Explore P: 0.2420\n",
      "Episode: 322 Total reward: 195.0 Training loss: 1.2989 Explore P: 0.2392\n",
      "Episode: 323 Total reward: 195.0 Training loss: 3.6018 Explore P: 0.2365\n",
      "Episode: 324 Total reward: 195.0 Training loss: 0.6688 Explore P: 0.2339\n",
      "Episode: 325 Total reward: 195.0 Training loss: 0.7526 Explore P: 0.2313\n",
      "Episode: 326 Total reward: 195.0 Training loss: 0.5854 Explore P: 0.2288\n",
      "Episode: 327 Total reward: 195.0 Training loss: 1.5907 Explore P: 0.2263\n",
      "Episode: 328 Total reward: 195.0 Training loss: 0.7917 Explore P: 0.2238\n",
      "Episode: 329 Total reward: 195.0 Training loss: 0.5840 Explore P: 0.2214\n",
      "Episode: 330 Total reward: 195.0 Training loss: 2.8320 Explore P: 0.2191\n",
      "Episode: 331 Total reward: 195.0 Training loss: 0.8949 Explore P: 0.2168\n",
      "Episode: 332 Total reward: 195.0 Training loss: 0.6926 Explore P: 0.2145\n",
      "Episode: 333 Total reward: 195.0 Training loss: 0.5614 Explore P: 0.2123\n",
      "Episode: 334 Total reward: 195.0 Training loss: 0.8678 Explore P: 0.2102\n",
      "Episode: 335 Total reward: 195.0 Training loss: 0.8408 Explore P: 0.2080\n",
      "Episode: 336 Total reward: 195.0 Training loss: 0.9846 Explore P: 0.2060\n",
      "Episode: 337 Total reward: 195.0 Training loss: 1.2079 Explore P: 0.2039\n",
      "Episode: 338 Total reward: 195.0 Training loss: 1.7139 Explore P: 0.2019\n",
      "Episode: 339 Total reward: 195.0 Training loss: 1.2503 Explore P: 0.1999\n",
      "Episode: 340 Total reward: 195.0 Training loss: 3.2984 Explore P: 0.1980\n",
      "Episode: 341 Total reward: 195.0 Training loss: 1.2851 Explore P: 0.1961\n",
      "Episode: 342 Total reward: 195.0 Training loss: 1.8157 Explore P: 0.1943\n",
      "Episode: 343 Total reward: 195.0 Training loss: 0.5559 Explore P: 0.1924\n",
      "Episode: 344 Total reward: 195.0 Training loss: 1.7220 Explore P: 0.1906\n",
      "Episode: 345 Total reward: 195.0 Training loss: 0.6524 Explore P: 0.1889\n",
      "Episode: 346 Total reward: 195.0 Training loss: 0.4414 Explore P: 0.1872\n",
      "Episode: 347 Total reward: 195.0 Training loss: 0.1584 Explore P: 0.1855\n",
      "Episode: 348 Total reward: 195.0 Training loss: 0.6032 Explore P: 0.1838\n",
      "Episode: 349 Total reward: 195.0 Training loss: 0.1002 Explore P: 0.1822\n",
      "Episode: 350 Total reward: 195.0 Training loss: 0.7079 Explore P: 0.1806\n",
      "Episode: 351 Total reward: 195.0 Training loss: 0.1020 Explore P: 0.1791\n",
      "Episode: 352 Total reward: 195.0 Training loss: 0.1266 Explore P: 0.1776\n",
      "Episode: 353 Total reward: 195.0 Training loss: 0.0943 Explore P: 0.1761\n",
      "Episode: 354 Total reward: 195.0 Training loss: 0.2560 Explore P: 0.1746\n",
      "Episode: 355 Total reward: 195.0 Training loss: 0.0814 Explore P: 0.1731\n",
      "Episode: 356 Total reward: 195.0 Training loss: 0.2887 Explore P: 0.1717\n",
      "Episode: 357 Total reward: 195.0 Training loss: 0.1502 Explore P: 0.1704\n",
      "Episode: 358 Total reward: 195.0 Training loss: 0.0430 Explore P: 0.1690\n",
      "Episode: 359 Total reward: 195.0 Training loss: 0.1173 Explore P: 0.1677\n",
      "Episode: 360 Total reward: 195.0 Training loss: 0.0850 Explore P: 0.1664\n",
      "Episode: 361 Total reward: 195.0 Training loss: 0.0971 Explore P: 0.1651\n",
      "Episode: 362 Total reward: 195.0 Training loss: 0.0534 Explore P: 0.1638\n",
      "Episode: 363 Total reward: 195.0 Training loss: 0.0210 Explore P: 0.1626\n",
      "Episode: 364 Total reward: 195.0 Training loss: 0.0550 Explore P: 0.1614\n",
      "Episode: 365 Total reward: 195.0 Training loss: 0.0402 Explore P: 0.1602\n",
      "Episode: 366 Total reward: 195.0 Training loss: 0.0392 Explore P: 0.1590\n",
      "Episode: 367 Total reward: 195.0 Training loss: 0.0452 Explore P: 0.1579\n",
      "Episode: 368 Total reward: 195.0 Training loss: 0.0255 Explore P: 0.1568\n",
      "Episode: 369 Total reward: 195.0 Training loss: 0.0163 Explore P: 0.1557\n",
      "Episode: 370 Total reward: 195.0 Training loss: 0.0566 Explore P: 0.1546\n",
      "Episode: 371 Total reward: 195.0 Training loss: 0.0516 Explore P: 0.1535\n",
      "Episode: 372 Total reward: 195.0 Training loss: 0.0370 Explore P: 0.1525\n",
      "Episode: 373 Total reward: 195.0 Training loss: 0.0408 Explore P: 0.1515\n",
      "Episode: 374 Total reward: 195.0 Training loss: 0.1082 Explore P: 0.1505\n",
      "Episode: 375 Total reward: 195.0 Training loss: 0.0115 Explore P: 0.1495\n",
      "Episode: 376 Total reward: 195.0 Training loss: 0.0174 Explore P: 0.1486\n",
      "Episode: 377 Total reward: 195.0 Training loss: 0.0083 Explore P: 0.1476\n",
      "Episode: 378 Total reward: 195.0 Training loss: 0.1375 Explore P: 0.1467\n",
      "Episode: 379 Total reward: 195.0 Training loss: 0.0132 Explore P: 0.1458\n",
      "Episode: 380 Total reward: 195.0 Training loss: 0.0166 Explore P: 0.1449\n",
      "Episode: 381 Total reward: 195.0 Training loss: 0.0155 Explore P: 0.1441\n",
      "Episode: 382 Total reward: 195.0 Training loss: 0.0134 Explore P: 0.1432\n",
      "Episode: 383 Total reward: 195.0 Training loss: 0.0182 Explore P: 0.1424\n",
      "Episode: 384 Total reward: 195.0 Training loss: 0.0111 Explore P: 0.1416\n",
      "Episode: 385 Total reward: 195.0 Training loss: 0.0893 Explore P: 0.1408\n",
      "Episode: 386 Total reward: 195.0 Training loss: 0.0083 Explore P: 0.1400\n",
      "Episode: 387 Total reward: 195.0 Training loss: 0.0044 Explore P: 0.1392\n",
      "Episode: 388 Total reward: 195.0 Training loss: 0.0212 Explore P: 0.1384\n",
      "Episode: 389 Total reward: 195.0 Training loss: 0.0045 Explore P: 0.1377\n",
      "Episode: 390 Total reward: 195.0 Training loss: 0.0040 Explore P: 0.1370\n",
      "Episode: 391 Total reward: 195.0 Training loss: 0.0027 Explore P: 0.1363\n",
      "Episode: 392 Total reward: 195.0 Training loss: 0.0055 Explore P: 0.1356\n",
      "Episode: 393 Total reward: 195.0 Training loss: 0.0050 Explore P: 0.1349\n",
      "Episode: 394 Total reward: 195.0 Training loss: 0.0029 Explore P: 0.1342\n",
      "Episode: 395 Total reward: 195.0 Training loss: 0.0026 Explore P: 0.1335\n",
      "Episode: 396 Total reward: 195.0 Training loss: 0.0012 Explore P: 0.1329\n",
      "Episode: 397 Total reward: 195.0 Training loss: 0.0012 Explore P: 0.1322\n",
      "Episode: 398 Total reward: 195.0 Training loss: 0.0019 Explore P: 0.1316\n",
      "Episode: 399 Total reward: 195.0 Training loss: 0.0011 Explore P: 0.1310\n"
     ]
    }
   ],
   "source": [
    "test,train = test_and_train_qnetwork(train_episodes=500,\\\n",
    "                                           gamma=0.99,\\\n",
    "                                           explore_start=exp_start,\\\n",
    "                                           explore_stop=exp_stop,\\\n",
    "                                           decay_rate=0.0001,\\\n",
    "                                           hidden_layers=1,\\\n",
    "                                           hidden_size=64,\\\n",
    "                                           learning_rate=0.001,\\\n",
    "                                           batch_size=10,\\\n",
    "                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.050000000000022\n",
      "100.14\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
