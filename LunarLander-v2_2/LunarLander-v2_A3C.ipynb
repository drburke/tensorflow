{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed()\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x,alpha=0.02):\n",
    "    return tf.maximum(alpha*x,x)\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create class QNetwork\n",
    "class A3CNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=8, \n",
    "                 action_size=4, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0.1, \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # Dropout\n",
    "        self.keep_prob_ = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "        # State\n",
    "        self.state_ = tf.placeholder(tf.float32,[None, state_size],name='state')\n",
    "        \n",
    "        # Actions, not one hot\n",
    "        self.actions_ = tf.placeholder(tf.int32,[None],name='actions')\n",
    "\n",
    "        # Actions, one hot\n",
    "        self.one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "        \n",
    "        # R value\n",
    "        self.R_ = tf.placeholder(tf.float32,[None,1],name='R')\n",
    "        \n",
    "        self.value_ = tf.placeholder(tf.float32,[None,1],name='value_input')\n",
    "        \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "#             self.fcl_weights = tf.Variable(tf.truncated_normal((state_size, hidden_size), mean=0.0, stddev=0.1),name='weights') \n",
    "#             self.fcl_bias = tf.Variable(tf.zeros(hidden_size),name=\"bias\")\n",
    "#             self.fcl_sum = tf.add(tf.matmul(self.state_, self.fcl_weights), self.fcl_bias)\n",
    "#             self.fcl_relu = leaky_relu(fcl)\n",
    "            self.fcl = tf.layers.dense(self.state_, hidden_size,activation=None,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fcl = leaky_relu(self.fcl)\n",
    "    \n",
    "        with tf.variable_scope(\"policy\"):\n",
    "#             self.policy_weights = tf.Variable(tf.truncated_normal((hidden_size,action_size)),name=\"weights\")\n",
    "#             self.policy_bias = tf.Variable(tf.zeros(action_size),name=\"bias\")\n",
    "#             self.policy = tf.add(tf.matmul(self.fcl_relu,self.policy_weights),self.policy_bias)\n",
    "            self.policy = tf.layers.dense(self.fcl, action_size,activation=None,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.policy_softmax = tf.nn.softmax(self.policy)\n",
    "            self.log_policy_softmax = tf.log(self.policy_softmax)\n",
    "        \n",
    "        with tf.variable_scope(\"value\"):\n",
    "#             self.value_weights = tf.Variable(tf.truncated_normal((hidden_size,1)),name=\"weights\")\n",
    "#             self.value_bias = tf.Variable(tf.zeros(1),name=\"bias\")\n",
    "#             self.value = tf.add(tf.matmul(self.fcl_relu,self.value_weights),self.value_bias)\n",
    "            self.value = tf.layers.dense(self.fcl, 1,activation=None,kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.policy_var = [var for var in t_vars if (var.name.startswith('policy') or var.name.startswith('encoder'))]\n",
    "        self.value_var = [var for var in t_vars if var.name.startswith('value') or var.name.startswith('encoder')]\n",
    "        \n",
    "        self.policy_loss = -tf.reduce_mean(tf.multiply(self.log_policy_softmax , (self.R_ - self.value_))) - \\\n",
    "            (-0.0001*tf.reduce_mean(tf.multiply(self.policy_softmax,self.log_policy_softmax)))\n",
    "        self.value_loss = tf.reduce_mean(tf.square(self.R_ - self.value))\n",
    "        \n",
    "        self.policy_opt = tf.train.AdamOptimizer(learning_rate).minimize(self.policy_loss)\n",
    "        self.value_opt = tf.train.AdamOptimizer(learning_rate).minimize(self.value_loss)\n",
    "        \n",
    "    def reset_gradients(self):\n",
    "        \n",
    "        self.fcl_weights_grad = tf.zeros(self.fcl_weights.get_shape().as_list())\n",
    "        self.fcl_bias_grad = tf.zeros(self.fcl_bias.get_shape().as_list())\n",
    "        \n",
    "        self.policy_weights_grad = tf.zeros(self.policy_weights.get_shape().as_list())\n",
    "        self.policy_bias_grad = tf.zeros(self.policy_bias.get_shape().as_list())\n",
    "        \n",
    "        self.value_weights_grad = tf.zeros(self.value_weights.get_shape().as_list())\n",
    "        self.value_bias_grad = tf.zeros(self.value_bias.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 10000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=True)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "    \n",
    "    def pull_all(self):\n",
    "        return self.buffer\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = deque(maxlen=self.max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_state(x, denormalize=False):\n",
    "    # Rough max/min extent for states, normalize to +/- 1\n",
    "    # [-1,1] [-0.2,1.2] [-2,2] [0.5,-2]  [3.5,-3.5]  [6,-6] [1,0]  [1,0]\n",
    "    y = x / [1.,1.,2.,1.5,3.5,6.,1.,1.]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_a3c_network(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   max_steps=5000,\\\n",
    "                   alpha=0.1,\\\n",
    "                   verbose=True,\\\n",
    "                   num_trains=50,\\\n",
    "                   num_bots=16,\\\n",
    "                   action_size=4):\n",
    "    \n",
    "    \n",
    "    # Create the network\n",
    "    mainQN = A3CNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    # Memory for asynchronous replay\n",
    "    memory = Memory(max_size=memory_size)\n",
    "    \n",
    "    # Reset state, normalize\n",
    "    state = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    #  Create output variables\n",
    "    total_rewards_tf = tf.placeholder(tf.float32, None, name='total_rewards')\n",
    "    max_q_tf = tf.placeholder(tf.float32, None, name='max_qs')\n",
    "\n",
    "    # Add scalar summary trackers\n",
    "    tf.summary.scalar('total_reward', total_rewards_tf)\n",
    "    tf.summary.scalar('max_q', max_q_tf)\n",
    "    merged_tf = tf.summary.merge_all()\n",
    "    \n",
    "\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    rewards_step_list = []\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Create file writer\n",
    "        file_writer = tf.summary.FileWriter(log_path,sess.graph)\n",
    "        \n",
    "        step = 0\n",
    "        rewards_list = []\n",
    "        \n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            \n",
    "            do_render = os.path.isfile('./render.txt')\n",
    "            biggest_target = -9e9\n",
    "            done = 0\n",
    "            memory.clear()\n",
    "            \n",
    "            for bot in range(num_bots):\n",
    "                t = 0\n",
    "                prev_reward = 0\n",
    "                R = 0\n",
    "                while not done:\n",
    "                    step += 1\n",
    "                    \n",
    "                    if do_render:\n",
    "                        env.render()  \n",
    "\n",
    "                    \n",
    "                    # Get action from policy-network\n",
    "                    feed = {mainQN.state_: state.reshape((1, *state.shape))}\n",
    "                    Qs,Qraw,Value = sess.run([mainQN.policy_softmax,mainQN.policy,mainQN.value], feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "                    \n",
    "                    # Choose random action based on softmax probabilities\n",
    "                    rand = np.random.rand()\n",
    "                    action = 0\n",
    "                    sum_iter = Qs[0,action]\n",
    "#                     print(Qs)\n",
    "#                     print(num_iter)\n",
    "#                     print(sum_iter)\n",
    "                    while sum_iter < rand:\n",
    "                        action += 1\n",
    "#                         print(num_iter)\n",
    "#                         print(Qs[num_iter])\n",
    "#                         print('sum=',sum_iter,'  rand=',rand)\n",
    "                        sum_iter += Qs[0,action]\n",
    "#                         if sum_iter >= rand:\n",
    "#                             print('final sum=',sum_iter,'  rand=',rand)\n",
    "#                             break                    \n",
    "\n",
    "                    \n",
    "#                     print(state,'  Qs=',Qs,'  a=',action)\n",
    "                    Qraw_max = np.max(Qraw)\n",
    "                    biggest_target = np.maximum(Qraw_max,biggest_target)\n",
    "                \n",
    "\n",
    "                    # Take action, get new state and reward\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    new_reward = reward\n",
    "#                     reward -= prev_reward\n",
    "                    prev_reward = new_reward\n",
    "                    R = reward + gamma * R\n",
    "                    \n",
    "                    next_state = normalize_state(next_state)\n",
    "                    state = next_state\n",
    "                    total_reward += reward\n",
    "                    t += 1\n",
    "                                   \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, Qs, reward, done, Value, R))\n",
    "\n",
    "                \n",
    "                rewards_step_list.append(total_reward)\n",
    "                state = env.reset()\n",
    "                state = normalize_state(state)  \n",
    "                done = 0\n",
    "                    \n",
    "            \n",
    "  \n",
    "              # Sample mini-batch from memory\n",
    "            batch = memory.pull_all()\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            dones = np.array([each[3] for each in batch])\n",
    "            values = np.array([each[4] for each in batch])\n",
    "            Rs = np.array([each[5] for each in batch])\n",
    "            \n",
    "#             # Now bot updates gradients\n",
    "#             for i in range(len(dones)):\n",
    "\n",
    "#                 if dones[i] == 1:\n",
    "#                     R = 0\n",
    "#                 else:\n",
    "#                     R = rewards[i] + gamma * R\n",
    "\n",
    "#                 print('R=',R,'  reward[i]=',rewards[i])\n",
    "#                 memory.add((state, Qs, reward, done, Value))\n",
    "                \n",
    "#             print('states=',np.shape(states))\n",
    "#             print('actions=',np.shape(actions))\n",
    "            re_actions = np.squeeze(actions)\n",
    "\n",
    "            re_values = np.squeeze(values,axis=2)\n",
    "\n",
    "            re_rs = np.reshape(Rs,(len(Rs),1))\n",
    "            \n",
    "            policy_loss, _, value_loss, _ = sess.run([mainQN.policy_loss,mainQN.policy_opt,mainQN.value_loss,mainQN.value_opt],\n",
    "                                                    feed_dict={mainQN.state_:states,\n",
    "                                                              mainQN.R_:re_rs,\n",
    "                                                              mainQN.value_:re_values})\n",
    "# ,\n",
    "#                                                               mainQN.policy_softmax:re_actions,\n",
    "#                                                               mainQN.value:re_values\n",
    "\n",
    "            total_reward = total_reward / num_bots\n",
    "            rewards_list.append((ep, total_reward))   \n",
    "            runningMean = np.mean(rewards_step_list[-100:])\n",
    "\n",
    "            summary = sess.run(merged_tf, feed_dict={total_rewards_tf: total_reward, \n",
    "                                                     max_q_tf: biggest_target})\n",
    "            file_writer.add_summary(summary,ep)\n",
    "            if verbose:\n",
    "\n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'TReward: {}'.format(total_reward),\n",
    "                      'RunMean : {:.4f}'.format(runningMean),\n",
    "                      'MaxTarg : {:.4f}'.format(biggest_target))\n",
    "               \n",
    "#             if ep>0:\n",
    "#                 return rewards_list, mainQN, saver, runningMean\n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver, runningMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_and_train_qnetwork(train_episodes=1000,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   test_episodes=10,\\\n",
    "                   render=False,\\\n",
    "                   alpha=0.,\\\n",
    "                   verbose=True,\\\n",
    "                   num_trains=50):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver, runningMean = train_a3c_network(train_episodes = train_episodes, \\\n",
    "                                                  gamma=gamma,\\\n",
    "                                                  explore_start=explore_start,\\\n",
    "                                                  explore_stop=explore_stop,\\\n",
    "                                                  decay_rate=decay_rate,\\\n",
    "                                                  hidden_size=hidden_size,\\\n",
    "                                                  hidden_layers=hidden_layers,\\\n",
    "                                                  learning_rate=learning_rate,\\\n",
    "                                                  memory_size=memory_size,\\\n",
    "                                                  batch_size=batch_size,\\\n",
    "                                                  alpha=alpha,\\\n",
    "                                                  verbose=verbose,\\\n",
    "                                                  num_trains=num_trains)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    \n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    \n",
    "    return avg_train_rewards, mainQN, saver, len(rewards_list), runningMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_eps = 2000\n",
    "# verb = False\n",
    "# gamma = [0.99]\n",
    "# decay_rate = [0.0001]\n",
    "# exp_start=1.0\n",
    "# exp_stop=0.1\n",
    "# hidden_size=[64]\n",
    "\n",
    "# learning_rate=[0.001]\n",
    "# batch_size=[128]\n",
    "# num_averages = 1\n",
    "# results = []\n",
    "# alpha_relu = [0.1,0.02]\n",
    "# mem_sizes = [100000]\n",
    "# num_trains = [128]\n",
    "\n",
    "# env = gym.make('LunarLander-v2')\n",
    "# env.reset()\n",
    "\n",
    "# #log_path = './logs/2/logs_exp_1.0_0.05_no_third_conv_max_record'\n",
    "\n",
    "# for gaIndex in range(len(gamma)):\n",
    "#     for drIndex in range(len(decay_rate)):\n",
    "#         for hs in hidden_size:\n",
    "#             for lr in learning_rate:\n",
    "#                 for bs in batch_size:\n",
    "#                     for memS in mem_sizes:\n",
    "#                         for numTrs in num_trains:\n",
    "#                             ga = gamma[gaIndex]\n",
    "#                             dr = decay_rate[drIndex]\n",
    "#                             log_path = './logs/12/dr1='+str(dr)+'_ga='+str(ga)+'_hs='+str(hs)+'_lr'+str(lr)+'_bs'+str(bs)+'_ms='+str(memS)+'_nt='+str(numTrs)\n",
    "#                             average_test_rewards = 0.\n",
    "#                             average_train_rewards = 0.\n",
    "#                             for i in range(num_averages):\n",
    "#                                 test,train, mainQN, saver, num_episodes, runningMean = test_and_train_qnetwork(memory_size=memS,\\\n",
    "#                                                        train_episodes=train_eps,\\\n",
    "#                                                        gamma=ga,\\\n",
    "#                                                        explore_start=exp_start,\\\n",
    "#                                                        explore_stop=exp_stop,\\\n",
    "#                                                        decay_rate=dr,\\\n",
    "#                                                        hidden_size=hs,\\\n",
    "#                                                        learning_rate=lr,\\\n",
    "#                                                        batch_size=bs,\\\n",
    "#                                                        alpha = 0.05,\\\n",
    "#                                                        num_trains = numTrs,\\\n",
    "#                                                        verbose=verb)\n",
    "#                                 average_test_rewards += test\n",
    "#                                 average_train_rewards += train\n",
    "\n",
    "#                             average_test_rewards = average_test_rewards / num_averages\n",
    "#                             average_train_rewards = average_train_rewards / num_averages\n",
    "#                             results.append([log_path+' test='+str(average_test_rewards)+'  numEps='+str(num_episodes)+'  runMean='+str(runningMean)])\n",
    "#                             clear_output()\n",
    "#                             for each in results:\n",
    "#                                 print(each)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-12 22:01:40,611] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 TReward: -261.8098345713502 RunMean : -2368.9293 MaxTarg : 0.1432\n",
      "Episode: 1 TReward: -243.95741890378952 RunMean : -2205.9949 MaxTarg : 0.1488\n",
      "Episode: 2 TReward: -205.86159943423 RunMean : -2064.8639 MaxTarg : 0.1465\n",
      "Episode: 3 TReward: -223.14224988724743 RunMean : -2028.6448 MaxTarg : 0.1869\n",
      "Episode: 4 TReward: -221.8009779209162 RunMean : -1967.1877 MaxTarg : 0.2091\n",
      "Episode: 5 TReward: -239.58225510066285 RunMean : -1943.7117 MaxTarg : 0.1896\n",
      "Episode: 6 TReward: -253.0861656960655 RunMean : -2003.6412 MaxTarg : 0.1828\n",
      "Episode: 7 TReward: -196.56212600878857 RunMean : -1922.7905 MaxTarg : 0.1875\n",
      "Episode: 8 TReward: -239.2324600683204 RunMean : -1962.0696 MaxTarg : 0.1913\n",
      "Episode: 9 TReward: -250.2475401818743 RunMean : -2012.9197 MaxTarg : 0.2266\n",
      "Episode: 10 TReward: -234.2407638784457 RunMean : -2091.6357 MaxTarg : 0.2130\n",
      "Episode: 11 TReward: -215.3962100991994 RunMean : -2108.8536 MaxTarg : 0.2258\n",
      "Episode: 12 TReward: -193.71873822568784 RunMean : -2014.9876 MaxTarg : 0.1431\n",
      "Episode: 13 TReward: -235.08180447513004 RunMean : -2024.4548 MaxTarg : 0.1719\n",
      "Episode: 14 TReward: -220.79560562214022 RunMean : -2001.5422 MaxTarg : 0.2137\n",
      "Episode: 15 TReward: -204.76794586837923 RunMean : -1933.9576 MaxTarg : 0.1671\n",
      "Episode: 16 TReward: -232.7347518796231 RunMean : -1850.4920 MaxTarg : 0.1931\n",
      "Episode: 17 TReward: -262.70275526219785 RunMean : -1881.8546 MaxTarg : 0.2622\n",
      "Episode: 18 TReward: -252.2122795135288 RunMean : -1957.7036 MaxTarg : 0.2300\n",
      "Episode: 19 TReward: -223.110397819545 RunMean : -1996.5387 MaxTarg : 0.2694\n",
      "Episode: 20 TReward: -247.9382479675362 RunMean : -2023.2331 MaxTarg : 0.2048\n",
      "Episode: 21 TReward: -182.53621071845313 RunMean : -1981.5568 MaxTarg : 0.2230\n",
      "Episode: 22 TReward: -209.195061752804 RunMean : -1992.0476 MaxTarg : 0.1663\n",
      "Episode: 23 TReward: -226.60342637790745 RunMean : -1963.0589 MaxTarg : 0.2430\n",
      "Episode: 24 TReward: -252.2275736500151 RunMean : -1945.4672 MaxTarg : 0.2452\n",
      "Episode: 25 TReward: -224.80419306122064 RunMean : -1936.7656 MaxTarg : 0.2341\n",
      "Episode: 26 TReward: -277.020012080655 RunMean : -1992.4732 MaxTarg : 0.2843\n",
      "Episode: 27 TReward: -216.8977676538013 RunMean : -2044.4168 MaxTarg : 0.1915\n",
      "Episode: 28 TReward: -251.0753050173588 RunMean : -2139.5135 MaxTarg : 0.2805\n",
      "Episode: 29 TReward: -231.9738357216546 RunMean : -2186.5291 MaxTarg : 0.2089\n",
      "Episode: 30 TReward: -220.46992740897764 RunMean : -2197.0768 MaxTarg : 0.2500\n",
      "Episode: 31 TReward: -242.49384850771855 RunMean : -2209.5832 MaxTarg : 0.2748\n",
      "Episode: 32 TReward: -255.95569967608066 RunMean : -2194.6177 MaxTarg : 0.2834\n",
      "Episode: 33 TReward: -254.6822347889846 RunMean : -2177.2547 MaxTarg : 0.2986\n",
      "Episode: 34 TReward: -234.17265434747145 RunMean : -2169.5686 MaxTarg : 0.2859\n",
      "Episode: 35 TReward: -218.3975195248325 RunMean : -2086.7826 MaxTarg : 0.3159\n",
      "Episode: 36 TReward: -229.32160158748752 RunMean : -2013.7523 MaxTarg : 0.2123\n",
      "Episode: 37 TReward: -233.9302005450724 RunMean : -1993.1789 MaxTarg : 0.3226\n",
      "Episode: 38 TReward: -277.07821471960347 RunMean : -2011.4237 MaxTarg : 0.2508\n",
      "Episode: 39 TReward: -248.99466053984236 RunMean : -2032.0954 MaxTarg : 0.2914\n",
      "Episode: 40 TReward: -230.27531270706217 RunMean : -2017.9562 MaxTarg : 0.2608\n",
      "Episode: 41 TReward: -219.31200216206983 RunMean : -2044.7534 MaxTarg : 0.3406\n",
      "Episode: 42 TReward: -229.05051274374267 RunMean : -2104.4235 MaxTarg : 0.2414\n",
      "Episode: 43 TReward: -306.26594851460806 RunMean : -2182.8481 MaxTarg : 0.3092\n",
      "Episode: 44 TReward: -225.53899445086483 RunMean : -2154.3865 MaxTarg : 0.3087\n",
      "Episode: 45 TReward: -282.9739736844767 RunMean : -2163.5037 MaxTarg : 0.3456\n",
      "Episode: 46 TReward: -209.72106546871854 RunMean : -2126.3610 MaxTarg : 0.3005\n",
      "Episode: 47 TReward: -210.77894149153943 RunMean : -2079.8267 MaxTarg : 0.3163\n",
      "Episode: 48 TReward: -277.2919311570289 RunMean : -2153.2478 MaxTarg : 0.3640\n",
      "Episode: 49 TReward: -229.99066967936037 RunMean : -2091.9554 MaxTarg : 0.3108\n",
      "Episode: 50 TReward: -218.334483571622 RunMean : -2090.7899 MaxTarg : 0.3363\n",
      "Episode: 51 TReward: -229.96420810656522 RunMean : -2061.0913 MaxTarg : 0.2431\n",
      "Episode: 52 TReward: -211.82780396697223 RunMean : -1997.0857 MaxTarg : 0.3152\n",
      "Episode: 53 TReward: -237.7130680104355 RunMean : -2015.3887 MaxTarg : 0.3350\n",
      "Episode: 54 TReward: -245.0292786739048 RunMean : -2006.7609 MaxTarg : 0.3318\n",
      "Episode: 55 TReward: -277.0025312829865 RunMean : -2056.3227 MaxTarg : 0.3649\n",
      "Episode: 56 TReward: -222.78235663685294 RunMean : -1989.7517 MaxTarg : 0.4039\n",
      "Episode: 57 TReward: -274.1086335523498 RunMean : -2075.1719 MaxTarg : 0.3390\n",
      "Episode: 58 TReward: -293.44662143817214 RunMean : -2132.9069 MaxTarg : 0.4081\n",
      "Episode: 59 TReward: -253.2033693988249 RunMean : -2168.5375 MaxTarg : 0.4073\n",
      "Episode: 60 TReward: -288.1937820158626 RunMean : -2238.1456 MaxTarg : 0.3782\n",
      "Episode: 61 TReward: -196.01500273185778 RunMean : -2150.5504 MaxTarg : 0.3421\n",
      "Episode: 62 TReward: -267.72127133816514 RunMean : -2210.5681 MaxTarg : 0.3779\n",
      "Episode: 63 TReward: -270.4571360326781 RunMean : -2189.0965 MaxTarg : 0.4824\n",
      "Episode: 64 TReward: -230.39790390886225 RunMean : -2185.8370 MaxTarg : 0.4074\n",
      "Episode: 65 TReward: -206.29928393648086 RunMean : -2113.3592 MaxTarg : 0.4106\n",
      "Episode: 66 TReward: -234.35599486611918 RunMean : -2074.9175 MaxTarg : 0.4159\n",
      "Episode: 67 TReward: -232.64678802609558 RunMean : -2062.6924 MaxTarg : 0.4180\n",
      "Episode: 68 TReward: -267.24267048402766 RunMean : -2112.0857 MaxTarg : 0.4750\n",
      "Episode: 69 TReward: -202.71314490195212 RunMean : -2047.3043 MaxTarg : 0.4310\n",
      "Episode: 70 TReward: -180.46048226133308 RunMean : -1933.0314 MaxTarg : 0.3538\n",
      "Episode: 71 TReward: -170.42004599472634 RunMean : -1859.2713 MaxTarg : 0.3922\n",
      "Episode: 72 TReward: -270.54470980678707 RunMean : -1900.7187 MaxTarg : 0.4420\n",
      "Episode: 73 TReward: -205.06432158310724 RunMean : -1849.6567 MaxTarg : 0.3638\n",
      "Episode: 74 TReward: -287.777524894408 RunMean : -1906.7066 MaxTarg : 0.4534\n",
      "Episode: 75 TReward: -219.1059987847399 RunMean : -1861.0886 MaxTarg : 0.4444\n",
      "Episode: 76 TReward: -238.9149434689124 RunMean : -1916.4244 MaxTarg : 0.4601\n",
      "Episode: 77 TReward: -248.60980040452088 RunMean : -1973.6020 MaxTarg : 0.4999\n",
      "Episode: 78 TReward: -264.42203031897964 RunMean : -2023.0733 MaxTarg : 0.4875\n",
      "Episode: 79 TReward: -240.8237015510716 RunMean : -2067.9155 MaxTarg : 0.5205\n",
      "Episode: 80 TReward: -224.31452111581902 RunMean : -2020.7696 MaxTarg : 0.4537\n",
      "Episode: 81 TReward: -281.8147362808138 RunMean : -2111.8644 MaxTarg : 0.4076\n",
      "Episode: 82 TReward: -253.0476511870947 RunMean : -2149.1309 MaxTarg : 0.5161\n",
      "Episode: 83 TReward: -230.2036588541119 RunMean : -2204.1953 MaxTarg : 0.4889\n",
      "Episode: 84 TReward: -249.71014815825745 RunMean : -2183.9660 MaxTarg : 0.5141\n",
      "Episode: 85 TReward: -332.21132550847267 RunMean : -2257.9619 MaxTarg : 0.5318\n",
      "Episode: 86 TReward: -275.3140446317148 RunMean : -2310.3487 MaxTarg : 0.4939\n",
      "Episode: 87 TReward: -237.17740320760862 RunMean : -2318.3430 MaxTarg : 0.5367\n",
      "Episode: 88 TReward: -259.7714001539557 RunMean : -2295.5080 MaxTarg : 0.5171\n",
      "Episode: 89 TReward: -199.0106029271697 RunMean : -2264.3466 MaxTarg : 0.4950\n",
      "Episode: 90 TReward: -199.5622601054786 RunMean : -2289.3334 MaxTarg : 0.4988\n",
      "Episode: 91 TReward: -257.08571133874364 RunMean : -2267.4948 MaxTarg : 0.4415\n",
      "Episode: 92 TReward: -283.46366165430135 RunMean : -2262.2908 MaxTarg : 0.4921\n",
      "Episode: 93 TReward: -246.71685227166685 RunMean : -2204.5201 MaxTarg : 0.5028\n",
      "Episode: 94 TReward: -244.69573526991135 RunMean : -2177.9342 MaxTarg : 0.5557\n",
      "Episode: 95 TReward: -288.05920138615545 RunMean : -2213.1105 MaxTarg : 0.5989\n",
      "Episode: 96 TReward: -279.67975259170953 RunMean : -2238.2454 MaxTarg : 0.5947\n",
      "Episode: 97 TReward: -308.46366471989614 RunMean : -2325.4283 MaxTarg : 0.6337\n",
      "Episode: 98 TReward: -263.4092422622813 RunMean : -2335.2549 MaxTarg : 0.5724\n",
      "Episode: 99 TReward: -280.1490622689894 RunMean : -2306.7835 MaxTarg : 0.6290\n",
      "Episode: 100 TReward: -225.77484054737286 RunMean : -2300.7453 MaxTarg : 0.5747\n",
      "Episode: 101 TReward: -194.54972901260015 RunMean : -2234.7251 MaxTarg : 0.4989\n",
      "Episode: 102 TReward: -325.1613270993656 RunMean : -2353.4440 MaxTarg : 0.6859\n",
      "Episode: 103 TReward: -255.55881206811233 RunMean : -2322.7670 MaxTarg : 0.6538\n",
      "Episode: 104 TReward: -284.2092788652967 RunMean : -2315.7756 MaxTarg : 0.5739\n",
      "Episode: 105 TReward: -265.95307642847325 RunMean : -2328.3513 MaxTarg : 0.6000\n",
      "Episode: 106 TReward: -202.74504510147264 RunMean : -2235.0980 MaxTarg : 0.6636\n",
      "Episode: 107 TReward: -269.72344877475217 RunMean : -2303.5635 MaxTarg : 0.6448\n",
      "Episode: 108 TReward: -242.56555308359896 RunMean : -2229.4587 MaxTarg : 0.5415\n",
      "Episode: 109 TReward: -310.912063071764 RunMean : -2270.8911 MaxTarg : 0.7471\n",
      "Episode: 110 TReward: -220.8563139254067 RunMean : -2203.5195 MaxTarg : 0.5631\n",
      "Episode: 111 TReward: -239.69728822656774 RunMean : -2172.9219 MaxTarg : 0.5724\n",
      "Episode: 112 TReward: -283.19277824234376 RunMean : -2290.1644 MaxTarg : 0.7751\n",
      "Episode: 113 TReward: -253.71529678281019 RunMean : -2309.4594 MaxTarg : 0.6482\n",
      "Episode: 114 TReward: -216.5673920715228 RunMean : -2234.8815 MaxTarg : 0.6745\n",
      "Episode: 115 TReward: -347.3235249670717 RunMean : -2288.8658 MaxTarg : 0.7685\n",
      "Episode: 116 TReward: -255.23553544935342 RunMean : -2284.5911 MaxTarg : 0.5933\n",
      "Episode: 117 TReward: -307.977955926711 RunMean : -2372.2351 MaxTarg : 0.7229\n",
      "Episode: 118 TReward: -263.48343233985696 RunMean : -2419.5572 MaxTarg : 0.6169\n",
      "Episode: 119 TReward: -282.0250270722687 RunMean : -2468.2841 MaxTarg : 0.8014\n",
      "Episode: 120 TReward: -288.10434253698537 RunMean : -2596.7085 MaxTarg : 0.7213\n",
      "Episode: 121 TReward: -283.6714563992696 RunMean : -2568.0386 MaxTarg : 0.6946\n",
      "Episode: 122 TReward: -310.3931577601517 RunMean : -2515.9240 MaxTarg : 0.9443\n",
      "Episode: 123 TReward: -218.70188840182328 RunMean : -2409.0403 MaxTarg : 0.6301\n",
      "Episode: 124 TReward: -303.7782250896564 RunMean : -2421.5829 MaxTarg : 0.7443\n",
      "Episode: 125 TReward: -250.11836586857484 RunMean : -2337.4482 MaxTarg : 0.7432\n",
      "Episode: 126 TReward: -329.94763078530923 RunMean : -2357.8452 MaxTarg : 0.8074\n",
      "Episode: 127 TReward: -201.5097795233395 RunMean : -2315.4308 MaxTarg : 0.9173\n",
      "Episode: 128 TReward: -293.5789121918009 RunMean : -2362.6976 MaxTarg : 0.8118\n",
      "Episode: 129 TReward: -270.44398274962657 RunMean : -2413.0607 MaxTarg : 0.9025\n",
      "Episode: 130 TReward: -335.98093344651556 RunMean : -2474.7521 MaxTarg : 0.8634\n",
      "Episode: 131 TReward: -311.636479771841 RunMean : -2536.9696 MaxTarg : 0.7779\n",
      "Episode: 132 TReward: -261.19244159261825 RunMean : -2501.8123 MaxTarg : 0.6843\n",
      "Episode: 133 TReward: -272.2250958074436 RunMean : -2453.9133 MaxTarg : 0.8692\n",
      "Episode: 134 TReward: -303.50438940583126 RunMean : -2536.2698 MaxTarg : 0.9059\n",
      "Episode: 135 TReward: -257.71249205095967 RunMean : -2494.6676 MaxTarg : 0.7721\n",
      "Episode: 136 TReward: -255.60269820888374 RunMean : -2432.3637 MaxTarg : 0.7987\n",
      "Episode: 137 TReward: -284.55643419983693 RunMean : -2392.6788 MaxTarg : 0.9026\n",
      "Episode: 138 TReward: -347.01753863879054 RunMean : -2487.4592 MaxTarg : 0.9015\n",
      "Episode: 139 TReward: -314.3798981113165 RunMean : -2603.6040 MaxTarg : 0.8278\n",
      "Episode: 140 TReward: -307.69791088779215 RunMean : -2636.2810 MaxTarg : 1.1113\n",
      "Episode: 141 TReward: -277.27746127237026 RunMean : -2635.1927 MaxTarg : 0.8931\n",
      "Episode: 142 TReward: -321.987128468657 RunMean : -2680.4818 MaxTarg : 0.9152\n",
      "Episode: 143 TReward: -295.0195455339642 RunMean : -2701.8589 MaxTarg : 0.9116\n",
      "Episode: 144 TReward: -244.6084479232936 RunMean : -2632.4651 MaxTarg : 0.9295\n",
      "Episode: 145 TReward: -279.79501117464133 RunMean : -2514.7337 MaxTarg : 1.2136\n",
      "Episode: 146 TReward: -319.38523922503697 RunMean : -2494.1716 MaxTarg : 1.0273\n",
      "Episode: 147 TReward: -285.73608170565655 RunMean : -2515.8864 MaxTarg : 0.9057\n",
      "Episode: 148 TReward: -308.6909741570474 RunMean : -2527.2697 MaxTarg : 0.8735\n",
      "Episode: 149 TReward: -213.20572215008775 RunMean : -2369.9252 MaxTarg : 0.8726\n",
      "Episode: 150 TReward: -325.0917870008372 RunMean : -2408.5084 MaxTarg : 1.0220\n",
      "Episode: 151 TReward: -375.41947546660134 RunMean : -2600.0606 MaxTarg : 1.3678\n",
      "Episode: 152 TReward: -357.58921917975744 RunMean : -2717.6780 MaxTarg : 1.2763\n",
      "Episode: 153 TReward: -364.50575518153784 RunMean : -2792.4771 MaxTarg : 1.0421\n",
      "Episode: 154 TReward: -282.45812151312396 RunMean : -2785.0161 MaxTarg : 0.9456\n",
      "Episode: 155 TReward: -363.0831596205923 RunMean : -2969.7954 MaxTarg : 1.1649\n",
      "Episode: 156 TReward: -330.52499452171634 RunMean : -3062.2510 MaxTarg : 1.1526\n",
      "Episode: 157 TReward: -375.8542861005208 RunMean : -3088.5740 MaxTarg : 1.0999\n",
      "Episode: 158 TReward: -322.8534860880996 RunMean : -2981.8113 MaxTarg : 1.2171\n",
      "Episode: 159 TReward: -308.88123603559796 RunMean : -2898.9618 MaxTarg : 1.0292\n",
      "Episode: 160 TReward: -247.60919132476704 RunMean : -2798.3435 MaxTarg : 1.1011\n",
      "Episode: 161 TReward: -356.37415462202966 RunMean : -2830.5888 MaxTarg : 1.5308\n",
      "Episode: 162 TReward: -335.5109471678633 RunMean : -2799.1974 MaxTarg : 1.3539\n",
      "Episode: 163 TReward: -314.563297185721 RunMean : -2747.3781 MaxTarg : 1.1990\n",
      "Episode: 164 TReward: -281.9151012951813 RunMean : -2668.1402 MaxTarg : 1.0985\n",
      "Episode: 165 TReward: -311.565727906076 RunMean : -2631.4426 MaxTarg : 1.8844\n",
      "Episode: 166 TReward: -320.0494208735073 RunMean : -2651.6524 MaxTarg : 1.5912\n",
      "Episode: 167 TReward: -297.37706499523733 RunMean : -2653.0615 MaxTarg : 1.8132\n",
      "Episode: 168 TReward: -290.81041448445245 RunMean : -2575.5263 MaxTarg : 1.1865\n",
      "Episode: 169 TReward: -375.57963769773033 RunMean : -2695.9967 MaxTarg : 1.7025\n",
      "Episode: 170 TReward: -378.8612551787823 RunMean : -2780.3879 MaxTarg : 1.4076\n",
      "Episode: 171 TReward: -349.84948545408815 RunMean : -2876.5374 MaxTarg : 1.4606\n",
      "Episode: 172 TReward: -326.02468172020787 RunMean : -2910.5876 MaxTarg : 2.2094\n",
      "Episode: 173 TReward: -335.8559341761048 RunMean : -2924.8427 MaxTarg : 1.5645\n",
      "Episode: 174 TReward: -435.95739043687263 RunMean : -3008.7340 MaxTarg : 4.4630\n",
      "Episode: 175 TReward: -347.3054849764746 RunMean : -2986.2513 MaxTarg : 1.3109\n",
      "Episode: 176 TReward: -297.31404691334177 RunMean : -2867.4823 MaxTarg : 1.3093\n",
      "Episode: 177 TReward: -338.67434674836335 RunMean : -2867.1665 MaxTarg : 1.4514\n",
      "Episode: 178 TReward: -292.9684912765723 RunMean : -2786.2675 MaxTarg : 1.1502\n",
      "Episode: 179 TReward: -401.31011189261255 RunMean : -2887.9870 MaxTarg : 1.9149\n",
      "Episode: 180 TReward: -390.9009166706064 RunMean : -3009.9293 MaxTarg : 3.2206\n",
      "Episode: 181 TReward: -402.73730888155615 RunMean : -3061.0455 MaxTarg : 1.8395\n",
      "Episode: 182 TReward: -360.38888992651 RunMean : -3146.4414 MaxTarg : 1.6208\n",
      "Episode: 183 TReward: -372.2017020869044 RunMean : -3267.3398 MaxTarg : 1.6617\n",
      "Episode: 184 TReward: -323.12842954524564 RunMean : -3277.9389 MaxTarg : 1.4995\n",
      "Episode: 185 TReward: -375.1178563347296 RunMean : -3357.2020 MaxTarg : 2.7173\n",
      "Episode: 186 TReward: -384.1132104815001 RunMean : -3308.2019 MaxTarg : 2.2490\n",
      "Episode: 187 TReward: -295.5702562181879 RunMean : -3194.8321 MaxTarg : 1.5213\n",
      "Episode: 188 TReward: -289.22568195284435 RunMean : -3078.8110 MaxTarg : 1.5861\n",
      "Episode: 189 TReward: -374.52199439046956 RunMean : -3044.8552 MaxTarg : 1.6586\n",
      "Episode: 190 TReward: -328.59903272640366 RunMean : -2972.7944 MaxTarg : 3.7638\n",
      "Episode: 191 TReward: -402.0959264001625 RunMean : -3023.1222 MaxTarg : 3.0896\n",
      "Episode: 192 TReward: -350.1683291481319 RunMean : -3008.0512 MaxTarg : 1.7099\n",
      "Episode: 193 TReward: -391.2232834219089 RunMean : -3086.7688 MaxTarg : 2.0091\n",
      "Episode: 194 TReward: -487.3324921229906 RunMean : -3293.5496 MaxTarg : 5.4230\n",
      "Episode: 195 TReward: -426.93155321699777 RunMean : -3454.6763 MaxTarg : 2.6281\n",
      "Episode: 196 TReward: -390.2831031243142 RunMean : -3583.4058 MaxTarg : 1.8608\n",
      "Episode: 197 TReward: -355.64756719757116 RunMean : -3542.9896 MaxTarg : 2.3719\n",
      "Episode: 198 TReward: -424.21271906172007 RunMean : -3533.9859 MaxTarg : 2.5888\n",
      "Episode: 199 TReward: -401.39301174279313 RunMean : -3598.1525 MaxTarg : 2.0158\n",
      "Episode: 200 TReward: -417.26882631687266 RunMean : -3620.5223 MaxTarg : 2.1197\n",
      "Episode: 201 TReward: -424.7965125803577 RunMean : -3595.4975 MaxTarg : 3.1030\n",
      "Episode: 202 TReward: -489.49803680246106 RunMean : -3675.6734 MaxTarg : 4.1362\n",
      "Episode: 203 TReward: -403.46893216454924 RunMean : -3694.3970 MaxTarg : 1.8190\n",
      "Episode: 204 TReward: -457.25753668815594 RunMean : -3842.9542 MaxTarg : 4.1065\n",
      "Episode: 205 TReward: -540.7602284863117 RunMean : -3894.3034 MaxTarg : 8.0836\n",
      "Episode: 206 TReward: -451.58911698091106 RunMean : -3908.0844 MaxTarg : 4.1278\n",
      "Episode: 207 TReward: -476.0813925587271 RunMean : -3845.2829 MaxTarg : 4.9659\n",
      "Episode: 208 TReward: -448.66819979372843 RunMean : -3892.3387 MaxTarg : 2.5399\n",
      "Episode: 209 TReward: -355.2912919436306 RunMean : -3789.2087 MaxTarg : 2.9018\n",
      "Episode: 210 TReward: -416.3624589140769 RunMean : -3741.8988 MaxTarg : 2.3517\n",
      "Episode: 211 TReward: -464.9891682719946 RunMean : -3776.4464 MaxTarg : 4.4453\n",
      "Episode: 212 TReward: -407.8515758613944 RunMean : -3717.3561 MaxTarg : 2.2567\n",
      "Episode: 213 TReward: -486.7848935601644 RunMean : -3919.5395 MaxTarg : 6.1016\n",
      "Episode: 214 TReward: -418.1119902489497 RunMean : -3723.4056 MaxTarg : 2.8805\n",
      "Episode: 215 TReward: -596.6812396175856 RunMean : -3984.3121 MaxTarg : 5.1791\n",
      "Episode: 216 TReward: -480.27658247116904 RunMean : -4116.7060 MaxTarg : 3.6036\n",
      "Episode: 217 TReward: -434.663352565957 RunMean : -4111.6686 MaxTarg : 2.5805\n",
      "Episode: 218 TReward: -524.5257732284662 RunMean : -4264.1289 MaxTarg : 6.9579\n",
      "Episode: 219 TReward: -512.1300917883511 RunMean : -4219.2600 MaxTarg : 4.7006\n",
      "Episode: 220 TReward: -563.3982476864352 RunMean : -4458.8751 MaxTarg : 7.3136\n",
      "Episode: 221 TReward: -476.87149003407325 RunMean : -4462.0162 MaxTarg : 3.0489\n",
      "Episode: 222 TReward: -389.0606632979656 RunMean : -4281.5023 MaxTarg : 2.2832\n",
      "Episode: 223 TReward: -586.2834986726116 RunMean : -4517.0730 MaxTarg : 13.0421\n",
      "Episode: 224 TReward: -523.7887241317225 RunMean : -4578.2812 MaxTarg : 3.5757\n",
      "Episode: 225 TReward: -397.8130020985078 RunMean : -4503.2397 MaxTarg : 3.9858\n",
      "Episode: 226 TReward: -571.1939570289132 RunMean : -4438.7181 MaxTarg : 11.4474\n",
      "Episode: 227 TReward: -468.9168230198217 RunMean : -4316.5038 MaxTarg : 3.0210\n",
      "Episode: 228 TReward: -669.8389184388709 RunMean : -4598.4782 MaxTarg : 10.2495\n",
      "Episode: 229 TReward: -621.8511815651964 RunMean : -4756.3811 MaxTarg : 12.3262\n",
      "Episode: 230 TReward: -530.4980437907021 RunMean : -4770.7439 MaxTarg : 7.0737\n",
      "Episode: 231 TReward: -504.07133059409387 RunMean : -4813.2906 MaxTarg : 4.4518\n",
      "Episode: 232 TReward: -588.063216382636 RunMean : -4965.4761 MaxTarg : 8.8236\n",
      "Episode: 233 TReward: -538.5550728922258 RunMean : -5047.4332 MaxTarg : 4.6338\n",
      "Episode: 234 TReward: -476.6152823398273 RunMean : -4949.4993 MaxTarg : 4.1135\n",
      "Episode: 235 TReward: -638.1113955844052 RunMean : -4997.5526 MaxTarg : 8.7819\n",
      "Episode: 236 TReward: -614.7348927453617 RunMean : -4964.7349 MaxTarg : 7.6770\n",
      "Episode: 237 TReward: -582.7415045349888 RunMean : -4926.6655 MaxTarg : 11.2641\n",
      "Episode: 238 TReward: -856.714805662189 RunMean : -5019.3042 MaxTarg : 15.2106\n",
      "Episode: 239 TReward: -525.1806752031583 RunMean : -4902.0372 MaxTarg : 7.4141\n",
      "Episode: 240 TReward: -518.9153372536505 RunMean : -4868.2615 MaxTarg : 6.3594\n",
      "Episode: 241 TReward: -496.27112657753963 RunMean : -4676.3541 MaxTarg : 3.6665\n",
      "Episode: 242 TReward: -508.2699498788875 RunMean : -4469.6658 MaxTarg : 5.3727\n",
      "Episode: 243 TReward: -544.9607302225238 RunMean : -4406.3871 MaxTarg : 3.9923\n",
      "Episode: 244 TReward: -540.157228534091 RunMean : -4467.0768 MaxTarg : 5.4186\n",
      "Episode: 245 TReward: -631.833011590632 RunMean : -4523.7648 MaxTarg : 13.9756\n",
      "Episode: 246 TReward: -568.6369454870063 RunMean : -4725.1475 MaxTarg : 4.7185\n",
      "Episode: 247 TReward: -573.9271210494034 RunMean : -4811.2794 MaxTarg : 7.2672\n",
      "Episode: 248 TReward: -621.5936156513275 RunMean : -5055.5979 MaxTarg : 13.9373\n",
      "Episode: 249 TReward: -732.3495537152182 RunMean : -5292.1160 MaxTarg : 14.5167\n",
      "Episode: 250 TReward: -708.2981180255258 RunMean : -5525.8665 MaxTarg : 9.7256\n",
      "Episode: 251 TReward: -634.6322039223251 RunMean : -5578.1080 MaxTarg : 8.0493\n",
      "Episode: 252 TReward: -621.9656282193395 RunMean : -5463.8388 MaxTarg : 9.5444\n",
      "Episode: 253 TReward: -621.5398518182845 RunMean : -5518.6888 MaxTarg : 8.6731\n",
      "Episode: 254 TReward: -613.9778069137003 RunMean : -5500.1063 MaxTarg : 7.4071\n",
      "Episode: 255 TReward: -553.4724071454563 RunMean : -5437.5537 MaxTarg : 5.7680\n",
      "Episode: 256 TReward: -884.0953762802494 RunMean : -5621.3873 MaxTarg : 17.1033\n",
      "Episode: 257 TReward: -675.596512956654 RunMean : -5671.8097 MaxTarg : 9.3325\n",
      "Episode: 258 TReward: -550.9302380453724 RunMean : -5647.8040 MaxTarg : 5.6285\n",
      "Episode: 259 TReward: -524.0053841794696 RunMean : -5574.2815 MaxTarg : 5.2288\n",
      "Episode: 260 TReward: -628.3933090449316 RunMean : -5574.6071 MaxTarg : 7.3680\n",
      "Episode: 261 TReward: -555.4790015001859 RunMean : -5586.0766 MaxTarg : 6.4891\n",
      "Episode: 262 TReward: -590.7855233265637 RunMean : -5400.8200 MaxTarg : 5.8484\n",
      "Episode: 263 TReward: -691.1343818336445 RunMean : -5238.5198 MaxTarg : 12.6742\n",
      "Episode: 264 TReward: -469.649482616634 RunMean : -5067.5269 MaxTarg : 5.0245\n",
      "Episode: 265 TReward: -616.4406385681392 RunMean : -5231.1818 MaxTarg : 8.1374\n",
      "Episode: 266 TReward: -560.3062429269451 RunMean : -5193.3170 MaxTarg : 6.5122\n",
      "Episode: 267 TReward: -649.9734287194515 RunMean : -5261.9287 MaxTarg : 11.9776\n",
      "Episode: 268 TReward: -627.940022252451 RunMean : -5404.4621 MaxTarg : 10.7020\n",
      "Episode: 269 TReward: -557.6931133882574 RunMean : -5260.5571 MaxTarg : 11.1235\n",
      "Episode: 270 TReward: -732.7484121429251 RunMean : -5464.7281 MaxTarg : 14.5915\n",
      "Episode: 271 TReward: -557.2914825752879 RunMean : -5463.9670 MaxTarg : 8.0405\n",
      "Episode: 272 TReward: -614.6916438799826 RunMean : -5576.7708 MaxTarg : 6.9821\n",
      "Episode: 273 TReward: -609.4440437844286 RunMean : -5516.1854 MaxTarg : 10.4750\n",
      "Episode: 274 TReward: -617.0444121305937 RunMean : -5467.3484 MaxTarg : 7.8408\n",
      "Episode: 275 TReward: -603.0479695754354 RunMean : -5539.9398 MaxTarg : 8.5567\n",
      "Episode: 276 TReward: -866.4137905986719 RunMean : -5563.6744 MaxTarg : 41.9979\n",
      "Episode: 277 TReward: -183.43307170665614 RunMean : -4884.8103 MaxTarg : nan\n",
      "Episode: 278 TReward: -185.22891529688096 RunMean : -4272.1974 MaxTarg : nan\n",
      "Episode: 279 TReward: -172.45566880915257 RunMean : -3678.7051 MaxTarg : nan\n",
      "Episode: 280 TReward: -179.31895090926122 RunMean : -3065.8299 MaxTarg : nan\n",
      "Episode: 281 TReward: -183.5629958836977 RunMean : -2469.9286 MaxTarg : nan\n",
      "Episode: 282 TReward: -171.3910262302145 RunMean : -1847.0826 MaxTarg : nan\n",
      "Episode: 283 TReward: -184.06551462159 RunMean : -1554.8368 MaxTarg : nan\n",
      "Episode: 284 TReward: -170.92974454660776 RunMean : -1552.5383 MaxTarg : nan\n",
      "Episode: 285 TReward: -168.3309249006866 RunMean : -1545.8726 MaxTarg : nan\n",
      "Episode: 286 TReward: -177.17585385827323 RunMean : -1564.7079 MaxTarg : nan\n",
      "Episode: 287 TReward: -169.90123726194471 RunMean : -1567.1298 MaxTarg : nan\n",
      "Episode: 288 TReward: -177.88386842841803 RunMean : -1562.0688 MaxTarg : nan\n",
      "Episode: 289 TReward: -179.1794250210246 RunMean : -1553.3973 MaxTarg : nan\n",
      "Episode: 290 TReward: -180.97162127056174 RunMean : -1558.8785 MaxTarg : nan\n",
      "Episode: 291 TReward: -176.6820354400877 RunMean : -1568.6969 MaxTarg : nan\n",
      "Episode: 292 TReward: -176.20740495583274 RunMean : -1564.6670 MaxTarg : nan\n",
      "Episode: 293 TReward: -181.07942718459597 RunMean : -1560.1653 MaxTarg : nan\n",
      "Episode: 294 TReward: -191.88072868282103 RunMean : -1581.4959 MaxTarg : nan\n",
      "Episode: 295 TReward: -185.4698795723834 RunMean : -1607.0808 MaxTarg : nan\n",
      "Episode: 296 TReward: -179.29028484833563 RunMean : -1608.7431 MaxTarg : nan\n",
      "Episode: 297 TReward: -181.42379680995091 RunMean : -1609.1014 MaxTarg : nan\n",
      "Episode: 298 TReward: -186.41990598837975 RunMean : -1619.2934 MaxTarg : nan\n",
      "Episode: 299 TReward: -185.55720212167606 RunMean : -1627.2118 MaxTarg : nan\n",
      "Episode: 300 TReward: -197.83993738832964 RunMean : -1627.1775 MaxTarg : nan\n",
      "Episode: 301 TReward: -179.38150864220165 RunMean : -1611.3854 MaxTarg : nan\n",
      "Episode: 302 TReward: -168.57435547283467 RunMean : -1588.5064 MaxTarg : nan\n",
      "Episode: 303 TReward: -167.32572244393663 RunMean : -1546.6004 MaxTarg : nan\n",
      "Episode: 304 TReward: -181.29278421933515 RunMean : -1535.2963 MaxTarg : nan\n",
      "Episode: 305 TReward: -152.06369758925896 RunMean : -1472.1226 MaxTarg : nan\n",
      "Episode: 306 TReward: -178.2272422820856 RunMean : -1471.8091 MaxTarg : nan\n",
      "Episode: 307 TReward: -165.70756450820556 RunMean : -1440.2381 MaxTarg : nan\n",
      "Episode: 308 TReward: -177.21033976284193 RunMean : -1452.5383 MaxTarg : nan\n",
      "Episode: 309 TReward: -162.46588269443802 RunMean : -1468.3611 MaxTarg : nan\n",
      "Episode: 310 TReward: -183.20069893501744 RunMean : -1487.4743 MaxTarg : nan\n",
      "Episode: 311 TReward: -175.93373786355525 RunMean : -1511.9423 MaxTarg : nan\n",
      "Episode: 312 TReward: -168.21244513003677 RunMean : -1514.0328 MaxTarg : nan\n",
      "Episode: 313 TReward: -178.8049318059459 RunMean : -1524.6136 MaxTarg : nan\n",
      "Episode: 314 TReward: -158.94079681156438 RunMean : -1492.3736 MaxTarg : nan\n",
      "Episode: 315 TReward: -181.83301182294127 RunMean : -1508.9045 MaxTarg : nan\n",
      "Episode: 316 TReward: -181.50155211123834 RunMean : -1499.8911 MaxTarg : nan\n",
      "Episode: 317 TReward: -183.34735331484646 RunMean : -1514.5691 MaxTarg : nan\n",
      "Episode: 318 TReward: -173.28579689278124 RunMean : -1518.9503 MaxTarg : nan\n",
      "Episode: 319 TReward: -176.82877079522308 RunMean : -1524.0929 MaxTarg : nan\n",
      "Episode: 320 TReward: -183.60053108123748 RunMean : -1562.8514 MaxTarg : nan\n",
      "Episode: 321 TReward: -162.91618091718792 RunMean : -1553.0931 MaxTarg : nan\n",
      "Episode: 322 TReward: -160.6355811328991 RunMean : -1536.3310 MaxTarg : nan\n",
      "Episode: 323 TReward: -189.8954192134984 RunMean : -1541.6737 MaxTarg : nan\n",
      "Episode: 324 TReward: -174.23005683823254 RunMean : -1544.9108 MaxTarg : nan\n",
      "Episode: 325 TReward: -182.1663770753519 RunMean : -1562.1386 MaxTarg : nan\n",
      "Episode: 326 TReward: -157.94765820521846 RunMean : -1530.5538 MaxTarg : nan\n",
      "Episode: 327 TReward: -172.38362507566765 RunMean : -1531.4513 MaxTarg : nan\n",
      "Episode: 328 TReward: -155.7835783276823 RunMean : -1513.5655 MaxTarg : nan\n",
      "Episode: 329 TReward: -163.05968422738283 RunMean : -1488.5666 MaxTarg : nan\n",
      "Episode: 330 TReward: -188.30924259415087 RunMean : -1504.8908 MaxTarg : nan\n",
      "Episode: 331 TReward: -171.59570252057944 RunMean : -1468.9003 MaxTarg : nan\n",
      "Episode: 332 TReward: -175.37595379987894 RunMean : -1456.9208 MaxTarg : nan\n",
      "Episode: 333 TReward: -188.900730784194 RunMean : -1494.2470 MaxTarg : nan\n",
      "Episode: 334 TReward: -166.16020695905414 RunMean : -1486.9042 MaxTarg : nan\n",
      "Episode: 335 TReward: -151.60153135087882 RunMean : -1501.6593 MaxTarg : nan\n",
      "Episode: 336 TReward: -178.16950562062743 RunMean : -1489.9254 MaxTarg : nan\n",
      "Episode: 337 TReward: -177.7054221553101 RunMean : -1496.0838 MaxTarg : nan\n",
      "Episode: 338 TReward: -181.80213124348535 RunMean : -1511.8291 MaxTarg : nan\n",
      "Episode: 339 TReward: -182.92801933746338 RunMean : -1514.6607 MaxTarg : nan\n",
      "Episode: 340 TReward: -182.83821391317457 RunMean : -1538.0714 MaxTarg : nan\n",
      "Episode: 341 TReward: -168.81568585445663 RunMean : -1536.5836 MaxTarg : nan\n",
      "Episode: 342 TReward: -185.8577955522366 RunMean : -1566.8572 MaxTarg : nan\n",
      "Episode: 343 TReward: -185.67548797415233 RunMean : -1578.2766 MaxTarg : nan\n",
      "Episode: 344 TReward: -174.70017637515522 RunMean : -1582.3955 MaxTarg : nan\n"
     ]
    }
   ],
   "source": [
    "from gym import wrappers\n",
    "log_path = './logs/3/logs_run_drop=1.0_-=0.1'  \n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "#env = wrappers.Monitor(env, '/tmp/lunarlander-experiment-2',force=True)\n",
    "\n",
    "train, mainQN, saver, num_episodes, runningMean = test_and_train_qnetwork(memory_size=100000,\\\n",
    "                                     train_episodes=10000,\\\n",
    "                                           gamma=0.98,\\\n",
    "                                           explore_start=1.,\\\n",
    "                                           explore_stop=0.1,\\\n",
    "                                           decay_rate=0.0001,\\\n",
    "                                           hidden_layers=1,\\\n",
    "                                           hidden_size=256,\\\n",
    "                                           learning_rate=0.0001,\\\n",
    "                                           batch_size=128,\\\n",
    "                                           alpha=0.1,\\\n",
    "                                           num_trains = 128,\\\n",
    "                                           verbose=True)\n",
    "print('train=',str(train))\n",
    "print('number of episodes=',str(num_episodes))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()\n",
    "gym.upload('/tmp/cartpole-experiment-1', api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
