{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:11:34,155] Making new env: PredictObsCartpole-v0\n",
      "[2017-05-27 19:11:34,216] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-27 19:11:35,442] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n",
      "[2017-05-27 19:11:35,444] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Total reward: 15.0 Training loss: 1.0141 Prediction loss: 0.2869 Explore P: 0.9985 RunMean : 15.0000\n",
      "Episode: 1 Total reward: 12.0 Training loss: 1.0383 Prediction loss: 0.2254 Explore P: 0.9973 RunMean : 13.5000\n",
      "Episode: 2 Total reward: 32.0 Training loss: 1.0210 Prediction loss: 0.0747 Explore P: 0.9942 RunMean : 19.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:11:39,288] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3 Total reward: 70.0 Training loss: 1.1110 Prediction loss: 0.0663 Explore P: 0.9873 RunMean : 32.2500\n",
      "Episode: 4 Total reward: 31.0 Training loss: 1.2132 Prediction loss: 0.0316 Explore P: 0.9843 RunMean : 32.0000\n",
      "Episode: 5 Total reward: 20.0 Training loss: 1.1656 Prediction loss: 0.0243 Explore P: 0.9823 RunMean : 30.0000\n",
      "Episode: 6 Total reward: 45.0 Training loss: 1.2356 Prediction loss: 0.0303 Explore P: 0.9780 RunMean : 32.1429\n",
      "Episode: 7 Total reward: 16.0 Training loss: 1.2645 Prediction loss: 0.0123 Explore P: 0.9764 RunMean : 30.1250\n",
      "Episode: 8 Total reward: 18.0 Training loss: 1.3826 Prediction loss: 0.0117 Explore P: 0.9747 RunMean : 28.7778\n",
      "Episode: 9 Total reward: 17.0 Training loss: 1.5148 Prediction loss: 0.0230 Explore P: 0.9730 RunMean : 27.6000\n",
      "Episode: 10 Total reward: 15.0 Training loss: 1.8201 Prediction loss: 0.0374 Explore P: 0.9716 RunMean : 26.4545\n",
      "Episode: 11 Total reward: 10.0 Training loss: 1.7962 Prediction loss: 0.0244 Explore P: 0.9706 RunMean : 25.0833\n",
      "Episode: 12 Total reward: 26.0 Training loss: 1.8186 Prediction loss: 0.0125 Explore P: 0.9682 RunMean : 25.1538\n",
      "Episode: 13 Total reward: 43.0 Training loss: 1.7926 Prediction loss: 0.0048 Explore P: 0.9640 RunMean : 26.4286\n",
      "Episode: 14 Total reward: 12.0 Training loss: 1.7739 Prediction loss: 0.0055 Explore P: 0.9629 RunMean : 25.4667\n",
      "Episode: 15 Total reward: 16.0 Training loss: 1.8705 Prediction loss: 0.0036 Explore P: 0.9614 RunMean : 24.8750\n",
      "Episode: 16 Total reward: 20.0 Training loss: 2.7324 Prediction loss: 0.0070 Explore P: 0.9595 RunMean : 24.5882\n",
      "Episode: 17 Total reward: 32.0 Training loss: 4.7725 Prediction loss: 0.0084 Explore P: 0.9564 RunMean : 25.0000\n",
      "Episode: 18 Total reward: 34.0 Training loss: 2.2393 Prediction loss: 0.0029 Explore P: 0.9532 RunMean : 25.4737\n",
      "Episode: 19 Total reward: 18.0 Training loss: 2.2560 Prediction loss: 0.0022 Explore P: 0.9515 RunMean : 25.1000\n",
      "Episode: 20 Total reward: 12.0 Training loss: 6.6578 Prediction loss: 0.0186 Explore P: 0.9504 RunMean : 24.4762\n",
      "Episode: 21 Total reward: 10.0 Training loss: 7.9424 Prediction loss: 0.0124 Explore P: 0.9495 RunMean : 23.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:11:43,582] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 22 Total reward: 10.0 Training loss: 9.9580 Prediction loss: 0.0208 Explore P: 0.9485 RunMean : 23.2174\n",
      "Episode: 23 Total reward: 18.0 Training loss: 10.5869 Prediction loss: 0.0161 Explore P: 0.9468 RunMean : 23.0000\n",
      "Episode: 24 Total reward: 21.0 Training loss: 12.4748 Prediction loss: 0.0066 Explore P: 0.9449 RunMean : 22.9200\n",
      "Episode: 25 Total reward: 15.0 Training loss: 14.2603 Prediction loss: 0.0146 Explore P: 0.9435 RunMean : 22.6154\n",
      "Episode: 26 Total reward: 28.0 Training loss: 28.6287 Prediction loss: 0.0362 Explore P: 0.9409 RunMean : 22.8148\n",
      "Episode: 27 Total reward: 59.0 Training loss: 2.8663 Prediction loss: 0.0022 Explore P: 0.9354 RunMean : 24.1071\n",
      "Episode: 28 Total reward: 30.0 Training loss: 30.0557 Prediction loss: 0.0202 Explore P: 0.9326 RunMean : 24.3103\n",
      "Episode: 29 Total reward: 20.0 Training loss: 2.3925 Prediction loss: 0.0022 Explore P: 0.9308 RunMean : 24.1667\n",
      "Episode: 30 Total reward: 24.0 Training loss: 19.5039 Prediction loss: 0.0123 Explore P: 0.9286 RunMean : 24.1613\n",
      "Episode: 31 Total reward: 14.0 Training loss: 40.8929 Prediction loss: 0.0278 Explore P: 0.9273 RunMean : 23.8438\n",
      "Episode: 32 Total reward: 29.0 Training loss: 47.2134 Prediction loss: 0.0179 Explore P: 0.9246 RunMean : 24.0000\n",
      "Episode: 33 Total reward: 14.0 Training loss: 30.7703 Prediction loss: 0.0108 Explore P: 0.9233 RunMean : 23.7059\n",
      "Episode: 34 Total reward: 15.0 Training loss: 70.1828 Prediction loss: 0.0366 Explore P: 0.9220 RunMean : 23.4571\n",
      "Episode: 35 Total reward: 19.0 Training loss: 2.8880 Prediction loss: 0.0114 Explore P: 0.9202 RunMean : 23.3333\n",
      "Episode: 36 Total reward: 44.0 Training loss: 33.8052 Prediction loss: 0.0239 Explore P: 0.9162 RunMean : 23.8919\n",
      "Episode: 37 Total reward: 29.0 Training loss: 38.7327 Prediction loss: 0.0109 Explore P: 0.9136 RunMean : 24.0263\n",
      "Episode: 38 Total reward: 10.0 Training loss: 63.6427 Prediction loss: 0.0150 Explore P: 0.9127 RunMean : 23.6667\n",
      "Episode: 39 Total reward: 10.0 Training loss: 30.5107 Prediction loss: 0.0105 Explore P: 0.9118 RunMean : 23.3250\n",
      "Episode: 40 Total reward: 17.0 Training loss: 74.5161 Prediction loss: 0.0329 Explore P: 0.9103 RunMean : 23.1707\n",
      "Episode: 41 Total reward: 21.0 Training loss: 31.7906 Prediction loss: 0.0087 Explore P: 0.9084 RunMean : 23.1190\n",
      "Episode: 42 Total reward: 14.0 Training loss: 32.1653 Prediction loss: 0.0061 Explore P: 0.9071 RunMean : 22.9070\n",
      "Episode: 43 Total reward: 11.0 Training loss: 1.7705 Prediction loss: 0.0032 Explore P: 0.9061 RunMean : 22.6364\n",
      "Episode: 44 Total reward: 37.0 Training loss: 8.2664 Prediction loss: 0.0086 Explore P: 0.9028 RunMean : 22.9556\n",
      "Episode: 45 Total reward: 10.0 Training loss: 24.2115 Prediction loss: 0.0097 Explore P: 0.9019 RunMean : 22.6739\n",
      "Episode: 46 Total reward: 13.0 Training loss: 21.2553 Prediction loss: 0.0150 Explore P: 0.9008 RunMean : 22.4681\n",
      "Episode: 47 Total reward: 13.0 Training loss: 43.5622 Prediction loss: 0.0154 Explore P: 0.8996 RunMean : 22.2708\n",
      "Episode: 48 Total reward: 15.0 Training loss: 6.8227 Prediction loss: 0.0108 Explore P: 0.8983 RunMean : 22.1224\n",
      "Episode: 49 Total reward: 20.0 Training loss: 49.4444 Prediction loss: 0.0215 Explore P: 0.8965 RunMean : 22.0800\n",
      "Episode: 50 Total reward: 20.0 Training loss: 7.2163 Prediction loss: 0.0090 Explore P: 0.8947 RunMean : 22.0392\n",
      "Episode: 51 Total reward: 15.0 Training loss: 1.1454 Prediction loss: 0.0044 Explore P: 0.8934 RunMean : 21.9038\n",
      "Episode: 52 Total reward: 14.0 Training loss: 31.2567 Prediction loss: 0.0045 Explore P: 0.8922 RunMean : 21.7547\n",
      "Episode: 53 Total reward: 19.0 Training loss: 37.0903 Prediction loss: 0.0065 Explore P: 0.8905 RunMean : 21.7037\n",
      "Episode: 54 Total reward: 27.0 Training loss: 54.7162 Prediction loss: 0.0220 Explore P: 0.8881 RunMean : 21.8000\n",
      "Episode: 55 Total reward: 10.0 Training loss: 1.6302 Prediction loss: 0.0040 Explore P: 0.8873 RunMean : 21.5893\n",
      "Episode: 56 Total reward: 20.0 Training loss: 7.4204 Prediction loss: 0.0070 Explore P: 0.8855 RunMean : 21.5614\n",
      "Episode: 57 Total reward: 23.0 Training loss: 44.4972 Prediction loss: 0.0140 Explore P: 0.8835 RunMean : 21.5862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:11:52,827] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 58 Total reward: 19.0 Training loss: 39.2415 Prediction loss: 0.0141 Explore P: 0.8818 RunMean : 21.5424\n",
      "Episode: 59 Total reward: 10.0 Training loss: 44.0004 Prediction loss: 0.0111 Explore P: 0.8810 RunMean : 21.3500\n",
      "Episode: 60 Total reward: 26.0 Training loss: 2.7461 Prediction loss: 0.0021 Explore P: 0.8787 RunMean : 21.4262\n",
      "Episode: 61 Total reward: 15.0 Training loss: 7.4881 Prediction loss: 0.0069 Explore P: 0.8774 RunMean : 21.3226\n",
      "Episode: 62 Total reward: 20.0 Training loss: 2.2121 Prediction loss: 0.0017 Explore P: 0.8757 RunMean : 21.3016\n",
      "Episode: 63 Total reward: 29.0 Training loss: 1.6518 Prediction loss: 0.0010 Explore P: 0.8732 RunMean : 21.4219\n",
      "Episode: 64 Total reward: 11.0 Training loss: 2.0413 Prediction loss: 0.0011 Explore P: 0.8722 RunMean : 21.2615\n",
      "Episode: 65 Total reward: 16.0 Training loss: 7.2922 Prediction loss: 0.0115 Explore P: 0.8708 RunMean : 21.1818\n",
      "Episode: 66 Total reward: 11.0 Training loss: 59.0227 Prediction loss: 0.0231 Explore P: 0.8699 RunMean : 21.0299\n",
      "Episode: 67 Total reward: 12.0 Training loss: 7.8533 Prediction loss: 0.0049 Explore P: 0.8689 RunMean : 20.8971\n",
      "Episode: 68 Total reward: 13.0 Training loss: 7.6662 Prediction loss: 0.0043 Explore P: 0.8677 RunMean : 20.7826\n",
      "Episode: 69 Total reward: 14.0 Training loss: 2.5990 Prediction loss: 0.0015 Explore P: 0.8665 RunMean : 20.6857\n",
      "Episode: 70 Total reward: 32.0 Training loss: 97.2661 Prediction loss: 0.0117 Explore P: 0.8638 RunMean : 20.8451\n",
      "Episode: 71 Total reward: 15.0 Training loss: 63.0995 Prediction loss: 0.0166 Explore P: 0.8625 RunMean : 20.7639\n",
      "Episode: 72 Total reward: 17.0 Training loss: 2.6282 Prediction loss: 0.0050 Explore P: 0.8611 RunMean : 20.7123\n",
      "Episode: 73 Total reward: 24.0 Training loss: 2.2970 Prediction loss: 0.0051 Explore P: 0.8590 RunMean : 20.7568\n",
      "Episode: 74 Total reward: 11.0 Training loss: 137.7548 Prediction loss: 0.0197 Explore P: 0.8581 RunMean : 20.6267\n",
      "Episode: 75 Total reward: 17.0 Training loss: 6.8631 Prediction loss: 0.0092 Explore P: 0.8567 RunMean : 20.5789\n",
      "Episode: 76 Total reward: 26.0 Training loss: 3.0889 Prediction loss: 0.0030 Explore P: 0.8545 RunMean : 20.6494\n",
      "Episode: 77 Total reward: 14.0 Training loss: 75.7192 Prediction loss: 0.0098 Explore P: 0.8533 RunMean : 20.5641\n",
      "Episode: 78 Total reward: 30.0 Training loss: 2.9797 Prediction loss: 0.0040 Explore P: 0.8508 RunMean : 20.6835\n",
      "Episode: 79 Total reward: 14.0 Training loss: 3.0720 Prediction loss: 0.0021 Explore P: 0.8496 RunMean : 20.6000\n",
      "Episode: 80 Total reward: 12.0 Training loss: 16.5994 Prediction loss: 0.0221 Explore P: 0.8486 RunMean : 20.4938\n",
      "Episode: 81 Total reward: 14.0 Training loss: 44.9873 Prediction loss: 0.0086 Explore P: 0.8474 RunMean : 20.4146\n",
      "Episode: 82 Total reward: 31.0 Training loss: 2.4218 Prediction loss: 0.0016 Explore P: 0.8448 RunMean : 20.5422\n",
      "Episode: 83 Total reward: 18.0 Training loss: 49.6929 Prediction loss: 0.0100 Explore P: 0.8433 RunMean : 20.5119\n",
      "Episode: 84 Total reward: 17.0 Training loss: 6.0361 Prediction loss: 0.0083 Explore P: 0.8419 RunMean : 20.4706\n",
      "Episode: 85 Total reward: 21.0 Training loss: 3.3290 Prediction loss: 0.0047 Explore P: 0.8401 RunMean : 20.4767\n",
      "Episode: 86 Total reward: 18.0 Training loss: 2.6949 Prediction loss: 0.0034 Explore P: 0.8387 RunMean : 20.4483\n",
      "Episode: 87 Total reward: 42.0 Training loss: 170.2748 Prediction loss: 0.0356 Explore P: 0.8352 RunMean : 20.6932\n",
      "Episode: 88 Total reward: 18.0 Training loss: 34.8100 Prediction loss: 0.0061 Explore P: 0.8337 RunMean : 20.6629\n",
      "Episode: 89 Total reward: 14.0 Training loss: 83.6151 Prediction loss: 0.0138 Explore P: 0.8325 RunMean : 20.5889\n",
      "Episode: 90 Total reward: 11.0 Training loss: 35.1476 Prediction loss: 0.0093 Explore P: 0.8316 RunMean : 20.4835\n",
      "Episode: 91 Total reward: 12.0 Training loss: 53.2286 Prediction loss: 0.0364 Explore P: 0.8307 RunMean : 20.3913\n",
      "Episode: 92 Total reward: 12.0 Training loss: 39.2503 Prediction loss: 0.0047 Explore P: 0.8297 RunMean : 20.3011\n",
      "Episode: 93 Total reward: 37.0 Training loss: 6.7075 Prediction loss: 0.0083 Explore P: 0.8266 RunMean : 20.4787\n",
      "Episode: 94 Total reward: 10.0 Training loss: 2.8707 Prediction loss: 0.0022 Explore P: 0.8258 RunMean : 20.3684\n",
      "Episode: 95 Total reward: 24.0 Training loss: 8.3678 Prediction loss: 0.0072 Explore P: 0.8239 RunMean : 20.4062\n",
      "Episode: 96 Total reward: 10.0 Training loss: 42.3114 Prediction loss: 0.0125 Explore P: 0.8231 RunMean : 20.2990\n",
      "Episode: 97 Total reward: 11.0 Training loss: 136.1627 Prediction loss: 0.0150 Explore P: 0.8222 RunMean : 20.2041\n",
      "Episode: 98 Total reward: 15.0 Training loss: 31.0499 Prediction loss: 0.0048 Explore P: 0.8209 RunMean : 20.1515\n",
      "Episode: 99 Total reward: 25.0 Training loss: 39.5219 Prediction loss: 0.0083 Explore P: 0.8189 RunMean : 20.2000\n",
      "Episode: 100 Total reward: 20.0 Training loss: 11.5289 Prediction loss: 0.0098 Explore P: 0.8173 RunMean : 20.2500\n",
      "Episode: 101 Total reward: 18.0 Training loss: 70.8216 Prediction loss: 0.0191 Explore P: 0.8159 RunMean : 20.3100\n",
      "Episode: 102 Total reward: 33.0 Training loss: 31.6140 Prediction loss: 0.0070 Explore P: 0.8132 RunMean : 20.3200\n",
      "Episode: 103 Total reward: 22.0 Training loss: 2.0239 Prediction loss: 0.0097 Explore P: 0.8114 RunMean : 19.8400\n",
      "Episode: 104 Total reward: 15.0 Training loss: 34.5032 Prediction loss: 0.0114 Explore P: 0.8102 RunMean : 19.6800\n",
      "Episode: 105 Total reward: 13.0 Training loss: 65.6218 Prediction loss: 0.0255 Explore P: 0.8092 RunMean : 19.6100\n",
      "Episode: 106 Total reward: 13.0 Training loss: 4.7987 Prediction loss: 0.0102 Explore P: 0.8082 RunMean : 19.2900\n",
      "Episode: 107 Total reward: 12.0 Training loss: 14.1986 Prediction loss: 0.0261 Explore P: 0.8072 RunMean : 19.2500\n",
      "Episode: 108 Total reward: 40.0 Training loss: 61.3334 Prediction loss: 0.0217 Explore P: 0.8040 RunMean : 19.4700\n",
      "Episode: 109 Total reward: 15.0 Training loss: 58.0720 Prediction loss: 0.0113 Explore P: 0.8028 RunMean : 19.4500\n",
      "Episode: 110 Total reward: 17.0 Training loss: 31.4207 Prediction loss: 0.0094 Explore P: 0.8015 RunMean : 19.4700\n",
      "Episode: 111 Total reward: 19.0 Training loss: 5.5695 Prediction loss: 0.0039 Explore P: 0.8000 RunMean : 19.5600\n",
      "Episode: 112 Total reward: 16.0 Training loss: 43.0946 Prediction loss: 0.0261 Explore P: 0.7987 RunMean : 19.4600\n",
      "Episode: 113 Total reward: 23.0 Training loss: 35.0491 Prediction loss: 0.0138 Explore P: 0.7969 RunMean : 19.2600\n",
      "Episode: 114 Total reward: 33.0 Training loss: 1.5936 Prediction loss: 0.0045 Explore P: 0.7943 RunMean : 19.4700\n",
      "Episode: 115 Total reward: 9.0 Training loss: 31.3121 Prediction loss: 0.0074 Explore P: 0.7936 RunMean : 19.4000\n",
      "Episode: 116 Total reward: 18.0 Training loss: 7.7112 Prediction loss: 0.0060 Explore P: 0.7922 RunMean : 19.3800\n",
      "Episode: 117 Total reward: 27.0 Training loss: 8.9853 Prediction loss: 0.0111 Explore P: 0.7901 RunMean : 19.3300\n",
      "Episode: 118 Total reward: 10.0 Training loss: 29.1510 Prediction loss: 0.0063 Explore P: 0.7893 RunMean : 19.0900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:12:08,935] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 119 Total reward: 36.0 Training loss: 53.7682 Prediction loss: 0.0124 Explore P: 0.7865 RunMean : 19.2700\n",
      "Episode: 120 Total reward: 12.0 Training loss: 55.6469 Prediction loss: 0.0120 Explore P: 0.7856 RunMean : 19.2700\n",
      "Episode: 121 Total reward: 16.0 Training loss: 4.0873 Prediction loss: 0.0134 Explore P: 0.7843 RunMean : 19.3300\n",
      "Episode: 122 Total reward: 15.0 Training loss: 54.3138 Prediction loss: 0.0123 Explore P: 0.7832 RunMean : 19.3800\n",
      "Episode: 123 Total reward: 21.0 Training loss: 31.1438 Prediction loss: 0.0115 Explore P: 0.7816 RunMean : 19.4100\n",
      "Episode: 124 Total reward: 18.0 Training loss: 51.4868 Prediction loss: 0.0092 Explore P: 0.7802 RunMean : 19.3800\n",
      "Episode: 125 Total reward: 33.0 Training loss: 73.2468 Prediction loss: 0.0156 Explore P: 0.7776 RunMean : 19.5600\n",
      "Episode: 126 Total reward: 29.0 Training loss: 28.0366 Prediction loss: 0.0141 Explore P: 0.7754 RunMean : 19.5700\n",
      "Episode: 127 Total reward: 15.0 Training loss: 4.7075 Prediction loss: 0.0109 Explore P: 0.7743 RunMean : 19.1300\n",
      "Episode: 128 Total reward: 16.0 Training loss: 1.6145 Prediction loss: 0.0043 Explore P: 0.7730 RunMean : 18.9900\n",
      "Episode: 129 Total reward: 13.0 Training loss: 49.2251 Prediction loss: 0.0151 Explore P: 0.7720 RunMean : 18.9200\n",
      "Episode: 130 Total reward: 18.0 Training loss: 1.5323 Prediction loss: 0.0041 Explore P: 0.7707 RunMean : 18.8600\n",
      "Episode: 131 Total reward: 15.0 Training loss: 1.2077 Prediction loss: 0.0059 Explore P: 0.7695 RunMean : 18.8700\n",
      "Episode: 132 Total reward: 15.0 Training loss: 5.6727 Prediction loss: 0.0050 Explore P: 0.7684 RunMean : 18.7300\n",
      "Episode: 133 Total reward: 15.0 Training loss: 5.2926 Prediction loss: 0.0080 Explore P: 0.7673 RunMean : 18.7400\n",
      "Episode: 134 Total reward: 16.0 Training loss: 28.7474 Prediction loss: 0.0112 Explore P: 0.7660 RunMean : 18.7500\n",
      "Episode: 135 Total reward: 27.0 Training loss: 35.9124 Prediction loss: 0.0149 Explore P: 0.7640 RunMean : 18.8300\n",
      "Episode: 136 Total reward: 18.0 Training loss: 8.5075 Prediction loss: 0.0058 Explore P: 0.7627 RunMean : 18.5700\n",
      "Episode: 137 Total reward: 20.0 Training loss: 28.2828 Prediction loss: 0.0083 Explore P: 0.7611 RunMean : 18.4800\n",
      "Episode: 138 Total reward: 11.0 Training loss: 43.8535 Prediction loss: 0.0134 Explore P: 0.7603 RunMean : 18.4900\n",
      "Episode: 139 Total reward: 28.0 Training loss: 21.8657 Prediction loss: 0.0057 Explore P: 0.7582 RunMean : 18.6700\n",
      "Episode: 140 Total reward: 29.0 Training loss: 1.1117 Prediction loss: 0.0068 Explore P: 0.7561 RunMean : 18.7900\n",
      "Episode: 141 Total reward: 38.0 Training loss: 0.9278 Prediction loss: 0.0066 Explore P: 0.7532 RunMean : 18.9600\n",
      "Episode: 142 Total reward: 13.0 Training loss: 19.5143 Prediction loss: 0.0059 Explore P: 0.7523 RunMean : 18.9500\n",
      "Episode: 143 Total reward: 18.0 Training loss: 21.9857 Prediction loss: 0.0131 Explore P: 0.7509 RunMean : 19.0200\n",
      "Episode: 144 Total reward: 14.0 Training loss: 41.0313 Prediction loss: 0.0139 Explore P: 0.7499 RunMean : 18.7900\n",
      "Episode: 145 Total reward: 13.0 Training loss: 35.1217 Prediction loss: 0.0075 Explore P: 0.7489 RunMean : 18.8200\n",
      "Episode: 146 Total reward: 17.0 Training loss: 42.7980 Prediction loss: 0.0161 Explore P: 0.7477 RunMean : 18.8600\n",
      "Episode: 147 Total reward: 15.0 Training loss: 16.8078 Prediction loss: 0.0077 Explore P: 0.7466 RunMean : 18.8800\n",
      "Episode: 148 Total reward: 18.0 Training loss: 32.5194 Prediction loss: 0.0101 Explore P: 0.7452 RunMean : 18.9100\n",
      "Episode: 149 Total reward: 16.0 Training loss: 40.5716 Prediction loss: 0.0120 Explore P: 0.7441 RunMean : 18.8700\n",
      "Episode: 150 Total reward: 9.0 Training loss: 20.6980 Prediction loss: 0.0104 Explore P: 0.7434 RunMean : 18.7600\n",
      "Episode: 151 Total reward: 15.0 Training loss: 15.8292 Prediction loss: 0.0071 Explore P: 0.7423 RunMean : 18.7600\n",
      "Episode: 152 Total reward: 39.0 Training loss: 0.8867 Prediction loss: 0.0005 Explore P: 0.7395 RunMean : 19.0100\n",
      "Episode: 153 Total reward: 24.0 Training loss: 48.5081 Prediction loss: 0.0159 Explore P: 0.7377 RunMean : 19.0600\n",
      "Episode: 154 Total reward: 10.0 Training loss: 0.9456 Prediction loss: 0.0030 Explore P: 0.7370 RunMean : 18.8900\n",
      "Episode: 155 Total reward: 11.0 Training loss: 0.9257 Prediction loss: 0.0064 Explore P: 0.7362 RunMean : 18.9000\n",
      "Episode: 156 Total reward: 18.0 Training loss: 20.1175 Prediction loss: 0.0099 Explore P: 0.7349 RunMean : 18.8800\n",
      "Episode: 157 Total reward: 11.0 Training loss: 6.6909 Prediction loss: 0.0157 Explore P: 0.7341 RunMean : 18.7600\n",
      "Episode: 158 Total reward: 38.0 Training loss: 0.7352 Prediction loss: 0.0010 Explore P: 0.7313 RunMean : 18.9500\n",
      "Episode: 159 Total reward: 25.0 Training loss: 13.3286 Prediction loss: 0.0069 Explore P: 0.7295 RunMean : 19.1000\n",
      "Episode: 160 Total reward: 70.0 Training loss: 0.5867 Prediction loss: 0.0010 Explore P: 0.7245 RunMean : 19.5400\n",
      "Episode: 161 Total reward: 37.0 Training loss: 11.7004 Prediction loss: 0.0045 Explore P: 0.7219 RunMean : 19.7600\n",
      "Episode: 162 Total reward: 21.0 Training loss: 12.2057 Prediction loss: 0.0079 Explore P: 0.7204 RunMean : 19.7700\n",
      "Episode: 163 Total reward: 33.0 Training loss: 30.2759 Prediction loss: 0.0112 Explore P: 0.7180 RunMean : 19.8100\n",
      "Episode: 164 Total reward: 26.0 Training loss: 18.2679 Prediction loss: 0.0065 Explore P: 0.7162 RunMean : 19.9600\n",
      "Episode: 165 Total reward: 20.0 Training loss: 4.2096 Prediction loss: 0.0064 Explore P: 0.7148 RunMean : 20.0000\n",
      "Episode: 166 Total reward: 53.0 Training loss: 17.6697 Prediction loss: 0.0099 Explore P: 0.7111 RunMean : 20.4200\n",
      "Episode: 167 Total reward: 41.0 Training loss: 28.8427 Prediction loss: 0.0103 Explore P: 0.7082 RunMean : 20.7100\n",
      "Episode: 168 Total reward: 19.0 Training loss: 15.5928 Prediction loss: 0.0091 Explore P: 0.7069 RunMean : 20.7700\n",
      "Episode: 169 Total reward: 22.0 Training loss: 4.5207 Prediction loss: 0.0047 Explore P: 0.7053 RunMean : 20.8500\n",
      "Episode: 170 Total reward: 38.0 Training loss: 20.3295 Prediction loss: 0.0067 Explore P: 0.7027 RunMean : 20.9100\n",
      "Episode: 171 Total reward: 18.0 Training loss: 3.8562 Prediction loss: 0.0087 Explore P: 0.7015 RunMean : 20.9400\n",
      "Episode: 172 Total reward: 15.0 Training loss: 14.4648 Prediction loss: 0.0113 Explore P: 0.7004 RunMean : 20.9200\n",
      "Episode: 173 Total reward: 31.0 Training loss: 17.6456 Prediction loss: 0.0103 Explore P: 0.6983 RunMean : 20.9900\n",
      "Episode: 174 Total reward: 21.0 Training loss: 26.7209 Prediction loss: 0.0142 Explore P: 0.6968 RunMean : 21.0900\n",
      "Episode: 175 Total reward: 47.0 Training loss: 10.9587 Prediction loss: 0.0055 Explore P: 0.6936 RunMean : 21.3900\n",
      "Episode: 176 Total reward: 31.0 Training loss: 25.9980 Prediction loss: 0.0132 Explore P: 0.6915 RunMean : 21.4400\n",
      "Episode: 177 Total reward: 66.0 Training loss: 5.0634 Prediction loss: 0.0071 Explore P: 0.6870 RunMean : 21.9600\n",
      "Episode: 178 Total reward: 46.0 Training loss: 19.4595 Prediction loss: 0.0101 Explore P: 0.6839 RunMean : 22.1200\n",
      "Episode: 179 Total reward: 81.0 Training loss: 18.1420 Prediction loss: 0.0164 Explore P: 0.6785 RunMean : 22.7900\n",
      "Episode: 180 Total reward: 47.0 Training loss: 6.8129 Prediction loss: 0.0049 Explore P: 0.6753 RunMean : 23.1400\n",
      "Episode: 181 Total reward: 67.0 Training loss: 20.1683 Prediction loss: 0.0081 Explore P: 0.6709 RunMean : 23.6700\n",
      "Episode: 182 Total reward: 9.0 Training loss: 9.2988 Prediction loss: 0.0055 Explore P: 0.6703 RunMean : 23.4500\n",
      "Episode: 183 Total reward: 12.0 Training loss: 8.4409 Prediction loss: 0.0065 Explore P: 0.6695 RunMean : 23.3900\n",
      "Episode: 184 Total reward: 50.0 Training loss: 20.8569 Prediction loss: 0.0091 Explore P: 0.6662 RunMean : 23.7200\n",
      "Episode: 185 Total reward: 51.0 Training loss: 4.6790 Prediction loss: 0.0048 Explore P: 0.6629 RunMean : 24.0200\n",
      "Episode: 186 Total reward: 38.0 Training loss: 17.0735 Prediction loss: 0.0065 Explore P: 0.6604 RunMean : 24.2200\n",
      "Episode: 187 Total reward: 24.0 Training loss: 12.6143 Prediction loss: 0.0073 Explore P: 0.6589 RunMean : 24.0400\n",
      "Episode: 188 Total reward: 18.0 Training loss: 0.8051 Prediction loss: 0.0026 Explore P: 0.6577 RunMean : 24.0400\n",
      "Episode: 189 Total reward: 42.0 Training loss: 7.8501 Prediction loss: 0.0078 Explore P: 0.6550 RunMean : 24.3200\n",
      "Episode: 190 Total reward: 30.0 Training loss: 16.4356 Prediction loss: 0.0060 Explore P: 0.6530 RunMean : 24.5100\n",
      "Episode: 191 Total reward: 51.0 Training loss: 8.3854 Prediction loss: 0.0041 Explore P: 0.6498 RunMean : 24.9000\n",
      "Episode: 192 Total reward: 17.0 Training loss: 15.6619 Prediction loss: 0.0060 Explore P: 0.6487 RunMean : 24.9500\n",
      "Episode: 193 Total reward: 24.0 Training loss: 12.4177 Prediction loss: 0.0031 Explore P: 0.6471 RunMean : 24.8200\n",
      "Episode: 194 Total reward: 13.0 Training loss: 8.0546 Prediction loss: 0.0041 Explore P: 0.6463 RunMean : 24.8500\n",
      "Episode: 195 Total reward: 26.0 Training loss: 7.3544 Prediction loss: 0.0040 Explore P: 0.6447 RunMean : 24.8700\n",
      "Episode: 196 Total reward: 26.0 Training loss: 0.7885 Prediction loss: 0.0028 Explore P: 0.6430 RunMean : 25.0300\n",
      "Episode: 197 Total reward: 19.0 Training loss: 17.4506 Prediction loss: 0.0065 Explore P: 0.6418 RunMean : 25.1100\n",
      "Episode: 198 Total reward: 43.0 Training loss: 10.3637 Prediction loss: 0.0045 Explore P: 0.6391 RunMean : 25.3900\n",
      "Episode: 199 Total reward: 65.0 Training loss: 19.6392 Prediction loss: 0.0077 Explore P: 0.6350 RunMean : 25.7900\n",
      "Episode: 200 Total reward: 40.0 Training loss: 0.7436 Prediction loss: 0.0036 Explore P: 0.6325 RunMean : 25.9900\n",
      "Episode: 201 Total reward: 23.0 Training loss: 13.3364 Prediction loss: 0.0080 Explore P: 0.6311 RunMean : 26.0400\n",
      "Episode: 202 Total reward: 78.0 Training loss: 23.2810 Prediction loss: 0.0063 Explore P: 0.6263 RunMean : 26.4900\n",
      "Episode: 203 Total reward: 110.37242152316577 Training loss: 8.9921 Prediction loss: 0.0061 Explore P: 0.6196 RunMean : 27.3737\n",
      "Episode: 204 Total reward: 73.0 Training loss: 27.4994 Prediction loss: 0.0101 Explore P: 0.6152 RunMean : 27.9537\n",
      "Episode: 205 Total reward: 42.0 Training loss: 13.0072 Prediction loss: 0.0036 Explore P: 0.6126 RunMean : 28.2437\n",
      "Episode: 206 Total reward: 33.0 Training loss: 1.0981 Prediction loss: 0.0020 Explore P: 0.6106 RunMean : 28.4437\n",
      "Episode: 207 Total reward: 34.0 Training loss: 8.4154 Prediction loss: 0.0060 Explore P: 0.6086 RunMean : 28.6637\n",
      "Episode: 208 Total reward: 40.0 Training loss: 1.4986 Prediction loss: 0.0034 Explore P: 0.6062 RunMean : 28.6637\n",
      "Episode: 209 Total reward: 45.0 Training loss: 0.8073 Prediction loss: 0.0021 Explore P: 0.6035 RunMean : 28.9637\n",
      "Episode: 210 Total reward: 63.0 Training loss: 11.9857 Prediction loss: 0.0043 Explore P: 0.5998 RunMean : 29.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:12:45,877] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 211 Total reward: 86.0 Training loss: 18.2262 Prediction loss: 0.0088 Explore P: 0.5948 RunMean : 30.0937\n",
      "Episode: 212 Total reward: 19.0 Training loss: 11.8683 Prediction loss: 0.0045 Explore P: 0.5937 RunMean : 30.1237\n",
      "Episode: 213 Total reward: 28.0 Training loss: 16.1325 Prediction loss: 0.0048 Explore P: 0.5920 RunMean : 30.1737\n",
      "Episode: 214 Total reward: 99.0 Training loss: 1.1532 Prediction loss: 0.0053 Explore P: 0.5863 RunMean : 30.8337\n",
      "Episode: 215 Total reward: 30.0 Training loss: 14.5294 Prediction loss: 0.0042 Explore P: 0.5846 RunMean : 31.0437\n",
      "Episode: 216 Total reward: 26.0 Training loss: 0.9624 Prediction loss: 0.0025 Explore P: 0.5831 RunMean : 31.1237\n",
      "Episode: 217 Total reward: 63.0 Training loss: 1.3930 Prediction loss: 0.0007 Explore P: 0.5795 RunMean : 31.4837\n",
      "Episode: 218 Total reward: 73.0 Training loss: 1.2936 Prediction loss: 0.0023 Explore P: 0.5753 RunMean : 32.1137\n",
      "Episode: 219 Total reward: 13.0 Training loss: 1.6639 Prediction loss: 0.0026 Explore P: 0.5746 RunMean : 31.8837\n",
      "Episode: 220 Total reward: 50.0 Training loss: 11.5469 Prediction loss: 0.0040 Explore P: 0.5718 RunMean : 32.2637\n",
      "Episode: 221 Total reward: 37.0 Training loss: 33.4458 Prediction loss: 0.0061 Explore P: 0.5697 RunMean : 32.4737\n",
      "Episode: 222 Total reward: 38.0 Training loss: 30.6957 Prediction loss: 0.0091 Explore P: 0.5676 RunMean : 32.7037\n",
      "Episode: 223 Total reward: 86.0 Training loss: 15.3292 Prediction loss: 0.0026 Explore P: 0.5628 RunMean : 33.3537\n",
      "Episode: 224 Total reward: 121.2356272640261 Training loss: 10.6871 Prediction loss: 0.0047 Explore P: 0.5563 RunMean : 34.3861\n",
      "Episode: 225 Total reward: 27.0 Training loss: 22.5162 Prediction loss: 0.0053 Explore P: 0.5548 RunMean : 34.3261\n",
      "Episode: 226 Total reward: 73.0 Training loss: 10.7969 Prediction loss: 0.0026 Explore P: 0.5509 RunMean : 34.7661\n",
      "Episode: 227 Total reward: 38.0 Training loss: 33.3881 Prediction loss: 0.0186 Explore P: 0.5488 RunMean : 34.9961\n",
      "Episode: 228 Total reward: 34.0 Training loss: 43.6887 Prediction loss: 0.0066 Explore P: 0.5470 RunMean : 35.1761\n",
      "Episode: 229 Total reward: 99.0 Training loss: 1.9225 Prediction loss: 0.0011 Explore P: 0.5417 RunMean : 36.0361\n",
      "Episode: 230 Total reward: 36.0 Training loss: 1.1885 Prediction loss: 0.0018 Explore P: 0.5398 RunMean : 36.2161\n",
      "Episode: 231 Total reward: 156.50857780418994 Training loss: 13.3813 Prediction loss: 0.0054 Explore P: 0.5321 RunMean : 37.6312\n",
      "Episode: 232 Total reward: 125.11284875747434 Training loss: 39.9611 Prediction loss: 0.0148 Explore P: 0.5258 RunMean : 38.7323\n",
      "Episode: 233 Total reward: 99.0 Training loss: 6.3495 Prediction loss: 0.0034 Explore P: 0.5207 RunMean : 39.5723\n",
      "Episode: 234 Total reward: 110.61016089105244 Training loss: 1.6403 Prediction loss: 0.0006 Explore P: 0.5152 RunMean : 40.5184\n",
      "Episode: 235 Total reward: 25.0 Training loss: 12.8046 Prediction loss: 0.0024 Explore P: 0.5139 RunMean : 40.4984\n",
      "Episode: 236 Total reward: 121.41510442259029 Training loss: 2.6640 Prediction loss: 0.0024 Explore P: 0.5081 RunMean : 41.5325\n",
      "Episode: 237 Total reward: 132.9551674122955 Training loss: 2.4055 Prediction loss: 0.0006 Explore P: 0.5017 RunMean : 42.6621\n",
      "Episode: 238 Total reward: 149.69276037438553 Training loss: 2.4656 Prediction loss: 0.0025 Explore P: 0.4949 RunMean : 44.0490\n",
      "Episode: 239 Total reward: 120.35010611169615 Training loss: 39.0332 Prediction loss: 0.0044 Explore P: 0.4892 RunMean : 44.9725\n",
      "Episode: 240 Total reward: 101.0 Training loss: 2.3968 Prediction loss: 0.0005 Explore P: 0.4844 RunMean : 45.6925\n",
      "Episode: 241 Total reward: 50.0 Training loss: 36.5895 Prediction loss: 0.0069 Explore P: 0.4820 RunMean : 45.8125\n",
      "Episode: 242 Total reward: 154.94496159621383 Training loss: 21.4065 Prediction loss: 0.0015 Explore P: 0.4753 RunMean : 47.2320\n",
      "Episode: 243 Total reward: 11.0 Training loss: 18.3349 Prediction loss: 0.0056 Explore P: 0.4748 RunMean : 47.1620\n",
      "Episode: 244 Total reward: 187.41622911534245 Training loss: 9.0015 Prediction loss: 0.0018 Explore P: 0.4671 RunMean : 48.8961\n",
      "Episode: 245 Total reward: 156.7262555424348 Training loss: 32.4561 Prediction loss: 0.0049 Explore P: 0.4606 RunMean : 50.3334\n",
      "Episode: 246 Total reward: 161.30320111371267 Training loss: 25.4910 Prediction loss: 0.0021 Explore P: 0.4539 RunMean : 51.7764\n",
      "Episode: 247 Total reward: 207.23343209809298 Training loss: 21.5630 Prediction loss: 0.0118 Explore P: 0.4460 RunMean : 53.6988\n",
      "Episode: 248 Total reward: 19.0 Training loss: 2.2370 Prediction loss: 0.0008 Explore P: 0.4452 RunMean : 53.7088\n",
      "Episode: 249 Total reward: 136.00354586031284 Training loss: 2.7597 Prediction loss: 0.0020 Explore P: 0.4396 RunMean : 54.9088\n",
      "Episode: 250 Total reward: 175.36525262793444 Training loss: 2.9658 Prediction loss: 0.0059 Explore P: 0.4329 RunMean : 56.5725\n",
      "Episode: 251 Total reward: 127.16842378728806 Training loss: 2.8709 Prediction loss: 0.0005 Explore P: 0.4278 RunMean : 57.6941\n",
      "Episode: 252 Total reward: 33.0 Training loss: 65.1610 Prediction loss: 0.0052 Explore P: 0.4264 RunMean : 57.6341\n",
      "Episode: 253 Total reward: 170.77213040732056 Training loss: 42.8773 Prediction loss: 0.0027 Explore P: 0.4201 RunMean : 59.1019\n",
      "Episode: 254 Total reward: 170.95276991465886 Training loss: 3.1010 Prediction loss: 0.0053 Explore P: 0.4139 RunMean : 60.7114\n",
      "Episode: 255 Total reward: 226.17027624293243 Training loss: 32.1020 Prediction loss: 0.0038 Explore P: 0.4059 RunMean : 62.8631\n",
      "Episode: 256 Total reward: 198.88717432393992 Training loss: 39.0565 Prediction loss: 0.0158 Explore P: 0.3990 RunMean : 64.6720\n",
      "Episode: 257 Total reward: 230.01908759055559 Training loss: 3.2295 Prediction loss: 0.0011 Explore P: 0.3913 RunMean : 66.8622\n",
      "Episode: 258 Total reward: 234.23809893925744 Training loss: 4.1729 Prediction loss: 0.0001 Explore P: 0.3838 RunMean : 68.8245\n",
      "Episode: 259 Total reward: 208.11474492574183 Training loss: 3.7161 Prediction loss: 0.0002 Explore P: 0.3772 RunMean : 70.6557\n",
      "Episode: 260 Total reward: 205.78043158877043 Training loss: 2.7094 Prediction loss: 0.0015 Explore P: 0.3707 RunMean : 72.0135\n",
      "Episode: 261 Total reward: 37.0 Training loss: 3.9110 Prediction loss: 0.0007 Explore P: 0.3694 RunMean : 72.0135\n",
      "Episode: 262 Total reward: 142.37242759456794 Training loss: 98.9360 Prediction loss: 0.0042 Explore P: 0.3646 RunMean : 73.2272\n",
      "Episode: 263 Total reward: 37.0 Training loss: 4.2323 Prediction loss: 0.0024 Explore P: 0.3633 RunMean : 73.2672\n",
      "Episode: 264 Total reward: 153.4454942856374 Training loss: 4.2307 Prediction loss: 0.0004 Explore P: 0.3583 RunMean : 74.5417\n",
      "Episode: 265 Total reward: 151.07269868542494 Training loss: 4.5664 Prediction loss: 0.0017 Explore P: 0.3535 RunMean : 75.8524\n",
      "Episode: 266 Total reward: 178.44538594949614 Training loss: 5.8095 Prediction loss: 0.0029 Explore P: 0.3482 RunMean : 77.1068\n",
      "Episode: 267 Total reward: 31.0 Training loss: 4.2487 Prediction loss: 0.0010 Explore P: 0.3471 RunMean : 77.0068\n",
      "Episode: 268 Total reward: 215.91067155395092 Training loss: 307.6974 Prediction loss: 0.0102 Explore P: 0.3409 RunMean : 78.9760\n",
      "Episode: 269 Total reward: 200.74353536651745 Training loss: 3.2757 Prediction loss: 0.0005 Explore P: 0.3352 RunMean : 80.7634\n",
      "Episode: 270 Total reward: 179.4051907091553 Training loss: 5.7317 Prediction loss: 0.0012 Explore P: 0.3300 RunMean : 82.1774\n",
      "Episode: 271 Total reward: 166.76428654617874 Training loss: 75.5789 Prediction loss: 0.0026 Explore P: 0.3252 RunMean : 83.6651\n",
      "Episode: 272 Total reward: 132.95958667579933 Training loss: 2.4071 Prediction loss: 0.0001 Explore P: 0.3213 RunMean : 84.8447\n",
      "Episode: 273 Total reward: 115.36187947633468 Training loss: 3.3350 Prediction loss: 0.0001 Explore P: 0.3178 RunMean : 85.6883\n",
      "Episode: 274 Total reward: 176.16329315791972 Training loss: 4.0895 Prediction loss: 0.0001 Explore P: 0.3130 RunMean : 87.2399\n",
      "Episode: 275 Total reward: 198.15587259563765 Training loss: 43.0011 Prediction loss: 0.0017 Explore P: 0.3077 RunMean : 88.7515\n",
      "Episode: 276 Total reward: 163.96653637891484 Training loss: 17.0461 Prediction loss: 0.0045 Explore P: 0.3034 RunMean : 90.0812\n",
      "Episode: 277 Total reward: 129.91220304028545 Training loss: 109.8786 Prediction loss: 0.0023 Explore P: 0.2997 RunMean : 90.7203\n",
      "Episode: 278 Total reward: 160.65879673032495 Training loss: 28.3885 Prediction loss: 0.0025 Explore P: 0.2955 RunMean : 91.8669\n",
      "Episode: 279 Total reward: 236.48611105089986 Training loss: 138.5972 Prediction loss: 0.0043 Explore P: 0.2899 RunMean : 93.4217\n",
      "Episode: 280 Total reward: 232.87573455429978 Training loss: 3.2123 Prediction loss: 0.0008 Explore P: 0.2844 RunMean : 95.2805\n",
      "Episode: 281 Total reward: 228.0809661490238 Training loss: 4.4241 Prediction loss: 0.0007 Explore P: 0.2791 RunMean : 96.8913\n",
      "Episode: 282 Total reward: 238.49532280694848 Training loss: 3.2741 Prediction loss: 0.0004 Explore P: 0.2738 RunMean : 99.1862\n",
      "Episode: 283 Total reward: 238.62878929465433 Training loss: 136.7869 Prediction loss: 0.0018 Explore P: 0.2685 RunMean : 101.4525\n",
      "Episode: 284 Total reward: 238.19224489611798 Training loss: 3.6113 Prediction loss: 0.0001 Explore P: 0.2634 RunMean : 103.3345\n",
      "Episode: 285 Total reward: 237.1125515782112 Training loss: 3.6148 Prediction loss: 0.0003 Explore P: 0.2584 RunMean : 105.1956\n",
      "Episode: 286 Total reward: 235.75954538611268 Training loss: 3.3547 Prediction loss: 0.0001 Explore P: 0.2535 RunMean : 107.1732\n",
      "Episode: 287 Total reward: 241.75543121295598 Training loss: 245.1668 Prediction loss: 0.0026 Explore P: 0.2487 RunMean : 109.3507\n",
      "Episode: 288 Total reward: 240.19024902654263 Training loss: 2.9322 Prediction loss: 0.0002 Explore P: 0.2439 RunMean : 111.5726\n",
      "Episode: 289 Total reward: 237.5867801467188 Training loss: 4.2473 Prediction loss: 0.0003 Explore P: 0.2393 RunMean : 113.5285\n",
      "Episode: 290 Total reward: 234.10913580499047 Training loss: 13.6991 Prediction loss: 0.0133 Explore P: 0.2348 RunMean : 115.5696\n",
      "Episode: 291 Total reward: 238.72454710135978 Training loss: 2.6474 Prediction loss: 0.0001 Explore P: 0.2303 RunMean : 117.4468\n",
      "Episode: 292 Total reward: 241.69354017452406 Training loss: 5.9002 Prediction loss: 0.0017 Explore P: 0.2259 RunMean : 119.6938\n",
      "Episode: 293 Total reward: 180.4889188955641 Training loss: 3.0853 Prediction loss: 0.0022 Explore P: 0.2225 RunMean : 121.2587\n",
      "Episode: 294 Total reward: 241.4057926683231 Training loss: 3.2596 Prediction loss: 0.0001 Explore P: 0.2183 RunMean : 123.5427\n",
      "Episode: 295 Total reward: 239.22150914518193 Training loss: 2.0969 Prediction loss: 0.0001 Explore P: 0.2142 RunMean : 125.6749\n",
      "Episode: 296 Total reward: 238.9695196304943 Training loss: 3.5024 Prediction loss: 0.0000 Explore P: 0.2102 RunMean : 127.8046\n",
      "Episode: 297 Total reward: 239.73157615468062 Training loss: 74.6026 Prediction loss: 0.0079 Explore P: 0.2062 RunMean : 130.0119\n",
      "Episode: 298 Total reward: 239.71467307734335 Training loss: 2.9290 Prediction loss: 0.0003 Explore P: 0.2023 RunMean : 131.9791\n",
      "Episode: 299 Total reward: 241.42368980340277 Training loss: 2.0103 Prediction loss: 0.0002 Explore P: 0.1985 RunMean : 133.7433\n",
      "Episode: 300 Total reward: 242.01721396423926 Training loss: 1.3742 Prediction loss: 0.0005 Explore P: 0.1948 RunMean : 135.7635\n",
      "Episode: 301 Total reward: 198.19071746281926 Training loss: 0.7275 Prediction loss: 0.0006 Explore P: 0.1916 RunMean : 137.5154\n",
      "Episode: 302 Total reward: 236.873803432588 Training loss: 109.2372 Prediction loss: 0.0018 Explore P: 0.1880 RunMean : 139.1042\n",
      "Episode: 303 Total reward: 204.54322811453696 Training loss: 1.6831 Prediction loss: 0.0003 Explore P: 0.1849 RunMean : 140.0459\n",
      "Episode: 304 Total reward: 241.0751149490027 Training loss: 0.7067 Prediction loss: 0.0000 Explore P: 0.1814 RunMean : 141.7266\n",
      "Episode: 305 Total reward: 237.46134353796606 Training loss: 1.2237 Prediction loss: 0.0000 Explore P: 0.1780 RunMean : 143.6812\n",
      "Episode: 306 Total reward: 241.87715282183103 Training loss: 1.0486 Prediction loss: 0.0001 Explore P: 0.1747 RunMean : 145.7700\n",
      "Episode: 307 Total reward: 233.87133272425186 Training loss: 1.9439 Prediction loss: 0.0002 Explore P: 0.1715 RunMean : 147.7687\n",
      "Episode: 308 Total reward: 242.2485642777311 Training loss: 1.6059 Prediction loss: 0.0001 Explore P: 0.1683 RunMean : 149.7912\n",
      "Episode: 309 Total reward: 240.6166263929359 Training loss: 0.5024 Prediction loss: 0.0002 Explore P: 0.1652 RunMean : 151.7474\n",
      "Episode: 310 Total reward: 242.2034890117231 Training loss: 0.8070 Prediction loss: 0.0002 Explore P: 0.1621 RunMean : 153.5394\n",
      "Episode: 311 Total reward: 241.5029770886129 Training loss: 1.3858 Prediction loss: 0.0082 Explore P: 0.1591 RunMean : 155.0944\n",
      "Episode: 312 Total reward: 238.94663684607826 Training loss: 1.0203 Prediction loss: 0.0003 Explore P: 0.1561 RunMean : 157.2939\n",
      "Episode: 313 Total reward: 241.43988871538457 Training loss: 1.1791 Prediction loss: 0.0004 Explore P: 0.1533 RunMean : 159.4283\n",
      "Episode: 314 Total reward: 242.63409428047112 Training loss: 0.6999 Prediction loss: 0.0000 Explore P: 0.1504 RunMean : 160.8646\n",
      "Episode: 315 Total reward: 244.35875029546145 Training loss: 1.7549 Prediction loss: 0.0002 Explore P: 0.1476 RunMean : 163.0082\n",
      "Episode: 316 Total reward: 235.75995699184566 Training loss: 1.1474 Prediction loss: 0.0002 Explore P: 0.1449 RunMean : 165.1058\n",
      "Episode: 317 Total reward: 240.41190632267467 Training loss: 1.1637 Prediction loss: 0.0002 Explore P: 0.1423 RunMean : 166.8799\n",
      "Episode: 318 Total reward: 240.9924770799245 Training loss: 1.0023 Prediction loss: 0.0001 Explore P: 0.1396 RunMean : 168.5599\n",
      "Episode: 319 Total reward: 241.41532176341684 Training loss: 0.2487 Prediction loss: 0.0002 Explore P: 0.1371 RunMean : 170.8440\n",
      "Episode: 320 Total reward: 243.45059022151082 Training loss: 2.2603 Prediction loss: 0.0127 Explore P: 0.1346 RunMean : 172.7785\n",
      "Episode: 321 Total reward: 244.84450821428862 Training loss: 0.5499 Prediction loss: 0.0001 Explore P: 0.1321 RunMean : 174.8570\n",
      "Episode: 322 Total reward: 242.50174538277017 Training loss: 0.8322 Prediction loss: 0.0001 Explore P: 0.1297 RunMean : 176.9020\n",
      "Episode: 323 Total reward: 242.52262216542394 Training loss: 0.3294 Prediction loss: 0.0000 Explore P: 0.1273 RunMean : 178.4672\n",
      "Episode: 324 Total reward: 241.55011680783892 Training loss: 1.3010 Prediction loss: 0.0001 Explore P: 0.1250 RunMean : 179.6704\n",
      "Episode: 325 Total reward: 243.1587534527447 Training loss: 121.5893 Prediction loss: 0.0039 Explore P: 0.1227 RunMean : 181.8319\n",
      "Episode: 326 Total reward: 242.38628426538034 Training loss: 0.5230 Prediction loss: 0.0000 Explore P: 0.1205 RunMean : 183.5258\n",
      "Episode: 327 Total reward: 243.59751795465513 Training loss: 0.3991 Prediction loss: 0.0001 Explore P: 0.1183 RunMean : 185.5818\n",
      "Episode: 328 Total reward: 241.3859276523793 Training loss: 1.1585 Prediction loss: 0.0001 Explore P: 0.1161 RunMean : 187.6556\n",
      "Episode: 329 Total reward: 244.4352751383601 Training loss: 0.6102 Prediction loss: 0.0000 Explore P: 0.1140 RunMean : 189.1100\n",
      "Episode: 330 Total reward: 242.45690510386783 Training loss: 0.8545 Prediction loss: 0.0001 Explore P: 0.1120 RunMean : 191.1746\n",
      "Episode: 331 Total reward: 243.44049082524586 Training loss: 0.4433 Prediction loss: 0.0001 Explore P: 0.1100 RunMean : 192.0439\n",
      "Episode: 332 Total reward: 222.0456505462054 Training loss: 1.8779 Prediction loss: 0.0090 Explore P: 0.1081 RunMean : 193.0132\n",
      "Episode: 333 Total reward: 243.82630983396845 Training loss: 0.4519 Prediction loss: 0.0000 Explore P: 0.1061 RunMean : 194.4615\n",
      "Episode: 334 Total reward: 242.0780776027853 Training loss: 0.6126 Prediction loss: 0.0000 Explore P: 0.1042 RunMean : 195.7762\n",
      "Episode: 335 Total reward: 240.72822364464568 Training loss: 1.3212 Prediction loss: 0.0102 Explore P: 0.1024 RunMean : 197.9334\n",
      "Episode: 336 Total reward: 225.87475589647917 Training loss: 0.2233 Prediction loss: 0.0000 Explore P: 0.1006 RunMean : 198.9780\n",
      "Episode: 337 Total reward: 245.38159959281668 Training loss: 0.6430 Prediction loss: 0.0000 Explore P: 0.0988 RunMean : 200.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:20:34,287] Disabling video recorder because <TimeLimit<PredictObsCartpoleEnv<PredictObsCartpole-v0>>> neither supports video mode \"rgb_array\" nor \"ansi\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 338 Total reward: 244.75584993836063 Training loss: 0.5921 Prediction loss: 0.0000 Explore P: 0.0971 RunMean : 201.0529\n",
      "Episode: 339 Total reward: 242.9879550293222 Training loss: 0.4692 Prediction loss: 0.0000 Explore P: 0.0954 RunMean : 202.2793\n",
      "Episode: 340 Total reward: 242.5906501269794 Training loss: 0.1699 Prediction loss: 0.0000 Explore P: 0.0937 RunMean : 203.6952\n",
      "Episode: 341 Total reward: 244.34886594612564 Training loss: 0.7092 Prediction loss: 0.0000 Explore P: 0.0920 RunMean : 205.6387\n",
      "Episode: 342 Total reward: 241.59383304325314 Training loss: 0.8630 Prediction loss: 0.0003 Explore P: 0.0904 RunMean : 206.5052\n",
      "Episode: 343 Total reward: 243.05254336787266 Training loss: 1.3782 Prediction loss: 0.0093 Explore P: 0.0888 RunMean : 208.8257\n",
      "Episode: 344 Total reward: 240.38642660920152 Training loss: 0.3063 Prediction loss: 0.0003 Explore P: 0.0872 RunMean : 209.3554\n",
      "Episode: 345 Total reward: 243.02256756700086 Training loss: 0.7445 Prediction loss: 0.0000 Explore P: 0.0857 RunMean : 210.2184\n",
      "Episode: 346 Total reward: 242.62546846877396 Training loss: 1.0206 Prediction loss: 0.0001 Explore P: 0.0842 RunMean : 211.0316\n",
      "Episode: 347 Total reward: 244.38479494379004 Training loss: 0.2211 Prediction loss: 0.0000 Explore P: 0.0827 RunMean : 211.4031\n",
      "Episode: 348 Total reward: 244.98783882156204 Training loss: 0.3960 Prediction loss: 0.0000 Explore P: 0.0813 RunMean : 213.6630\n",
      "Episode: 349 Total reward: 243.1341248525649 Training loss: 0.2078 Prediction loss: 0.0001 Explore P: 0.0799 RunMean : 214.7343\n",
      "Episode: 350 Total reward: 243.97368027146658 Training loss: 0.3026 Prediction loss: 0.0001 Explore P: 0.0785 RunMean : 215.4204\n",
      "Episode: 351 Total reward: 242.5391036443501 Training loss: 0.8915 Prediction loss: 0.0001 Explore P: 0.0771 RunMean : 216.5741\n",
      "Episode: 352 Total reward: 243.74428926215586 Training loss: 0.4789 Prediction loss: 0.0003 Explore P: 0.0758 RunMean : 218.6815\n",
      "Episode: 353 Total reward: 244.17705881054675 Training loss: 239.8291 Prediction loss: 0.0004 Explore P: 0.0745 RunMean : 219.4156\n",
      "Episode: 354 Total reward: 243.2178542302716 Training loss: 2.4693 Prediction loss: 0.0084 Explore P: 0.0732 RunMean : 220.1382\n",
      "Episode: 355 Total reward: 243.466393102267 Training loss: 181.3161 Prediction loss: 0.0007 Explore P: 0.0720 RunMean : 220.3112\n",
      "Episode: 356 Total reward: 243.5957190775452 Training loss: 0.3570 Prediction loss: 0.0000 Explore P: 0.0708 RunMean : 220.7583\n",
      "Episode: 357 Total reward: 245.4454915352234 Training loss: 313.5989 Prediction loss: 0.0001 Explore P: 0.0695 RunMean : 220.9125\n",
      "Episode: 358 Total reward: 243.02320356597974 Training loss: 0.1791 Prediction loss: 0.0004 Explore P: 0.0684 RunMean : 221.0004\n",
      "Episode: 359 Total reward: 244.85178758049946 Training loss: 0.4056 Prediction loss: 0.0001 Explore P: 0.0672 RunMean : 221.3678\n",
      "Episode: 360 Total reward: 243.2308518270633 Training loss: 0.2486 Prediction loss: 0.0000 Explore P: 0.0661 RunMean : 221.7423\n",
      "Episode: 361 Total reward: 246.10989928343412 Training loss: 0.5634 Prediction loss: 0.0001 Explore P: 0.0650 RunMean : 223.8334\n",
      "Episode: 362 Total reward: 243.32881832541375 Training loss: 0.1928 Prediction loss: 0.0000 Explore P: 0.0639 RunMean : 224.8429\n",
      "Episode: 363 Total reward: 241.94924000218174 Training loss: 0.5290 Prediction loss: 0.0000 Explore P: 0.0628 RunMean : 226.8924\n",
      "Episode: 364 Total reward: 244.92563796871303 Training loss: 0.3279 Prediction loss: 0.0000 Explore P: 0.0618 RunMean : 227.8072\n",
      "Episode: 365 Total reward: 244.10948755613646 Training loss: 0.0743 Prediction loss: 0.0001 Explore P: 0.0607 RunMean : 228.7376\n",
      "Episode: 366 Total reward: 244.40917234322475 Training loss: 0.1364 Prediction loss: 0.0000 Explore P: 0.0597 RunMean : 229.3972\n",
      "Episode: 367 Total reward: 245.89573968259873 Training loss: 0.1598 Prediction loss: 0.0000 Explore P: 0.0588 RunMean : 231.5462\n",
      "Episode: 368 Total reward: 245.23756146889295 Training loss: 0.1988 Prediction loss: 0.0000 Explore P: 0.0578 RunMean : 231.8395\n",
      "Episode: 369 Total reward: 246.1072273437575 Training loss: 0.2687 Prediction loss: 0.0000 Explore P: 0.0568 RunMean : 232.2931\n",
      "Episode: 370 Total reward: 241.7247447329269 Training loss: 0.3230 Prediction loss: 0.0000 Explore P: 0.0559 RunMean : 232.9163\n",
      "Episode: 371 Total reward: 245.694318288003 Training loss: 157.5448 Prediction loss: 0.0006 Explore P: 0.0550 RunMean : 233.7056\n",
      "Episode: 372 Total reward: 243.11926973078243 Training loss: 0.3624 Prediction loss: 0.0001 Explore P: 0.0541 RunMean : 234.8072\n",
      "Episode: 373 Total reward: 245.90585441180352 Training loss: 0.4005 Prediction loss: 0.0082 Explore P: 0.0532 RunMean : 236.1126\n",
      "Episode: 374 Total reward: 245.00404231894953 Training loss: 0.2361 Prediction loss: 0.0000 Explore P: 0.0524 RunMean : 236.8010\n",
      "Episode: 375 Total reward: 244.45754799383838 Training loss: 0.2567 Prediction loss: 0.0000 Explore P: 0.0515 RunMean : 237.2641\n",
      "Episode: 376 Total reward: 244.72202214110908 Training loss: 0.6901 Prediction loss: 0.0083 Explore P: 0.0507 RunMean : 238.0716\n",
      "Episode: 377 Total reward: 245.28932325432652 Training loss: 0.2858 Prediction loss: 0.0001 Explore P: 0.0499 RunMean : 239.2254\n",
      "Episode: 378 Total reward: 245.6875871989008 Training loss: 0.2625 Prediction loss: 0.0000 Explore P: 0.0491 RunMean : 240.0757\n",
      "Episode: 379 Total reward: 245.03053885886408 Training loss: 0.2086 Prediction loss: 0.0000 Explore P: 0.0484 RunMean : 240.1611\n",
      "Episode: 380 Total reward: 245.38569653456958 Training loss: 1.0851 Prediction loss: 0.0074 Explore P: 0.0476 RunMean : 240.2862\n",
      "Episode: 381 Total reward: 246.80058480676462 Training loss: 0.0974 Prediction loss: 0.0001 Explore P: 0.0468 RunMean : 240.4734\n",
      "Episode: 382 Total reward: 246.2796830847285 Training loss: 3.2031 Prediction loss: 0.0075 Explore P: 0.0461 RunMean : 240.5512\n",
      "Episode: 383 Total reward: 244.00690836594913 Training loss: 0.2864 Prediction loss: 0.0000 Explore P: 0.0454 RunMean : 240.6050\n",
      "Episode: 384 Total reward: 244.8734226306783 Training loss: 0.1224 Prediction loss: 0.0000 Explore P: 0.0447 RunMean : 240.6718\n",
      "Episode: 385 Total reward: 244.06714968803408 Training loss: 0.2986 Prediction loss: 0.0000 Explore P: 0.0440 RunMean : 240.7414\n",
      "Episode: 386 Total reward: 244.92244290748806 Training loss: 0.2980 Prediction loss: 0.0000 Explore P: 0.0433 RunMean : 240.8330\n",
      "Episode: 387 Total reward: 243.1742614999821 Training loss: 0.1338 Prediction loss: 0.0004 Explore P: 0.0427 RunMean : 240.8472\n",
      "Episode: 388 Total reward: 245.7401038549388 Training loss: 0.1234 Prediction loss: 0.0000 Explore P: 0.0420 RunMean : 240.9027\n",
      "Episode: 389 Total reward: 245.24463894000368 Training loss: 0.2149 Prediction loss: 0.0000 Explore P: 0.0414 RunMean : 240.9793\n",
      "Episode: 390 Total reward: 246.21167163683418 Training loss: 0.2797 Prediction loss: 0.0000 Explore P: 0.0408 RunMean : 241.1003\n",
      "Episode: 391 Total reward: 244.31247048639216 Training loss: 0.1153 Prediction loss: 0.0000 Explore P: 0.0402 RunMean : 241.1562\n",
      "Episode: 392 Total reward: 243.53033415271338 Training loss: 0.3785 Prediction loss: 0.0000 Explore P: 0.0396 RunMean : 241.1746\n",
      "Episode: 393 Total reward: 246.7466683930995 Training loss: 0.2142 Prediction loss: 0.0000 Explore P: 0.0390 RunMean : 241.8371\n",
      "Episode: 394 Total reward: 246.45918967942887 Training loss: 0.1427 Prediction loss: 0.0000 Explore P: 0.0384 RunMean : 241.8877\n",
      "Episode: 395 Total reward: 246.1877015881628 Training loss: 0.2883 Prediction loss: 0.0000 Explore P: 0.0378 RunMean : 241.9573\n",
      "Episode: 396 Total reward: 244.646957083386 Training loss: 0.1355 Prediction loss: 0.0000 Explore P: 0.0373 RunMean : 242.0141\n",
      "Episode: 397 Total reward: 246.21443381690017 Training loss: 0.1712 Prediction loss: 0.0000 Explore P: 0.0368 RunMean : 242.0789\n",
      "Episode: 398 Total reward: 245.17853604730064 Training loss: 0.3115 Prediction loss: 0.0000 Explore P: 0.0362 RunMean : 242.1336\n",
      "Episode: 399 Total reward: 245.571587738207 Training loss: 0.5862 Prediction loss: 0.0000 Explore P: 0.0357 RunMean : 242.1750\n",
      "Episode: 400 Total reward: 246.4794955571624 Training loss: 0.1410 Prediction loss: 0.0001 Explore P: 0.0352 RunMean : 242.2197\n",
      "Episode: 401 Total reward: 245.7126879893877 Training loss: 0.3203 Prediction loss: 0.0001 Explore P: 0.0347 RunMean : 242.6949\n",
      "Episode: 402 Total reward: 245.0539010483164 Training loss: 18.0244 Prediction loss: 0.0023 Explore P: 0.0342 RunMean : 242.7767\n",
      "Episode: 403 Total reward: 245.78456694024746 Training loss: 0.2177 Prediction loss: 0.0000 Explore P: 0.0337 RunMean : 243.1891\n",
      "Episode: 404 Total reward: 244.00957565414578 Training loss: 0.2903 Prediction loss: 0.0000 Explore P: 0.0333 RunMean : 243.2184\n",
      "Episode: 405 Total reward: 246.08849595554835 Training loss: 0.2650 Prediction loss: 0.0000 Explore P: 0.0328 RunMean : 243.3047\n",
      "Episode: 406 Total reward: 244.85876756963646 Training loss: 0.2605 Prediction loss: 0.0000 Explore P: 0.0323 RunMean : 243.3345\n",
      "Episode: 407 Total reward: 244.0581570761694 Training loss: 175.4272 Prediction loss: 0.0003 Explore P: 0.0319 RunMean : 243.4364\n",
      "Episode: 408 Total reward: 245.4693808283286 Training loss: 0.1864 Prediction loss: 0.0000 Explore P: 0.0315 RunMean : 243.4686\n",
      "Episode: 409 Total reward: 246.18535949626101 Training loss: 0.1710 Prediction loss: 0.0000 Explore P: 0.0310 RunMean : 243.5243\n",
      "Episode: 410 Total reward: 238.0474463861037 Training loss: 4.5321 Prediction loss: 0.0077 Explore P: 0.0306 RunMean : 243.4827\n",
      "Episode: 411 Total reward: 245.97924417559761 Training loss: 0.2126 Prediction loss: 0.0000 Explore P: 0.0302 RunMean : 243.5275\n",
      "Episode: 412 Total reward: 244.29940243585764 Training loss: 0.1978 Prediction loss: 0.0000 Explore P: 0.0298 RunMean : 243.5810\n",
      "Episode: 413 Total reward: 245.3477151726394 Training loss: 287.9951 Prediction loss: 0.0026 Explore P: 0.0294 RunMean : 243.6201\n",
      "Episode: 414 Total reward: 245.7498640276035 Training loss: 0.2693 Prediction loss: 0.0000 Explore P: 0.0290 RunMean : 243.6513\n",
      "Episode: 415 Total reward: 246.2279476271266 Training loss: 0.1571 Prediction loss: 0.0000 Explore P: 0.0287 RunMean : 243.6700\n",
      "Episode: 416 Total reward: 246.9829173709815 Training loss: 10.6322 Prediction loss: 0.0021 Explore P: 0.0283 RunMean : 243.7822\n",
      "Episode: 417 Total reward: 247.27348076847383 Training loss: 0.0671 Prediction loss: 0.0000 Explore P: 0.0279 RunMean : 243.8508\n",
      "Episode: 418 Total reward: 244.87259837166658 Training loss: 0.1609 Prediction loss: 0.0001 Explore P: 0.0276 RunMean : 243.8896\n",
      "Episode: 419 Total reward: 245.65964998526502 Training loss: 0.1380 Prediction loss: 0.0000 Explore P: 0.0272 RunMean : 243.9320\n",
      "Episode: 420 Total reward: 245.6649277811586 Training loss: 0.3016 Prediction loss: 0.0000 Explore P: 0.0269 RunMean : 243.9542\n",
      "Episode: 421 Total reward: 245.5910438768567 Training loss: 0.1298 Prediction loss: 0.0000 Explore P: 0.0266 RunMean : 243.9617\n",
      "Episode: 422 Total reward: 246.36451035308562 Training loss: 0.3229 Prediction loss: 0.0000 Explore P: 0.0262 RunMean : 244.0003\n",
      "Episode: 423 Total reward: 245.39463167246865 Training loss: 0.5266 Prediction loss: 0.0000 Explore P: 0.0259 RunMean : 244.0290\n",
      "Episode: 424 Total reward: 245.44022754228524 Training loss: 0.3571 Prediction loss: 0.0000 Explore P: 0.0256 RunMean : 244.0679\n",
      "Episode: 425 Total reward: 244.72804635314827 Training loss: 0.2925 Prediction loss: 0.0000 Explore P: 0.0253 RunMean : 244.0836\n",
      "Episode: 426 Total reward: 245.1341968786845 Training loss: 0.3454 Prediction loss: 0.0000 Explore P: 0.0250 RunMean : 244.1111\n",
      "Episode: 427 Total reward: 245.07555266968484 Training loss: 0.2716 Prediction loss: 0.0000 Explore P: 0.0247 RunMean : 244.1259\n",
      "Episode: 428 Total reward: 244.03991557873718 Training loss: 270.6325 Prediction loss: 0.0000 Explore P: 0.0244 RunMean : 244.1524\n",
      "Episode: 429 Total reward: 246.57943197480003 Training loss: 0.1425 Prediction loss: 0.0000 Explore P: 0.0241 RunMean : 244.1738\n",
      "Episode: 430 Total reward: 245.81747266341614 Training loss: 522.6747 Prediction loss: 0.0009 Explore P: 0.0238 RunMean : 244.2074\n",
      "Episode: 431 Total reward: 244.9770383465222 Training loss: 0.1881 Prediction loss: 0.0000 Explore P: 0.0236 RunMean : 244.2228\n",
      "Episode: 432 Total reward: 245.46797691159324 Training loss: 0.3180 Prediction loss: 0.0000 Explore P: 0.0233 RunMean : 244.4570\n",
      "Episode: 433 Total reward: 244.81544100892933 Training loss: 0.2493 Prediction loss: 0.0000 Explore P: 0.0230 RunMean : 244.4669\n",
      "Episode: 434 Total reward: 246.58771121458318 Training loss: 0.4948 Prediction loss: 0.0000 Explore P: 0.0228 RunMean : 244.5120\n",
      "Episode: 435 Total reward: 246.3906922174826 Training loss: 0.3794 Prediction loss: 0.0000 Explore P: 0.0225 RunMean : 244.5686\n",
      "Episode: 436 Total reward: 246.70454356502935 Training loss: 0.1775 Prediction loss: 0.0000 Explore P: 0.0223 RunMean : 244.7769\n",
      "Episode: 437 Total reward: 248.02781514915895 Training loss: 0.1990 Prediction loss: 0.0000 Explore P: 0.0220 RunMean : 244.8034\n",
      "Episode: 438 Total reward: 245.48788832760926 Training loss: 0.5526 Prediction loss: 0.0000 Explore P: 0.0218 RunMean : 244.8107\n",
      "Episode: 439 Total reward: 246.2738594679441 Training loss: 0.5038 Prediction loss: 0.0000 Explore P: 0.0216 RunMean : 244.8436\n",
      "Episode: 440 Total reward: 245.18699708405865 Training loss: 0.3205 Prediction loss: 0.0000 Explore P: 0.0213 RunMean : 244.8695\n",
      "Episode: 441 Total reward: 245.78463308956847 Training loss: 0.2950 Prediction loss: 0.0000 Explore P: 0.0211 RunMean : 244.8839\n",
      "Episode: 442 Total reward: 246.1463809885285 Training loss: 0.1991 Prediction loss: 0.0000 Explore P: 0.0209 RunMean : 244.9294\n",
      "Episode: 443 Total reward: 247.19069609200625 Training loss: 0.2864 Prediction loss: 0.0000 Explore P: 0.0207 RunMean : 244.9708\n",
      "Episode: 444 Total reward: 245.72926120553322 Training loss: 0.5483 Prediction loss: 0.0000 Explore P: 0.0205 RunMean : 245.0242\n",
      "Episode: 445 Total reward: 245.55258699880022 Training loss: 217.5879 Prediction loss: 0.0000 Explore P: 0.0202 RunMean : 245.0495\n",
      "Episode: 446 Total reward: 247.08332363769222 Training loss: 0.5412 Prediction loss: 0.0000 Explore P: 0.0200 RunMean : 245.0941\n",
      "Episode: 447 Total reward: 246.85738534312313 Training loss: 0.1870 Prediction loss: 0.0000 Explore P: 0.0198 RunMean : 245.1188\n",
      "Episode: 448 Total reward: 247.64212179244106 Training loss: 240.0838 Prediction loss: 0.0001 Explore P: 0.0196 RunMean : 245.1454\n",
      "Episode: 449 Total reward: 241.83015187132645 Training loss: 0.3452 Prediction loss: 0.0000 Explore P: 0.0195 RunMean : 245.1324\n",
      "Episode: 450 Total reward: 245.6562956566759 Training loss: 0.3157 Prediction loss: 0.0000 Explore P: 0.0193 RunMean : 245.1492\n",
      "Episode: 451 Total reward: 245.25980898094696 Training loss: 0.2960 Prediction loss: 0.0000 Explore P: 0.0191 RunMean : 245.1764\n",
      "Episode: 452 Total reward: 245.9415740035622 Training loss: 0.4432 Prediction loss: 0.0000 Explore P: 0.0189 RunMean : 245.1984\n",
      "Episode: 453 Total reward: 245.9456239668021 Training loss: 9.6998 Prediction loss: 0.0026 Explore P: 0.0187 RunMean : 245.2160\n",
      "Episode: 454 Total reward: 244.9398342677514 Training loss: 0.4352 Prediction loss: 0.0000 Explore P: 0.0186 RunMean : 245.2333\n",
      "Episode: 455 Total reward: 243.91479360357337 Training loss: 0.3660 Prediction loss: 0.0000 Explore P: 0.0184 RunMean : 245.2377\n",
      "Episode: 456 Total reward: 244.83825429533158 Training loss: 0.4841 Prediction loss: 0.0000 Explore P: 0.0182 RunMean : 245.2502\n",
      "Episode: 457 Total reward: 246.5379952231068 Training loss: 0.2896 Prediction loss: 0.0000 Explore P: 0.0181 RunMean : 245.2611\n",
      "Episode: 458 Total reward: 246.53779133283828 Training loss: 0.3693 Prediction loss: 0.0000 Explore P: 0.0179 RunMean : 245.2962\n",
      "Episode: 459 Total reward: 247.3849965238494 Training loss: 0.2305 Prediction loss: 0.0000 Explore P: 0.0177 RunMean : 245.3216\n",
      "Episode: 460 Total reward: 244.2515691399576 Training loss: 297.7038 Prediction loss: 0.0001 Explore P: 0.0176 RunMean : 245.3318\n",
      "Episode: 461 Total reward: 246.2378732510421 Training loss: 0.4533 Prediction loss: 0.0000 Explore P: 0.0174 RunMean : 245.3331\n",
      "Episode: 462 Total reward: 245.72372282229392 Training loss: 454.8138 Prediction loss: 0.0021 Explore P: 0.0173 RunMean : 245.3570\n",
      "Episode: 463 Total reward: 245.46801406164224 Training loss: 222.3119 Prediction loss: 0.0019 Explore P: 0.0171 RunMean : 245.3922\n",
      "Episode: 464 Total reward: 246.1573481199041 Training loss: 0.7508 Prediction loss: 0.0000 Explore P: 0.0170 RunMean : 245.4045\n",
      "Episode: 465 Total reward: 228.91693209712452 Training loss: 0.3730 Prediction loss: 0.0000 Explore P: 0.0169 RunMean : 245.2526\n",
      "Episode: 466 Total reward: 241.55224429135626 Training loss: 0.5998 Prediction loss: 0.0000 Explore P: 0.0167 RunMean : 245.2240\n",
      "Episode: 467 Total reward: 246.06189346094055 Training loss: 1.0872 Prediction loss: 0.0000 Explore P: 0.0166 RunMean : 245.2257\n",
      "Episode: 468 Total reward: 246.49936763531215 Training loss: 0.5128 Prediction loss: 0.0000 Explore P: 0.0165 RunMean : 245.2383\n",
      "Episode: 469 Total reward: 244.89151539158212 Training loss: 0.3426 Prediction loss: 0.0000 Explore P: 0.0163 RunMean : 245.2261\n",
      "Episode: 470 Total reward: 245.05886864231596 Training loss: 497.5809 Prediction loss: 0.0051 Explore P: 0.0162 RunMean : 245.2595\n",
      "Episode: 471 Total reward: 244.92105723039305 Training loss: 0.4145 Prediction loss: 0.0000 Explore P: 0.0161 RunMean : 245.2518\n",
      "Episode: 472 Total reward: 243.64688394821889 Training loss: 0.4826 Prediction loss: 0.0000 Explore P: 0.0160 RunMean : 245.2570\n",
      "Episode: 473 Total reward: 246.34866800881326 Training loss: 0.2420 Prediction loss: 0.0000 Explore P: 0.0159 RunMean : 245.2615\n",
      "Episode: 474 Total reward: 246.20161583841661 Training loss: 0.4071 Prediction loss: 0.0000 Explore P: 0.0157 RunMean : 245.2734\n",
      "Episode: 475 Total reward: 244.83778605471718 Training loss: 0.6138 Prediction loss: 0.0000 Explore P: 0.0156 RunMean : 245.2772\n",
      "Episode: 476 Total reward: 247.2512029118396 Training loss: 0.5092 Prediction loss: 0.0000 Explore P: 0.0155 RunMean : 245.3025\n",
      "Episode: 477 Total reward: 242.4813929010689 Training loss: 0.4017 Prediction loss: 0.0000 Explore P: 0.0154 RunMean : 245.2744\n",
      "Episode: 478 Total reward: 245.6084060437009 Training loss: 0.4784 Prediction loss: 0.0000 Explore P: 0.0153 RunMean : 245.2737\n",
      "Episode: 479 Total reward: 247.49998500373277 Training loss: 0.4205 Prediction loss: 0.0000 Explore P: 0.0152 RunMean : 245.2983\n",
      "Episode: 480 Total reward: 245.46278882605432 Training loss: 0.6261 Prediction loss: 0.0000 Explore P: 0.0151 RunMean : 245.2991\n",
      "Episode: 481 Total reward: 243.5863677904114 Training loss: 0.4659 Prediction loss: 0.0000 Explore P: 0.0150 RunMean : 245.2670\n",
      "Episode: 482 Total reward: 237.19172212939787 Training loss: 0.4991 Prediction loss: 0.0000 Explore P: 0.0149 RunMean : 245.1761\n",
      "Episode: 483 Total reward: 245.3576952623813 Training loss: 291.8293 Prediction loss: 0.0086 Explore P: 0.0148 RunMean : 245.1896\n",
      "Episode: 484 Total reward: 243.72707789483215 Training loss: 0.7092 Prediction loss: 0.0000 Explore P: 0.0147 RunMean : 245.1781\n",
      "Episode: 485 Total reward: 246.10193081744924 Training loss: 260.2836 Prediction loss: 0.0150 Explore P: 0.0146 RunMean : 245.1985\n",
      "Episode: 486 Total reward: 246.36372763951587 Training loss: 0.2810 Prediction loss: 0.0000 Explore P: 0.0145 RunMean : 245.2129\n",
      "Episode: 487 Total reward: 246.9271025906976 Training loss: 0.4873 Prediction loss: 0.0000 Explore P: 0.0144 RunMean : 245.2504\n",
      "Episode: 488 Total reward: 247.0018326733195 Training loss: 0.5711 Prediction loss: 0.0001 Explore P: 0.0143 RunMean : 245.2630\n",
      "Episode: 489 Total reward: 243.7996295573056 Training loss: 0.4688 Prediction loss: 0.0000 Explore P: 0.0143 RunMean : 245.2486\n",
      "Episode: 490 Total reward: 244.83325632381187 Training loss: 0.3640 Prediction loss: 0.0000 Explore P: 0.0142 RunMean : 245.2348\n",
      "Episode: 491 Total reward: 245.9537469979118 Training loss: 0.3731 Prediction loss: 0.0000 Explore P: 0.0141 RunMean : 245.2512\n",
      "Episode: 492 Total reward: 241.04010861997955 Training loss: 0.7024 Prediction loss: 0.0001 Explore P: 0.0140 RunMean : 245.2263\n",
      "Episode: 493 Total reward: 245.42478047476183 Training loss: 0.3381 Prediction loss: 0.0000 Explore P: 0.0139 RunMean : 245.2131\n",
      "Episode: 494 Total reward: 245.62270255340124 Training loss: 155.0858 Prediction loss: 0.0049 Explore P: 0.0138 RunMean : 245.2047\n",
      "Episode: 495 Total reward: 244.70344728036193 Training loss: 0.5210 Prediction loss: 0.0000 Explore P: 0.0138 RunMean : 245.1899\n",
      "Episode: 496 Total reward: 230.9997407043114 Training loss: 0.3150 Prediction loss: 0.0000 Explore P: 0.0137 RunMean : 245.0534\n",
      "Episode: 497 Total reward: 242.38742493313833 Training loss: 0.4837 Prediction loss: 0.0000 Explore P: 0.0136 RunMean : 245.0152\n",
      "Episode: 498 Total reward: 244.66172300723463 Training loss: 122.4903 Prediction loss: 0.0089 Explore P: 0.0136 RunMean : 245.0100\n",
      "Episode: 499 Total reward: 208.24042554848035 Training loss: 155.3623 Prediction loss: 0.0085 Explore P: 0.0135 RunMean : 244.6367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:34:14,816] Finished writing results. You can upload them to the scoreboard via gym.upload('D:\\\\tmp\\\\PredictObsCartpole-experiment-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average training reward =  133.309011436\n",
      "average test reward =  133.309011436    number of trials =  500\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dc2df92f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from gym import wrappers\n",
    "import random\n",
    "\n",
    "random.seed()\n",
    "%matplotlib inline\n",
    "# base code originally from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb\n",
    "\n",
    "# Restricts to running on a single GPU, in this case the second GPU (\"1\")\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0., \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            # placeholder for input states\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "            \n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "            \n",
    "            # placeholder for next state\n",
    "            self.next_state_ = tf.placeholder(tf.float32,\\\n",
    "                                             [None, state_size],\\\n",
    "                                             name='next_state')\n",
    "            \n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "            \n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "            \n",
    "                \n",
    "            # Q Value network :\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=None,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.maximum(alpha*self.fc1,self.fc1)\n",
    "            \n",
    "            self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.maximum(alpha*self.fc2,self.fc2)\n",
    "                \n",
    "            out_layer = self.fc2\n",
    "            \n",
    "\n",
    "            # Predict next state network :\n",
    "            self.pred_input = tf.concat([self.inputs_,one_hot_actions],1)\n",
    "            \n",
    "            self.pred_fc1 = tf.layers.dense(self.pred_input, \\\n",
    "                                           hidden_size,\\\n",
    "                                           activation=None,\\\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.pred_fc1 = tf.maximum(alpha*self.pred_fc1,self.pred_fc1)\n",
    "            \n",
    "            self.pred_fc2 = tf.layers.dense(self.pred_fc1, \\\n",
    "                                           hidden_size,\\\n",
    "                                           activation=None,\\\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.pred_fc2 = tf.maximum(alpha*self.pred_fc2,self.pred_fc2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # Linear output layer for next state prediction\n",
    "            self.prediction_state = tf.layers.dense(self.pred_fc2, state_size, \\\n",
    "                                                    activation=None,\\\n",
    "                                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            # loss and optimizer for Q network\n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            \n",
    "            # loss and optimizer for next state network\n",
    "            self.prediction_loss = tf.reduce_mean(tf.square(self.next_state_ - self.prediction_state))\n",
    "            self.pred_opt = tf.train.AdamOptimizer(learning_rate).minimize(self.prediction_loss)\n",
    "\n",
    "\n",
    "            \n",
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N \n",
    "\n",
    "def normalize_state(x, denormalize=False):\n",
    "    normalizer = [2.,3.,0.3,2.]\n",
    "    if denormalize:\n",
    "        y = x * normalizer\n",
    "    else:\n",
    "        \n",
    "        y = x / normalizer\n",
    "    return y\n",
    "\n",
    "def predict_next_state(mainQN,state,sess):\n",
    "    Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: state})\n",
    "    action = np.argmax(Qs)\n",
    "    pred_state = sess.run(mainQN.prediction_state, \n",
    "                          feed_dict={mainQN.inputs_: state,\n",
    "                                    mainQN.actions_: np.expand_dims(action, axis=0)})\n",
    "\n",
    "    return pred_state\n",
    "\n",
    "def initialize_memory_rand_states(memory_size=1000,pretrain_length=32):\n",
    "    \n",
    "    # Initialize the simulation\n",
    "    # Make a random action\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    ii = 0\n",
    "    while ii < pretrain_length or not done:\n",
    "        \n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        action = action[0]\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "            state = normalize_state(state)\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        \n",
    "        ii = ii + 1\n",
    "            \n",
    "    return memory\n",
    "\n",
    "def train_q_network(mainQN,\\\n",
    "                    memory,\\\n",
    "                    train_episodes=1500,\\\n",
    "                    gamma=0.99,\\\n",
    "                    explore_start=1.0,\\\n",
    "                    explore_stop=0.01,\\\n",
    "                    decay_rate=0.0001,\\\n",
    "                    batch_size=32,\\\n",
    "                    max_steps=500,\\\n",
    "                    verbose=True):\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        steps_list = []\n",
    "        \n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            \n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "\n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    #action = env.action_space.sample()\n",
    "                    action = random.randint(0,1)\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "\n",
    "\n",
    "                #action is first\n",
    "                pred_action = []\n",
    "                pred_action.append(action)\n",
    "                \n",
    "                #predict next state from this action\n",
    "                next_state = state.reshape((1, *state.shape))\n",
    "                for iterator in range(5):\n",
    "                    #predict next action from this state\n",
    "                    next_state = predict_next_state(mainQN,next_state,sess)\n",
    "                    pred_action.append(normalize_state(next_state,denormalize=True))\n",
    "                                                \n",
    "                \n",
    "                                   \n",
    "                                   \n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(pred_action)\n",
    "                next_state = normalize_state(next_state)\n",
    "                \n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    t = t+1\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    steps_list.append(total_reward)\n",
    "                    t = max_steps\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = env.reset()\n",
    "                    state = normalize_state(state)\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "                \n",
    "                pred_loss, _ = sess.run([mainQN.prediction_loss, mainQN.pred_opt],\n",
    "                                       feed_dict={mainQN.inputs_: states,\n",
    "                                                mainQN.actions_: actions,\n",
    "                                                mainQN.next_state_: next_states})\n",
    "            \n",
    "            rewards_list.append((ep, total_reward))   \n",
    "            runningMean = np.mean(steps_list[-100:])\n",
    "            if verbose:\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Prediction loss: {:.4f}'.format(pred_loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p),\n",
    "                      'RunMean : {:.4f}'.format(runningMean))\n",
    "               \n",
    "            \n",
    "            \n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver, runningMean\n",
    "\n",
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "def generate_and_train_qnetwork(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=128,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=32,\\\n",
    "                   alpha=0.1,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, \\\n",
    "                      hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "\n",
    "    \n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver, runningMean = train_q_network(mainQN,\\\n",
    "                                  memory,\\\n",
    "                                  train_episodes = train_episodes, \\\n",
    "                                  gamma=gamma,\\\n",
    "                                  explore_start=explore_start,\\\n",
    "                                  explore_stop=explore_stop,\\\n",
    "                                  decay_rate=decay_rate,\\\n",
    "                                  batch_size=batch_size,\\\n",
    "                                  verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    \n",
    "    return avg_train_rewards, mainQN, saver, len(rewards_list)\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('PredictObsCartpole-v0')\n",
    "\n",
    "# Start monitor\n",
    "env = wrappers.Monitor(env, '/tmp/PredictObsCartpole-experiment-1',force=True)\n",
    "\n",
    "# Train network\n",
    "avg_train_rewards, mainQN, saver, number_of_episodes = generate_and_train_qnetwork(train_episodes=500, verbose=True)\n",
    "print('average test reward = ', avg_train_rewards,'   number of trials = ',number_of_episodes)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-27 19:39:40,519] [PredictObsCartpole-v0] Uploading 503 episodes of training data\n",
      "[2017-05-27 19:39:41,892] [PredictObsCartpole-v0] Creating evaluation object from /tmp/PredictObsCartpole-experiment-1 with learning curve\n",
      "[2017-05-27 19:39:42,192] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on PredictObsCartpole-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_iFOMMpBuQeq2bA2WvUgryA\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gym.upload('/tmp/PredictObsCartpole-experiment-1', api_key='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
