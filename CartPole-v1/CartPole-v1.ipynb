{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "# base code from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 17:39:33,840] Making new env: CartPole-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.41691972e-02   4.36618228e-02   2.24535334e-02  -6.69160486e-05]\n"
     ]
    }
   ],
   "source": [
    "# Create new cart pole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "\n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "\n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "\n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "\n",
    "                \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=tf.nn.relu,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            if hidden_layers == 1:\n",
    "                out_layer = self.fc1\n",
    "            else:\n",
    "                \n",
    "                self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=tf.nn.relu,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                if hidden_layers == 2:\n",
    "                    out_layer = self.fc2\n",
    "                else:\n",
    "                    self.fc3 = tf.layers.dense(self.fc2, hidden_size,\\\n",
    "                                            activation=tf.nn.relu,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "                    out_layer = self.fc3\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_memory_rand_states(memory_size=10000,pretrain_length=20):\n",
    "    # Initialize the simulation\n",
    "    state = env.reset()\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    for ii in range(pretrain_length):\n",
    "\n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            \n",
    "    return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_q_network(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   max_steps=500,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        state = env.reset()\n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "                # Uncomment this next line to watch the training\n",
    "                # env.render() \n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    t = max_steps\n",
    "\n",
    "                    if verbose:\n",
    "                        print('Episode: {}'.format(ep),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_p))\n",
    "                    rewards_list.append((ep, total_reward))\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "\n",
    "                    # Start new episode\n",
    "                    state = env.reset()\n",
    "                    # Take one random step to get the pole and cart moving\n",
    "                    #state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "\n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_q_network(mainQN, saver, test_episodes=20, test_max_steps=500, render=True):\n",
    "\n",
    "    env.reset()\n",
    "    avg_rewards = 0.\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "   \n",
    "        state = env.reset()\n",
    "        for ep in range(test_episodes):\n",
    "            t = 0\n",
    "            while t < test_max_steps:\n",
    "                if render:\n",
    "                    env.render() \n",
    "\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                avg_rewards = avg_rewards + reward / test_episodes\n",
    "                if done:\n",
    "                    t = test_max_steps\n",
    "                    state = env.reset()\n",
    "                    # Take one random step to get the pole and cart moving\n",
    "                    #state, reward, done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "                    \n",
    "    return avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_and_train_qnetwork(train_episodes=500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=64,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=20,\\\n",
    "                   test_episodes=20,\\\n",
    "                   render=False,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver = train_q_network(train_episodes = train_episodes, \\\n",
    "                                                  gamma=gamma,\\\n",
    "                                                  explore_start=explore_start,\\\n",
    "                                                  explore_stop=explore_stop,\\\n",
    "                                                  decay_rate=decay_rate,\\\n",
    "                                                  hidden_size=hidden_size,\\\n",
    "                                                  hidden_layers=hidden_layers,\\\n",
    "                                                  learning_rate=learning_rate,\\\n",
    "                                                  memory_size=memory_size,\\\n",
    "                                                  batch_size=batch_size,\\\n",
    "                                                  verbose=False)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    # test q-network\n",
    "    avg_test_rewards = test_q_network(mainQN, saver, test_episodes=test_episodes, render=render)\n",
    "    if verbose:\n",
    "        print('average test reward = ', avg_test_rewards)\n",
    "    \n",
    "    return avg_test_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(20, 4), b.shape=(4, 64), m=20, n=64, k=4\n\t [[Node: main/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_main/inputs_0/_1, main/dense/kernel/read)]]\n\nCaused by op 'main/dense/MatMul', defined at:\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-de538cd827d6>\", line 2, in <module>\n    train_q_network()\n  File \"<ipython-input-41-106c99132d80>\", line 3, in train_q_network\n    mainQN = QNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate)\n  File \"<ipython-input-37-b51efbc2094d>\", line 25, in __init__\n    self.fc1 = tf.layers.dense(self.inputs_,                                         hidden_size,                                        activation=tf.nn.relu,                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 218, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 144, in call\n    outputs = standard_ops.matmul(inputs, self.kernel)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(20, 4), b.shape=(4, 64), m=20, n=64, k=4\n\t [[Node: main/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_main/inputs_0/_1, main/dense/kernel/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(20, 4), b.shape=(4, 64), m=20, n=64, k=4\n\t [[Node: main/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_main/inputs_0/_1, main/dense/kernel/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-de538cd827d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_q_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-106c99132d80>\u001b[0m in \u001b[0;36mtrain_q_network\u001b[0;34m(train_episodes, gamma, explore_start, explore_stop, decay_rate, hidden_size, hidden_layers, learning_rate, memory_size, batch_size, max_steps, verbose)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[1;31m# Train network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mtarget_Qs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[1;31m# Set target_Qs to 0 for states where episode ends\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(20, 4), b.shape=(4, 64), m=20, n=64, k=4\n\t [[Node: main/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_main/inputs_0/_1, main/dense/kernel/read)]]\n\nCaused by op 'main/dense/MatMul', defined at:\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 653, in launch_instance\n    app.start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-44-de538cd827d6>\", line 2, in <module>\n    train_q_network()\n  File \"<ipython-input-41-106c99132d80>\", line 3, in train_q_network\n    mainQN = QNetwork(name='main', hidden_size=hidden_size, hidden_layers=hidden_layers, learning_rate=learning_rate)\n  File \"<ipython-input-37-b51efbc2094d>\", line 25, in __init__\n    self.fc1 = tf.layers.dense(self.inputs_,                                         hidden_size,                                        activation=tf.nn.relu,                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 218, in dense\n    return layer.apply(inputs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 320, in apply\n    return self.__call__(inputs, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 290, in __call__\n    outputs = self.call(inputs, **kwargs)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 144, in call\n    outputs = standard_ops.matmul(inputs, self.kernel)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1801, in matmul\n    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1263, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\drbur\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Blas GEMM launch failed : a.shape=(20, 4), b.shape=(4, 64), m=20, n=64, k=4\n\t [[Node: main/dense/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_main/inputs_0/_1, main/dense/kernel/read)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_q_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-22 15:54:17,537] Restoring parameters from checkpoints\\cartpole.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test reward =  9.249999999999996\n"
     ]
    }
   ],
   "source": [
    "# test implementation\n",
    "average_rewards = test_and_train_qnetwork(train_episodes=100, verbose=False)\n",
    "print('average test reward = ', average_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_eps = 500\n",
    "verb = False\n",
    "gamma = 0.99\n",
    "exp_start=1.0\n",
    "exp_stop=0.1\n",
    "decay_rate=0.0001\n",
    "hidden_size=[16,32,64]\n",
    "hidden_layers=[1,2,3]\n",
    "learning_rate=[0.01,0.001,0.0001]\n",
    "batch_size=[10,20,40]\n",
    "num_averages = 2\n",
    "results = []\n",
    "\n",
    "for hs in hidden_size:\n",
    "    for hl in hidden_layers:\n",
    "        for lr in learning_rate:\n",
    "            for bs in batch_size:\n",
    "                train_params_name = 'hs='+str(hs)+'_hl='+str(hl)+'_lr='+str(lr)+'_bs='+str(bs)\n",
    "                average_rewards = 0.\n",
    "                for i in range(num_averages):\n",
    "                    average_rewards += test_and_train_qnetwork(train_episodes=train_eps,\\\n",
    "                                           gamma=gamma,\\\n",
    "                                           explore_start=exp_start,\\\n",
    "                                           explore_stop=exp_stop,\\\n",
    "                                           decay_rate=decay_rate,\\\n",
    "                                           hidden_layers=hl,\\\n",
    "                                           hidden_size=hs,\\\n",
    "                                           learning_rate=lr,\\\n",
    "                                           batch_size=bs,\\\n",
    "                                           verbose=verb)\n",
    "                average_rewards = average_rewards / num_averages\n",
    "                results.append([train_params_name+' '+str(average_rewards)])\n",
    "                clear_output()\n",
    "                for each in results:\n",
    "                    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpu:0', '/gpu:1']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
