{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-24 19:43:23,657] Making new env: CartPole-v1\n",
      "[2017-05-24 19:43:23,673] Finished writing results. You can upload them to the scoreboard via gym.upload('D:\\\\tmp\\\\cartpole-experiment-1')\n",
      "[2017-05-24 19:43:23,687] Clearing 20 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-24 19:43:24,039] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000000.mp4\n",
      "[2017-05-24 19:43:24,479] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000001.mp4\n",
      "[2017-05-24 19:43:25,851] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000008.mp4\n",
      "[2017-05-24 19:43:27,636] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000027.mp4\n",
      "[2017-05-24 19:43:30,846] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000064.mp4\n",
      "[2017-05-24 19:43:37,273] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000125.mp4\n",
      "[2017-05-24 19:43:53,809] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000216.mp4\n",
      "[2017-05-24 19:47:56,437] Starting new video recorder writing to D:\\tmp\\cartpole-experiment-1\\openaigym.video.1.14292.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average test reward =  195.848717949    number of trials =  390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Monitor.close of <Monitor<TimeLimit<CartPoleEnv<CartPole-v1>>>>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from gym import wrappers\n",
    "\n",
    "%matplotlib inline\n",
    "# base code from udacity-deep-learning/reinforcement/Q-learning-cart.ipynb\n",
    "\n",
    "# Create class QNetwork\n",
    "class QNetwork:\n",
    "    def __init__(self, \\\n",
    "                 learning_rate=0.01, \\\n",
    "                 state_size=4, \n",
    "                 action_size=2, \\\n",
    "                 hidden_size=10, \\\n",
    "                 hidden_layers=2, \\\n",
    "                 alpha=0., \\\n",
    "                 name='QNetwork'):\n",
    "        \n",
    "        # create Q Network\n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, \\\n",
    "                                          [None, state_size], \\\n",
    "                                          name='inputs')\n",
    "            \n",
    "            # placeholder for actions, to be one-hot encoded next\n",
    "            self.actions_ = tf.placeholder(tf.int32, \\\n",
    "                                           [None], \\\n",
    "                                           name='actions')\n",
    "            \n",
    "            # one hot encode actions\n",
    "            one_hot_actions = tf.one_hot(self.actions_, \\\n",
    "                                         action_size)\n",
    "            \n",
    "            # placeholder for target Qs\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, \\\n",
    "                                            [None], \\\n",
    "                                            name='target')\n",
    "            \n",
    "                \n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.layers.dense(self.inputs_, \\\n",
    "                                        hidden_size,\\\n",
    "                                        activation=None,\\\n",
    "                                        kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc1 = tf.maximum(alpha*self.fc1,self.fc1)\n",
    "            \n",
    "            self.fc2 = tf.layers.dense(self.fc1, hidden_size,\\\n",
    "                                            activation=None,\\\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.fc2 = tf.maximum(alpha*self.fc2,self.fc2)\n",
    "                \n",
    "            out_layer = self.fc2\n",
    "            \n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.layers.dense(out_layer, action_size, \\\n",
    "                                          activation=None,\\\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            ### Train with loss (targetQ - Q)^2\n",
    "            # output has length 2, for two actions. This next line chooses\n",
    "            # one value from output (per row) according to the one-hot encoded actions.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "            \n",
    "# create memory class for storing previous experiences\n",
    "class Memory():\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
    "                               size=batch_size, \n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]\n",
    "\n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N \n",
    "\n",
    "def normalize_state(x):\n",
    "    normalizer = [2.,3.,0.3,2.]\n",
    "    y = x / normalizer\n",
    "    return y\n",
    "\n",
    "def initialize_memory_rand_states(memory_size=1000,pretrain_length=32):\n",
    "    \n",
    "    # Initialize the simulation\n",
    "    # Make a random action\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    memory = Memory(max_size=memory_size)\n",
    "\n",
    "    # Make a bunch of random actions and store the experiences\n",
    "    ii = 0\n",
    "    while ii < pretrain_length or not done:\n",
    "        \n",
    "        # Make a random action\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            # The simulation fails so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            \n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            state = env.reset()\n",
    "            state = normalize_state(state)\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "        \n",
    "        ii = ii + 1\n",
    "            \n",
    "    return memory\n",
    "\n",
    "def train_q_network(mainQN,\\\n",
    "                    memory,\\\n",
    "                    train_episodes=1500,\\\n",
    "                    gamma=0.99,\\\n",
    "                    explore_start=1.0,\\\n",
    "                    explore_stop=0.01,\\\n",
    "                    decay_rate=0.0001,\\\n",
    "                    batch_size=32,\\\n",
    "                    max_steps=500,\\\n",
    "                    verbose=True):\n",
    "    \n",
    "    \n",
    "    state = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    \n",
    "    # Now train with experiences\n",
    "    saver = tf.train.Saver()\n",
    "    rewards_list = []\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        steps_list = []\n",
    "        \n",
    "        for ep in range(train_episodes):\n",
    "            total_reward = 0\n",
    "            t = 0\n",
    "            \n",
    "            while t < max_steps:\n",
    "                step += 1\n",
    "\n",
    "                # Explore or Exploit\n",
    "                explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
    "                if explore_p > np.random.rand():\n",
    "                    # Make a random action\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    # Get action from Q-network\n",
    "                    state_normalizer = [2.,3.,0.3,3.]\n",
    "                    feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                    Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                    action = np.argmax(Qs)\n",
    "\n",
    "                # Take action, get new state and reward\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = normalize_state(next_state)\n",
    "                \n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    t = t+1\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros(state.shape)\n",
    "                    steps_list.append(t)\n",
    "                    t = max_steps\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = env.reset()\n",
    "                    state = normalize_state(state)\n",
    "                else:\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state))\n",
    "                    state = next_state\n",
    "                    t += 1\n",
    "\n",
    "                # Sample mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states = np.array([each[0] for each in batch])\n",
    "                actions = np.array([each[1] for each in batch])\n",
    "                rewards = np.array([each[2] for each in batch])\n",
    "                next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "                # Train network\n",
    "                target_Qs = sess.run(mainQN.output, feed_dict={mainQN.inputs_: next_states})\n",
    "\n",
    "                # Set target_Qs to 0 for states where episode ends\n",
    "                episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
    "                target_Qs[episode_ends] = (0, 0)\n",
    "\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "                loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                    feed_dict={mainQN.inputs_: states,\n",
    "                                               mainQN.targetQs_: targets,\n",
    "                                               mainQN.actions_: actions})\n",
    "            \n",
    "            rewards_list.append((ep, total_reward))   \n",
    "            runningMean = np.mean(steps_list[-100:])\n",
    "            if verbose:\n",
    "                \n",
    "                print('Episode: {}'.format(ep),\n",
    "                      'Total reward: {}'.format(total_reward),\n",
    "                      'Training loss: {:.4f}'.format(loss),\n",
    "                      'Explore P: {:.4f}'.format(explore_p),\n",
    "                      'RunMean : {:.4f}'.format(runningMean))\n",
    "               \n",
    "            \n",
    "            \n",
    "            if runningMean > 495.:\n",
    "                saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "                return rewards_list, mainQN, saver\n",
    "            \n",
    "        saver.save(sess, \"checkpoints/cartpole.ckpt\")\n",
    "        return rewards_list, mainQN, saver\n",
    "\n",
    "def plot_rewards(rewards_list):\n",
    "    eps, rews = np.array(rewards_list).T\n",
    "    smoothed_rews = running_mean(rews, 10)\n",
    "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
    "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "def generate_and_train_qnetwork(train_episodes=1500,\\\n",
    "                   gamma=0.99,\\\n",
    "                   explore_start=1.0,\\\n",
    "                   explore_stop=0.01,\\\n",
    "                   decay_rate=0.0001,\\\n",
    "                   hidden_size=128,\\\n",
    "                   hidden_layers=2,\\\n",
    "                   learning_rate=0.0001,\\\n",
    "                   memory_size=10000,\\\n",
    "                   batch_size=32,\\\n",
    "                   render=False,\\\n",
    "                   alpha=0.1,\\\n",
    "                   verbose=True):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    mainQN = QNetwork(name='main', hidden_size=hidden_size, \\\n",
    "                      hidden_layers=hidden_layers, learning_rate=learning_rate, alpha=alpha)\n",
    "    \n",
    "    memory = initialize_memory_rand_states(memory_size=memory_size,pretrain_length=batch_size)\n",
    "\n",
    "    \n",
    "    # train q-network\n",
    "    rewards_list, mainQN, saver = train_q_network(mainQN,\\\n",
    "                                  memory,\\\n",
    "                                  train_episodes = train_episodes, \\\n",
    "                                  gamma=gamma,\\\n",
    "                                  explore_start=explore_start,\\\n",
    "                                  explore_stop=explore_stop,\\\n",
    "                                  decay_rate=decay_rate,\\\n",
    "                                  batch_size=batch_size,\\\n",
    "                                  verbose=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        # plot training\n",
    "        plot_rewards(rewards_list)\n",
    "    \n",
    "    avg_train_rewards = np.sum([each[1] for each in rewards_list]) / len(rewards_list)\n",
    "    if verbose:\n",
    "        print('average training reward = ',avg_train_rewards)\n",
    "\n",
    "    \n",
    "    return avg_train_rewards, mainQN, saver, len(rewards_list)\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Start monitor\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1',force=True)\n",
    "\n",
    "# Train network\n",
    "avg_train_rewards, mainQN, saver, number_of_episodes = generate_and_train_qnetwork(train_episodes=1000, verbose=False)\n",
    "print('average test reward = ', avg_train_rewards,'   number of trials = ',number_of_episodes)\n",
    "\n",
    "# Close environment\n",
    "env.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-24 19:52:17,301] Finished writing results. You can upload them to the scoreboard via gym.upload('D:\\\\tmp\\\\cartpole-experiment-1')\n",
      "[2017-05-24 19:52:17,303] [CartPole-v1] Uploading 392 episodes of training data\n",
      "[2017-05-24 19:52:18,901] [CartPole-v1] Uploading videos of 8 training episodes (51022 bytes)\n",
      "[2017-05-24 19:52:19,329] [CartPole-v1] Creating evaluation object from /tmp/cartpole-experiment-1 with learning curve and training video\n",
      "[2017-05-24 19:52:19,643] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on CartPole-v1 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_O2E4DuxjS7GAjEYSb0edqQ\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "gym.upload('/tmp/cartpole-experiment-1', api_key='sk_2nAEHbARwKPuKcao8nWRw')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
